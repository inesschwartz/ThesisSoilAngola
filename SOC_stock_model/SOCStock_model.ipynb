{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOC Stock Model Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan: \n",
    "- **Gather the covariates** using the SCORPAN model \n",
    "- **add harmonized and transformed soc** (response)\n",
    "- **clean and process training data** -- Take out variables with moderate to high amnts of missing data, impute variables w low to mod\n",
    "- **Handle categorical variables** --> Convert text/categorical variables (e.g., moist_color_name, friability) to numeric codes or one-hot encoding.\n",
    "- **Handle numeric predictors**. Scale/normalize if using regression methods sensitive to magnitude (optional, regression kriging is usually OK without scaling).Ensure no extreme outliers that could bias the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare Training Data to estimate SOC stock throughout Angola\n",
    "import pandas as pd\n",
    "\n",
    "training_data = pd.read_csv(\"/Users/inesschwartz/Desktop/training_data_table_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['site_info_id', 'X_coord', 'Y_coord', 'profile', 'district', 'MRVBF',\n",
       "       'RLD', 'aspect', 'aspect_classes', 'aspect_cos', 'aspect_sin',\n",
       "       'dem_filledfiltered', 'flow_accumulation', 'relief', 'ridge_levels',\n",
       "       'roughness', 'slope', 'twi_300m', 'valleydepth2', 'aspect_label',\n",
       "       'annual_mean_temp', 'annual_precip2', 'isothermality_32733',\n",
       "       'max_temp_warmest_month32733', 'mean_temp_driest_quarter32733',\n",
       "       'mean_temp_warmest_quarter32733', 'mean_temp_wettest_quarter32733',\n",
       "       'min_temp_coldest_month32733', 'precip_coldest_quarter32733',\n",
       "       'precip_driest_month32733', 'precip_driest_quarter32733',\n",
       "       'precip_seasonality2', 'precip_warmest_quarter32733',\n",
       "       'precip_wettest_month32733', 'precip_wettest_quarter32733',\n",
       "       'temp_annual_range32733', 'temp_seasonality32733', 'landsurface_value',\n",
       "       'landsurface_label', 'eco_value', 'eco_subclass_clean',\n",
       "       'eco_class_clean', 'eco_division_clean', 'eco_subclass_code',\n",
       "       'eco_class_code', 'eco_division_code', 'FAOSOIL', 'DOMSOI',\n",
       "       'faosoil_id', 'lithology', 'log_soc_stock'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "identify numeric vs categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['site_info_id', 'X_coord', 'Y_coord', 'profile', 'district', 'MRVBF',\n",
       "       'RLD', 'aspect', 'aspect_classes', 'aspect_cos', 'aspect_sin',\n",
       "       'dem_filledfiltered', 'flow_accumulation', 'relief', 'ridge_levels',\n",
       "       'roughness', 'slope', 'twi_300m', 'valleydepth2', 'annual_mean_temp',\n",
       "       'annual_precip2', 'isothermality_32733', 'max_temp_warmest_month32733',\n",
       "       'mean_temp_driest_quarter32733', 'mean_temp_warmest_quarter32733',\n",
       "       'mean_temp_wettest_quarter32733', 'min_temp_coldest_month32733',\n",
       "       'precip_coldest_quarter32733', 'precip_driest_month32733',\n",
       "       'precip_driest_quarter32733', 'precip_seasonality2',\n",
       "       'precip_warmest_quarter32733', 'precip_wettest_month32733',\n",
       "       'precip_wettest_quarter32733', 'temp_annual_range32733',\n",
       "       'temp_seasonality32733', 'landsurface_value', 'landsurface_label',\n",
       "       'eco_value', 'eco_subclass_clean', 'eco_class_clean',\n",
       "       'eco_division_clean', 'eco_subclass_code', 'eco_class_code',\n",
       "       'eco_division_code', 'FAOSOIL', 'DOMSOI', 'faosoil_id', 'lithology',\n",
       "       'log_soc_stock'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric predictors:\n",
      "['MRVBF', 'RLD', 'aspect', 'aspect_classes', 'aspect_cos', 'aspect_sin', 'dem_filledfiltered', 'flow_accumulation', 'relief', 'ridge_levels', 'roughness', 'slope', 'twi_300m', 'valleydepth2', 'annual_mean_temp', 'annual_precip2', 'isothermality_32733', 'max_temp_warmest_month32733', 'mean_temp_driest_quarter32733', 'mean_temp_warmest_quarter32733', 'mean_temp_wettest_quarter32733', 'min_temp_coldest_month32733', 'precip_coldest_quarter32733', 'precip_driest_month32733', 'precip_driest_quarter32733', 'precip_seasonality2', 'precip_warmest_quarter32733', 'precip_wettest_month32733', 'precip_wettest_quarter32733', 'temp_annual_range32733', 'temp_seasonality32733', 'landsurface_value', 'eco_value', 'eco_subclass_code', 'eco_class_code', 'eco_division_code', 'faosoil_id']\n",
      "\n",
      "Categorical predictors:\n",
      "['DOMSOI', 'africa_lithology_90m.img.vat_lithology']\n"
     ]
    }
   ],
   "source": [
    "## identify numerica vs categorical variables\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "training_data\n",
    "\n",
    "# Separate numeric and categorical predictors\n",
    "numeric_cols = training_data.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_cols = training_data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove response variable from predictors\n",
    "numeric_cols.remove('log_soc_stock')  # response variable\n",
    "\n",
    "print(\"Numeric predictors:\")\n",
    "print(numeric_cols)\n",
    "\n",
    "print(\"\\nCategorical predictors:\")\n",
    "print(categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns included: ['DOMSOI', 'africa_lithology_90m.img.vat_lithology']\n",
      "Categorical columns skipped (too many categories): []\n",
      "Numeric columns: ['MRVBF', 'RLD', 'aspect', 'aspect_classes', 'aspect_cos', 'aspect_sin', 'dem_filledfiltered', 'flow_accumulation', 'relief', 'ridge_levels', 'roughness', 'slope', 'twi_300m', 'valleydepth2', 'annual_mean_temp', 'annual_precip2', 'isothermality_32733', 'max_temp_warmest_month32733', 'mean_temp_driest_quarter32733', 'mean_temp_warmest_quarter32733', 'mean_temp_wettest_quarter32733', 'min_temp_coldest_month32733', 'precip_coldest_quarter32733', 'precip_driest_month32733', 'precip_driest_quarter32733', 'precip_seasonality2', 'precip_warmest_quarter32733', 'precip_wettest_month32733', 'precip_wettest_quarter32733', 'temp_annual_range32733', 'temp_seasonality32733', 'landsurface_value', 'eco_value', 'eco_subclass_code', 'eco_class_code', 'eco_division_code', 'faosoil_id']\n",
      "\n",
      "Top 25 Features by Importance:\n",
      "                           feature  importance\n",
      "62             precip_seasonality2    0.219696\n",
      "46                    ridge_levels    0.042961\n",
      "41                      aspect_cos    0.036301\n",
      "52                  annual_precip2    0.035877\n",
      "43              dem_filledfiltered    0.035329\n",
      "38                             RLD    0.035174\n",
      "48                           slope    0.034563\n",
      "37                           MRVBF    0.033345\n",
      "66          temp_annual_range32733    0.032954\n",
      "65     precip_wettest_quarter32733    0.032732\n",
      "49                        twi_300m    0.032184\n",
      "42                      aspect_sin    0.030811\n",
      "73                      faosoil_id    0.027363\n",
      "44               flow_accumulation    0.027267\n",
      "39                          aspect    0.027005\n",
      "50                    valleydepth2    0.025801\n",
      "55   mean_temp_driest_quarter32733    0.025799\n",
      "64       precip_wettest_month32733    0.024236\n",
      "54     max_temp_warmest_month32733    0.022631\n",
      "63     precip_warmest_quarter32733    0.020435\n",
      "53             isothermality_32733    0.019843\n",
      "47                       roughness    0.018862\n",
      "45                          relief    0.017553\n",
      "67           temp_seasonality32733    0.015728\n",
      "57  mean_temp_wettest_quarter32733    0.014596\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ======================================================\n",
    "# --- Handle missing data and drop unwanted columns ---\n",
    "# ======================================================\n",
    "\n",
    "# Drop specific unwanted columns\n",
    "cols_to_drop = ['profile', 'site_info_id', 'district', 'landsurface_label', 'eco_subclass_clean', 'eco_class_clean', 'eco_division_clean', 'FAOSOIL', 'X_coord', 'Y_coord']\n",
    "training_data1 = training_data\n",
    "training_data1.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# --- Feature selection with categorical + numeric ---\n",
    "# ======================================================\n",
    "\n",
    "# Separate target\n",
    "y = training_data1['log_soc_stock']\n",
    "X = training_data1.drop(columns=['log_soc_stock'], errors='ignore')\n",
    "\n",
    "# Automatically detect categorical and numeric columns\n",
    "all_categorical = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Safeguard: keep only categorical columns with <= 50 unique categories\n",
    "cat_limit = 50\n",
    "categorical_cols = [col for col in all_categorical if X[col].nunique() <= cat_limit]\n",
    "\n",
    "print(\"Categorical columns included:\", categorical_cols)\n",
    "print(\"Categorical columns skipped (too many categories):\",\n",
    "      [col for col in all_categorical if col not in categorical_cols])\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "        ('num', 'passthrough', numeric_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Fit model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Get feature names after OHE\n",
    "ohe_feature_names = []\n",
    "if categorical_cols:\n",
    "    ohe_feature_names = pipeline.named_steps['preprocessor'] \\\n",
    "        .named_transformers_['cat'] \\\n",
    "        .get_feature_names_out(categorical_cols)\n",
    "\n",
    "all_feature_names = np.concatenate([ohe_feature_names, numeric_cols])\n",
    "\n",
    "# Feature importances\n",
    "importances = pipeline.named_steps['model'].feature_importances_\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': all_feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 25 Features by Importance:\")\n",
    "print(feature_importance.head(25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating prediction grid\n",
    "\n",
    "What script does:\n",
    "- 1 km hex grid — matches fine-resolution DEM, soil, ecosystems layers. \n",
    "    - Explicit raster type lists for continuous 1 km, continuous bioclimatic (~18.5 km), categorical 1 km, categorical lithology (~50 km)\n",
    "- Raster resolution check (xres, yres) — distinguishes whether raster is finer/equal or coarser than hex.\n",
    "- Continuous layers:\n",
    "    - Hex smaller than raster → value at centroid.\n",
    "    - Hex equal/finer → zonal mean.\n",
    "\n",
    "- Categorical layers:\n",
    "    - Hex smaller than raster → value at centroid.\n",
    "    - Hex equal/finer → majority value.\n",
    "- Chunked processing — memory-efficient.\n",
    "- Saves outputs in pickle, geopackage, CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Paths & parameters\n",
    "# ------------------------------\n",
    "\n",
    "# Angola boundary (projected in EPSG:32733)\n",
    "angola_boundary_path = \"/Volumes/One_Touch/angola_soils_thesis/GIS_Angola/data_processed/angola_boundaries_32733.gpkg\"\n",
    "\n",
    "# Covariate folders\n",
    "terrain_folder = \"/Volumes/One_Touch/angola_soils_thesis/GIS_Angola/data_processed/dem_1km\"\n",
    "bioclim_folder = \"/Volumes/One_Touch/angola_soils_thesis/GIS_Angola/data_processed/bioclimatic32733/\" #18.5km\n",
    "\n",
    "categorical_rasters = {\n",
    "    \"labelled_ecosystems32733\": \"/Volumes/One_Touch/angola_soils_thesis/GIS_Angola/data_processed/labelled_ecosystems32733_1km.tif\",\n",
    "    \"landsurfaceforms\": \"/Volumes/One_Touch/angola_soils_thesis/GIS_Angola/data_processed/landsurfaceforms/landsurfaceforms_1km.tif\",\n",
    "    \"lithology_raster\": \"/Volumes/One_Touch/angola_soils_thesis/GIS_Angola/data_processed/lithology_rasterized.tif\", #50km\n",
    "    \"soil_raster\": \"/Volumes/One_Touch/angola_soils_thesis/GIS_Angola/data_processed/angola_soil_data_raster_1km.tif\", #band = faosoil \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  3.  4.  5.  6.  7. 12. 14. 16. 20.]\n"
     ]
    }
   ],
   "source": [
    "import rasterio, numpy as np\n",
    "with rasterio.open(\"/Volumes/One_Touch/angola_soils_thesis/GIS_Angola/data_processed/lithology_rasterized.tif\") as src:\n",
    "    arr = src.read(1, masked=True)\n",
    "    print(np.unique(arr.compressed()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 2) Load Angola boundary\n",
    "# ------------------------------\n",
    "angola_gdf = gpd.read_file(angola_boundary_path).to_crs(epsg=32733)\n",
    "\n",
    "# ------------------------------\n",
    "# 3) List rasters\n",
    "# ------------------------------\n",
    "def list_tifs(folder):\n",
    "    return sorted([os.path.join(folder, f) for f in os.listdir(folder)\n",
    "                   if f.lower().endswith(\".tif\") and not f.startswith(\"._\")])\n",
    "\n",
    "terrain_rasters = list_tifs(terrain_folder)     # 1 km continuous\n",
    "bioclim_rasters = list_tifs(bioclim_folder)     # ~18.5 km continuous\n",
    "\n",
    "all_covariates = terrain_rasters + bioclim_rasters + list(categorical_rasters.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 4) Resampling map & types\n",
    "# ------------------------------\n",
    "resampling_map = {\n",
    "    \"nearest\": Resampling.nearest,\n",
    "    \"bilinear\": Resampling.bilinear,\n",
    "    \"cubic\": Resampling.cubic\n",
    "}\n",
    "\n",
    "categorical_paths = list(categorical_rasters.values())\n",
    "continuous_paths = [p for p in all_covariates if p not in categorical_paths]\n",
    "\n",
    "# ------------------------------\n",
    "# 5) Template raster (DEM 1 km)\n",
    "# ------------------------------\n",
    "template = rxr.open_rasterio(terrain_rasters[0], masked=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 6) Harmonise & stack\n",
    "# ------------------------------\n",
    "stack_list = []\n",
    "\n",
    "for path in all_covariates:\n",
    "    name = os.path.splitext(os.path.basename(path))[0]\n",
    "    da = rxr.open_rasterio(path, masked=True)\n",
    "\n",
    "    # Choose resampling method\n",
    "    if path in categorical_paths:\n",
    "        resample_method = resampling_map[\"nearest\"]\n",
    "    else:\n",
    "        resample_method = resampling_map[\"bilinear\"]\n",
    "\n",
    "    # Reproject/resample to match template\n",
    "    da = da.rio.reproject_match(template, resampling=resample_method) #bioclim = bilinear resampling → 1 km grid, lithology = nearest-neighbour resampling → 1 km grid.\n",
    "\n",
    "    # Clip to Angola\n",
    "    da = da.rio.clip(angola_gdf.geometry, angola_gdf.crs)\n",
    "\n",
    "    # Squeeze single-band rasters and rename\n",
    "    da = da.squeeze(drop=True).rename(name)\n",
    "\n",
    "    stack_list.append(da)\n",
    "\n",
    "# Combine into a single xarray Dataset (each covariate = one band)\n",
    "covariate_stack = xr.merge(stack_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Covariate raster stack saved as angola_covariate_stack.tif\n",
      "Variables: ['MRVBF_1km', 'RLD_1km', 'aspect_1km', 'aspect_cos_1km', 'aspect_sin_1km', 'dem_filledfiltered_1km', 'flow_accumulation_1km', 'relief_1km', 'ridge_levels_1km', 'roughness_1km', 'slope_1km', 'twi_300m_1km', 'valleydepth2_1km', 'annual_mean_temp', 'annual_precip2', 'isothermality_32733', 'max_temp_warmest_month32733', 'mean_temp_driest_quarter32733', 'mean_temp_warmest_quarter32733', 'mean_temp_wettest_quarter32733', 'min_temp_coldest_month32733', 'precip_coldest_quarter32733', 'precip_driest_month32733', 'precip_driest_quarter32733', 'precip_seasonality2', 'precip_warmest_quarter32733', 'precip_wettest_month32733', 'precip_wettest_quarter32733', 'temp_annual_range32733', 'temp_seasonality32733', 'labelled_ecosystems32733_1km', 'landsurfaceforms_1km', 'lithology_rasterized', 'angola_soil_data_raster_1km']\n",
      "✅ Flattened covariate stack ready for prediction\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 7) Optional: write multiband GeoTIFF\n",
    "# ------------------------------\n",
    "covariate_stack.to_array().rio.to_raster(\"angola_covariate_stack.tif\", compress=\"lzw\")\n",
    "\n",
    "print(\"✅ Covariate raster stack saved as angola_covariate_stack.tif\")\n",
    "print(\"Variables:\", list(covariate_stack.data_vars))\n",
    "\n",
    "# ------------------------------\n",
    "# 8) Flatten stack to 2D array for prediction (optional)\n",
    "# ------------------------------\n",
    "# Each row = pixel, each column = covariate\n",
    "flat_stack = covariate_stack.to_array().stack(pixel=(\"y\", \"x\")).transpose(\"pixel\", \"variable\")\n",
    "flat_stack_df = flat_stack.to_pandas()\n",
    "print(\"✅ Flattened covariate stack ready for prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data columns:\n",
      "['site_info_id', 'X_coord', 'Y_coord', 'profile', 'district', 'MRVBF', 'RLD', 'aspect', 'aspect_cos', 'aspect_sin', 'dem', 'flow_accumulation', 'relief', 'ridge_levels', 'roughness', 'slope', 'twi', 'valleydepth', 'annual_mean_temp', 'annual_precip', 'isothermality', 'max_temp_warmest_month', 'mean_temp_driest_quarter', 'mean_temp_warmest_quarter', 'mean_temp_wettest_quarter', 'min_temp_coldest_month', 'precip_coldest_quarter', 'precip_driest_month', 'precip_driest_quarter', 'precip_seasonality', 'precip_warmest_quarter', 'precip_wettest_month', 'precip_wettest_quarter', 'temp_annual_range', 'temp_seasonality', 'landsurface_value', 'landsurface_label', 'eco_value', 'eco_subclass_code', 'eco_class_code', 'eco_division_code', 'FAOSOIL', 'DOMSOI', 'faosoil_id', 'lithology', 'log_soc_stock']\n",
      "Total columns: 46\n",
      "\n",
      "Prediction grid (covariate stack) variables:\n",
      "['MRVBF_1km', 'RLD_1km', 'aspect_1km', 'aspect_cos_1km', 'aspect_sin_1km', 'dem_filledfiltered_1km', 'flow_accumulation_1km', 'relief_1km', 'ridge_levels_1km', 'roughness_1km', 'slope_1km', 'twi_300m_1km', 'valleydepth2_1km', 'annual_mean_temp', 'annual_precip2', 'isothermality_32733', 'max_temp_warmest_month32733', 'mean_temp_driest_quarter32733', 'mean_temp_warmest_quarter32733', 'mean_temp_wettest_quarter32733', 'min_temp_coldest_month32733', 'precip_coldest_quarter32733', 'precip_driest_month32733', 'precip_driest_quarter32733', 'precip_seasonality2', 'precip_warmest_quarter32733', 'precip_wettest_month32733', 'precip_wettest_quarter32733', 'temp_annual_range32733', 'temp_seasonality32733', 'labelled_ecosystems32733_1km', 'landsurfaceforms_1km', 'lithology_rasterized', 'angola_soil_data_raster_1km']\n",
      "Total variables: 34\n",
      "\n",
      "Columns in training data but missing in prediction grid: ['site_info_id', 'X_coord', 'Y_coord', 'profile', 'district', 'MRVBF', 'RLD', 'aspect', 'aspect_cos', 'aspect_sin', 'dem', 'flow_accumulation', 'relief', 'ridge_levels', 'roughness', 'slope', 'twi', 'valleydepth', 'annual_precip', 'isothermality', 'max_temp_warmest_month', 'mean_temp_driest_quarter', 'mean_temp_warmest_quarter', 'mean_temp_wettest_quarter', 'min_temp_coldest_month', 'precip_coldest_quarter', 'precip_driest_month', 'precip_driest_quarter', 'precip_seasonality', 'precip_warmest_quarter', 'precip_wettest_month', 'precip_wettest_quarter', 'temp_annual_range', 'temp_seasonality', 'landsurface_value', 'landsurface_label', 'eco_value', 'eco_subclass_code', 'eco_class_code', 'eco_division_code', 'FAOSOIL', 'DOMSOI', 'faosoil_id', 'lithology', 'log_soc_stock']\n",
      "Columns in prediction grid but missing in training data: ['MRVBF_1km', 'RLD_1km', 'aspect_1km', 'aspect_cos_1km', 'aspect_sin_1km', 'dem_filledfiltered_1km', 'flow_accumulation_1km', 'relief_1km', 'ridge_levels_1km', 'roughness_1km', 'slope_1km', 'twi_300m_1km', 'valleydepth2_1km', 'annual_precip2', 'isothermality_32733', 'max_temp_warmest_month32733', 'mean_temp_driest_quarter32733', 'mean_temp_warmest_quarter32733', 'mean_temp_wettest_quarter32733', 'min_temp_coldest_month32733', 'precip_coldest_quarter32733', 'precip_driest_month32733', 'precip_driest_quarter32733', 'precip_seasonality2', 'precip_warmest_quarter32733', 'precip_wettest_month32733', 'precip_wettest_quarter32733', 'temp_annual_range32733', 'temp_seasonality32733', 'labelled_ecosystems32733_1km', 'landsurfaceforms_1km', 'lithology_rasterized', 'angola_soil_data_raster_1km']\n"
     ]
    }
   ],
   "source": [
    "## COMPARING TRAINING DATA AND RASTER PREDICTION GRID\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Training data\n",
    "# ------------------------------\n",
    "training_data = pd.read_csv(\"/Users/inesschwartz/Desktop/training_data.csv\")\n",
    "training_columns = list(training_data.columns)\n",
    "print(\"Training data columns:\")\n",
    "print(training_columns)\n",
    "print(f\"Total columns: {len(training_columns)}\\n\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Covariate stack\n",
    "# ------------------------------\n",
    "# Assuming you already have covariate_stack loaded with xarray\n",
    "# If not, uncomment and load:\n",
    "# import rioxarray as rxr\n",
    "# import xarray as xr\n",
    "# covariate_stack = rxr.open_rasterio(\"angola_covariate_stack.tif\", masked=True)\n",
    "\n",
    "stack_columns = list(covariate_stack.data_vars)\n",
    "print(\"Prediction grid (covariate stack) variables:\")\n",
    "print(stack_columns)\n",
    "print(f\"Total variables: {len(stack_columns)}\\n\")\n",
    "\n",
    "# ------------------------------\n",
    "# 3) Compare\n",
    "# ------------------------------\n",
    "missing_in_stack = [c for c in training_columns if c not in stack_columns]\n",
    "missing_in_training = [c for c in stack_columns if c not in training_columns]\n",
    "\n",
    "print(\"Columns in training data but missing in prediction grid:\", missing_in_stack)\n",
    "print(\"Columns in prediction grid but missing in training data:\", missing_in_training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREVIOUS WORKFLOW (HEX GRID NOT RASTER GRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 64\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m y \u001b[38;5;241m<\u001b[39m ymax \u001b[38;5;241m+\u001b[39m dy:\n\u001b[1;32m     60\u001b[0m     hexagon \u001b[38;5;241m=\u001b[39m Polygon([\n\u001b[1;32m     61\u001b[0m         (x \u001b[38;5;241m+\u001b[39m hex_size \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mcos(a), y \u001b[38;5;241m+\u001b[39m hex_size \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msin(a))\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi, \u001b[38;5;241m7\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     63\u001b[0m     ])\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhexagon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintersects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mangola_boundary\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     65\u001b[0m         hexagons\u001b[38;5;241m.\u001b[39mappend(hexagon\u001b[38;5;241m.\u001b[39mintersection(angola_boundary))\n\u001b[1;32m     66\u001b[0m     y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dy\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/shapely/geometry/base.py:719\u001b[0m, in \u001b[0;36mBaseGeometry.intersects\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mintersects\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m    718\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns True if geometries intersect, else False\"\"\"\u001b[39;00m\n\u001b[0;32m--> 719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _maybe_unpack(\u001b[43mshapely\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintersects\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/shapely/decorators.py:77\u001b[0m, in \u001b[0;36mmultithreading_enabled.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m array_args:\n\u001b[1;32m     76\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arr, old_flag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(array_args, old_flags):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/shapely/predicates.py:778\u001b[0m, in \u001b[0;36mintersects\u001b[0;34m(a, b, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;129m@multithreading_enabled\u001b[39m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mintersects\u001b[39m(a, b, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    749\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns True if A and B share any portion of space.\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \n\u001b[1;32m    751\u001b[0m \u001b[38;5;124;03m    Intersects implies that overlaps, touches, covers, or within are True.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintersects\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#previous workflow hex grid\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Paths & parameters\n",
    "# ------------------------------------------------------------------\n",
    "angola_boundary_path = \"/Volumes/One_Touch/angola_soils_thesis/GIS_Angola/data_processed/angola_boundaries_32733.gpkg\"\n",
    "\n",
    "terrain_folder = \"/Volumes/One_Touch/angola_soils_thesis/GIS_Angola/data_processed/DEM_characteristics_300m\"\n",
    "bioclim_folder = \"/Volumes/One_Touch/angola_soils_thesis/GIS_Angola/data_processed/bioclimatic32733/\"\n",
    "\n",
    "categorical_rasters = {\n",
    "    \"labelled_ecosystems32733\": \"/Volumes/One_Touch/angola_soils_thesis/GIS_Angola/data_processed/labelled_ecosystems32733_1km.tif\",\n",
    "    \"landsurfaceforms\":          \"/Volumes/One_Touch/angola_soils_thesis/GIS_Angola/data_processed/landsurfaceforms/landsurfaceforms_1km.tif\",\n",
    "    \"lithology_raster\":          \"/Volumes/One_Touch/angola_soils_thesis/GIS_Angola/data_processed/lithology_rasterized.tif\",\n",
    "    \"soil_raster\":               \"/Volumes/One_Touch/angola_soils_thesis/GIS_Angola/data_processed/angola_soil_data_raster_1km.tif\",\n",
    "}\n",
    "\n",
    "def list_tifs(folder):\n",
    "    return sorted([\n",
    "        os.path.join(folder, f)\n",
    "        for f in os.listdir(folder)\n",
    "        if f.lower().endswith(\".tif\") and not f.startswith(\"._\")\n",
    "    ])\n",
    "\n",
    "terrain_rasters = list_tifs(terrain_folder)   # 1 km continuous\n",
    "bioclim_rasters = list_tifs(bioclim_folder)   # ~18.5 km continuous\n",
    "covariate_paths = terrain_rasters + bioclim_rasters + list(categorical_rasters.values())\n",
    "\n",
    "# Hex grid parameters\n",
    "hex_size = 1000      # 1 km\n",
    "chunk_size = 2000    # hexes per processing chunk\n",
    "# # ------------------------------------------------------------------\n",
    "# 2) Load Angola boundary vector\n",
    "# ------------------------------------------------------------------\n",
    "angola_gdf = gpd.read_file(angola_boundary_path)\n",
    "angola_gdf = angola_gdf.to_crs(epsg=32733)  # ensure projected in meters\n",
    "angola_boundary = angola_gdf.unary_union     # single polygon for intersections\n",
    "angola_crs = angola_gdf.crs\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Build 1 km hexagon grid\n",
    "# ------------------------------------------------------------------\n",
    "xmin, ymin, xmax, ymax = angola_boundary.bounds\n",
    "dx = np.sqrt(3) * hex_size\n",
    "dy = 1.5 * hex_size\n",
    "hexagons = []\n",
    "x = xmin\n",
    "row = 0\n",
    "\n",
    "while x < xmax + dx:\n",
    "    y = ymin - hex_size if row % 2 else ymin\n",
    "    while y < ymax + dy:\n",
    "        hexagon = Polygon([\n",
    "            (x + hex_size * np.cos(a), y + hex_size * np.sin(a))\n",
    "            for a in np.linspace(0, 2*np.pi, 7)[:-1]\n",
    "        ])\n",
    "        # Only keep hexes that intersect Angola\n",
    "        if hexagon.intersects(angola_boundary):\n",
    "            hexagons.append(hexagon.intersection(angola_boundary))\n",
    "        y += dy\n",
    "    x += dx\n",
    "    row += 1\n",
    "\n",
    "grid_gdf = gpd.GeoDataFrame(geometry=hexagons, crs=angola_crs)\n",
    "grid_gdf[\"hex_id\"] = range(1, len(grid_gdf) + 1)\n",
    "\n",
    "# Optional: log column to flag coarse layers\n",
    "grid_gdf[\"coarse_flag\"] = \"\"\n",
    "\n",
    "print(f\"✅ Hex grid created with {len(grid_gdf)} hexagons\")\n",
    "#  ------------------------------------------------------------------\n",
    "# 4) Chunked zonal statistics\n",
    "# ------------------------------------------------------------------\n",
    "# Prepare columns\n",
    "for rpath in covariate_paths:\n",
    "    name = [k for k, v in categorical_rasters.items() if v == rpath]\n",
    "    col_name = name[0] if name else os.path.splitext(os.path.basename(rpath))[0]\n",
    "    grid_gdf[col_name] = np.nan\n",
    "\n",
    "n_chunks = int(np.ceil(len(grid_gdf) / chunk_size))\n",
    "print(f\"Processing {len(grid_gdf)} hexagons in {n_chunks} chunks...\")\n",
    "\n",
    "for i, chunk_idx in enumerate(np.array_split(grid_gdf.index, n_chunks), start=1):\n",
    "    print(f\"  Chunk {i}/{n_chunks}\")\n",
    "    subset = grid_gdf.loc[chunk_idx]\n",
    "\n",
    "    for rpath in covariate_paths:\n",
    "        name = [k for k, v in categorical_rasters.items() if v == rpath]\n",
    "        col_name = name[0] if name else os.path.splitext(os.path.basename(rpath))[0]\n",
    "\n",
    "        with rasterio.open(rpath) as src:\n",
    "            nodata = src.nodata\n",
    "\n",
    "        # ----------------------\n",
    "        # Continuous rasters\n",
    "        # ----------------------\n",
    "        if rpath in continuous_1km:\n",
    "            # 1 km or finer -> zonal mean\n",
    "            stats = zonal_stats(subset, rpath, stats=\"mean\", nodata=nodata)\n",
    "            grid_gdf.loc[chunk_idx, col_name] = [s[\"mean\"] if s else np.nan for s in stats]\n",
    "\n",
    "        elif rpath in continuous_bioclim:\n",
    "            # Coarser than hex -> take raster value at centroid\n",
    "            values = []\n",
    "            with rasterio.open(rpath) as src:\n",
    "                for geom in subset.geometry:\n",
    "                    centroid = geom.centroid\n",
    "                    row, col = src.index(centroid.x, centroid.y)\n",
    "                    val = src.read(1)[row, col]\n",
    "                    values.append(float(val) if val != nodata else np.nan)\n",
    "            grid_gdf.loc[chunk_idx, col_name] = values\n",
    "            grid_gdf.loc[chunk_idx, \"coarse_flag\"] += f\"{col_name} \"\n",
    "\n",
    "        # ----------------------\n",
    "        # Categorical rasters\n",
    "        # ----------------------\n",
    "        elif rpath in categorical_1km:\n",
    "            stats = zonal_stats(subset, rpath, categorical=True, nodata=nodata)\n",
    "            grid_gdf.loc[chunk_idx, col_name] = [\n",
    "                max(s, key=s.get) if s else np.nan for s in stats\n",
    "            ]\n",
    "\n",
    "        elif rpath in categorical_lithology:\n",
    "            values = []\n",
    "            with rasterio.open(rpath) as src:\n",
    "                for geom in subset.geometry:\n",
    "                    centroid = geom.centroid\n",
    "                    row, col = src.index(centroid.x, centroid.y)\n",
    "                    val = src.read(1)[row, col]\n",
    "                    values.append(int(val) if val != nodata else np.nan)\n",
    "            grid_gdf.loc[chunk_idx, col_name] = values\n",
    "            grid_gdf.loc[chunk_idx, \"coarse_flag\"] += f\"{col_name} \"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) Save outputs\n",
    "# ------------------------------------------------------------------\n",
    "grid_gdf.to_pickle(\"angola_SOC_prediction_grid.pkl\")\n",
    "grid_gdf.to_file(\"angola_SOC_prediction_grid.gpkg\", driver=\"GPKG\")\n",
    "grid_gdf.to_csv(\"angola_SOC_prediction_grid.csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved outputs: .pkl, .gpkg, .csv\")\n",
    "print(\"✅ Coarse raster layers per hex recorded in 'coarse_flag' column\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANNK to build SOC map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annk_workflow\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# -------------------\n",
    "# USER PATHS / SETTINGS\n",
    "# -------------------\n",
    "TRAINING_CSV = \"/Users/inesschwartz/Desktop/training_data_table_final.csv\"\n",
    "PREDICTION_GRID_GPKG = \"angola_SOC_prediction_grid.gpkg\"  # adjust if path differs\n",
    "PREDICTION_GRID_LAYER = None  # None will use default/first layer\n",
    "OUTPUT_PREFIX = \"angola_ANNK\"\n",
    "SOC_COL = \"log_soc_stock\"            # change if different\n",
    "X_COL = \"X_coord\"          # change if different\n",
    "Y_COL = \"Y_coord\"          # change if different\n",
    "RANDOM_SEED = 42\n",
    "TEST_SIZE = 1/3            # validation fraction\n",
    "# ANN hyperparameters (tune if desired)\n",
    "ANN_PARAMS = {\n",
    "    \"hidden_layer_sizes\": (100, 50),  # example; tune for your problem\n",
    "    \"activation\": \"relu\",\n",
    "    \"solver\": \"adam\",\n",
    "    \"alpha\": 1e-4,\n",
    "    \"learning_rate\": \"adaptive\",\n",
    "    \"max_iter\": 300,\n",
    "    \"early_stopping\": True,\n",
    "    \"random_state\": RANDOM_SEED,\n",
    "    \"verbose\": True\n",
    "}\n",
    "# pykrige settings\n",
    "VARIOGRAM_MODEL = \"spherical\"  # options: 'linear','power','gaussian','spherical','exponential'\n",
    "N_SEARCH = 10  # pykrige internal parameter (keeps run time reasonable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# 1) Load training data\n",
    "# -------------------\n",
    "df = pd.read_csv(TRAINING_CSV)\n",
    "required_cols = {SOC_COL, X_COL, Y_COL}\n",
    "if not required_cols.issubset(df.columns):\n",
    "    missing = required_cols - set(df.columns)\n",
    "    raise ValueError(f\"Missing required columns in training CSV: {missing}\")\n",
    "\n",
    "# identify covariate columns (exclude coords and SOC and any identifier-like columns)\n",
    "exclude = {SOC_COL, X_COL, Y_COL}\n",
    "# also exclude common ID columns if present\n",
    "for c in [\"profile\", \"district\", \"site_info_id\"]:\n",
    "    exclude.add(c)\n",
    "covariate_cols = [c for c in df.columns if c not in exclude]\n",
    "\n",
    "if len(covariate_cols) == 0:\n",
    "    raise ValueError(\"No covariate columns detected in training CSV. Update covariate detection or file.\")\n",
    "\n",
    "print(f\"Detected {len(covariate_cols)} covariates: {covariate_cols[:10]}{'...' if len(covariate_cols)>10 else ''}\")\n",
    "\n",
    "# Drop rows with no coordinates or no SOC\n",
    "df = df.dropna(subset=[SOC_COL, X_COL, Y_COL]).copy()\n",
    "\n",
    "# -------------------\n",
    "# 2) Train/validation split (2/3 train, 1/3 validation)\n",
    "# -------------------\n",
    "train_df, val_df = train_test_split(df, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "print(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# 3) Preprocessing pipeline\n",
    "#    - auto-detect numeric vs categorical covariates from training data\n",
    "# -------------------\n",
    "# choose numeric vs categorical by pandas dtypes (object/category -> categorical)\n",
    "numeric_cols = [c for c in covariate_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "categorical_cols = [c for c in covariate_cols if c not in numeric_cols]\n",
    "\n",
    "print(f\"Numeric covariates: {numeric_cols[:10]}{'...' if len(numeric_cols)>10 else ''}\")\n",
    "print(f\"Categorical covariates: {categorical_cols[:10]}{'...' if len(categorical_cols)>10 else ''}\")\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numeric_pipeline, numeric_cols),\n",
    "    (\"cat\", categorical_pipeline, categorical_cols)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "# Fit preprocessor on training data\n",
    "X_train_raw = train_df[covariate_cols]\n",
    "y_train = train_df[SOC_COL].values\n",
    "\n",
    "X_val_raw = val_df[covariate_cols]\n",
    "y_val = val_df[SOC_COL].values\n",
    "\n",
    "preprocessor.fit(X_train_raw)\n",
    "\n",
    "X_train = preprocessor.transform(X_train_raw)\n",
    "X_val = preprocessor.transform(X_val_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# 4) Train MLPRegressor (ANN)\n",
    "# -------------------\n",
    "ann = MLPRegressor(**ANN_PARAMS)\n",
    "print(\"Training ANN...\")\n",
    "ann.fit(X_train, y_train)\n",
    "\n",
    "# ANN predictions on train and validation\n",
    "yhat_train = ann.predict(X_train)\n",
    "yhat_val = ann.predict(X_val)\n",
    "\n",
    "def print_metrics(y_true, y_pred, label):\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{label} RMSE: {rmse:.4f} | R2: {r2:.4f}\")\n",
    "    return rmse, r2\n",
    "\n",
    "print_metrics(y_train, yhat_train, \"Train (ANN)\")\n",
    "print_metrics(y_val, yhat_val, \"Validation (ANN)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# 5) Compute residuals at training locations (observed - ANN_pred)\n",
    "# -------------------\n",
    "# Need ANN predictions at all training points (use the same preprocessor)\n",
    "X_train_all = preprocessor.transform(df[covariate_cols])\n",
    "df[\"ann_pred\"] = ann.predict(X_train_all)\n",
    "df[\"residual\"] = df[SOC_COL] - df[\"ann_pred\"]\n",
    "\n",
    "# Subset for kriging: use training points only (the ANN was trained on train_df, but kriging uses all available observations)\n",
    "# Option: you could use only train_df residuals or all residuals; here I will use all residuals where covariates available\n",
    "krige_df = df.dropna(subset=[\"residual\", X_COL, Y_COL]).copy()\n",
    "kx = krige_df[X_COL].values\n",
    "ky = krige_df[Y_COL].values\n",
    "kz = krige_df[\"residual\"].values\n",
    "print(f\"Fitting kriging to {len(kz)} residual points\")\n",
    "\n",
    "# -------------------\n",
    "# 6) Fit Ordinary Kriging on residuals (pykrige)\n",
    "# -------------------\n",
    "# pykrige expects 1d arrays\n",
    "# For speed, estimate variogram/model by trying default; pykrige will fit sill/range if possible.\n",
    "OK = OrdinaryKriging(\n",
    "    kx, ky, kz,\n",
    "    variogram_model=VARIOGRAM_MODEL,\n",
    "    verbose=False,\n",
    "    enable_plotting=False,\n",
    "    nlags=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# 7) Load prediction grid and predict ANN on grid covariates\n",
    "# -------------------\n",
    "grid = gpd.read_file(PREDICTION_GRID_GPKG, layer=PREDICTION_GRID_LAYER)\n",
    "print(f\"Loaded prediction grid: {len(grid)} polygons\")\n",
    "\n",
    "# compute centroids for prediction points\n",
    "grid = grid.to_crs(epsg=dem_crs_to_epsg(grid.crs) if False else grid.crs)  # no-op; keep consistent, helper below removed\n",
    "grid[\"centroid\"] = grid.geometry.centroid\n",
    "grid[\"x_centroid\"] = grid.centroid.x\n",
    "grid[\"y_centroid\"] = grid.centroid.y\n",
    "\n",
    "# Important: ensure the grid has the same covariate columns as training covariates\n",
    "missing_covs = [c for c in covariate_cols if c not in grid.columns]\n",
    "if missing_covs:\n",
    "    raise ValueError(f\"The prediction grid is missing covariate columns required for ANN: {missing_covs}\")\n",
    "\n",
    "# prepare feature matrix for grid and transform\n",
    "X_grid_raw = grid[covariate_cols]\n",
    "X_grid = preprocessor.transform(X_grid_raw)\n",
    "# ANN prediction on grid\n",
    "grid[\"ann_pred\"] = ann.predict(X_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# 8) Krige residuals to grid centroids\n",
    "# -------------------\n",
    "# pykrige's execute with 'points' returns arrays of shape (n_points,)\n",
    "xpoints = grid[\"x_centroid\"].values\n",
    "ypoints = grid[\"y_centroid\"].values\n",
    "\n",
    "print(\"Kriging residuals to grid centroids (this may take some time)...\")\n",
    "z_pred, ss = OK.execute(\"points\", xpoints, ypoints)  # z_pred: predicted residuals, ss: variance\n",
    "grid[\"kriged_residual\"] = np.array(z_pred).ravel()\n",
    "grid[\"kriging_var\"] = np.array(ss).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# 9) Final ANNK prediction (ANN prediction + kriged residual)\n",
    "# -------------------\n",
    "grid[\"soc_annk\"] = grid[\"ann_pred\"] + grid[\"kriged_residual\"]\n",
    "\n",
    "# -------------------\n",
    "# 10) Save outputs and model objects (pickle)\n",
    "# -------------------\n",
    "# Save GeoPackage with predictions\n",
    "out_gpkg = f\"{OUTPUT_PREFIX}_prediction_grid.gpkg\"\n",
    "grid.drop(columns=[\"centroid\"]).to_file(out_gpkg, driver=\"GPKG\")\n",
    "print(f\"Saved GeoPackage: {out_gpkg}\")\n",
    "\n",
    "# Save CSV with attributes (geometry as WKT)\n",
    "grid_out = grid.copy()\n",
    "grid_out[\"geometry_wkt\"] = grid_out.geometry.to_wkt()\n",
    "grid_out.drop(columns=\"geometry\").to_csv(f\"{OUTPUT_PREFIX}_prediction_grid.csv\", index=False)\n",
    "print(f\"Saved CSV: {OUTPUT_PREFIX}_prediction_grid.csv\")\n",
    "\n",
    "# Save models and preprocessor for reproducibility\n",
    "joblib.dump(ann, f\"{OUTPUT_PREFIX}_ann_model.joblib\")\n",
    "joblib.dump(preprocessor, f\"{OUTPUT_PREFIX}_preprocessor.joblib\")\n",
    "joblib.dump(OK, f\"{OUTPUT_PREFIX}_kriging_ok.joblib\")  # note: pickling pykrige objects may be large but useful\n",
    "print(\"Saved ANN model, preprocessor, and kriging object via joblib\")\n",
    "\n",
    "# optional: compute approximate validation after kriging (if we can get residual kriged back to validation points)\n",
    "# predict ANN for validation points, krige residuals at validation coords and compare final predictions to observed\n",
    "X_val_all = preprocessor.transform(val_df[covariate_cols])\n",
    "val_df = val_df.copy()\n",
    "val_df[\"ann_only\"] = ann.predict(X_val_all)\n",
    "# krige residuals at val coords\n",
    "z_val_pred, ss_val = OK.execute(\"points\", val_df[X_COL].values, val_df[Y_COL].values)\n",
    "val_df[\"kriged_residual\"] = np.array(z_val_pred).ravel()\n",
    "val_df[\"annk_pred\"] = val_df[\"ann_only\"] + val_df[\"kriged_residual\"]\n",
    "print(\"Validation performance (ANN only):\")\n",
    "print_metrics(val_df[SOC_COL].values, val_df[\"ann_only\"].values, \"Validation (ANN only)\")\n",
    "print(\"Validation performance (ANN + OK residuals):\")\n",
    "print_metrics(val_df[SOC_COL].values, val_df[\"annk_pred\"].values, \"Validation (ANNK)\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
