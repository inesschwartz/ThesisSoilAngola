{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.parser import parse\n",
    "\n",
    "#import and read data\n",
    "samples = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/AmostrasAngolaTerrario.xlsx\")\n",
    "analyses = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Horizontes Analises.xlsx\")\n",
    "morphology = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Horizontes_Morfologia.xlsx\")\n",
    "profile_loc = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Perfis_local.xlsx\")\n",
    "soil_profile = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Perfis_solo.xlsx\")\n",
    "elemental_analyses = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Data XRF Angola_inicial.xlsx\")\n",
    "#soil_type = pd.read_excel(\"/Users/inesschwartz/Desktop/Thesis/tables_soil_database/Perfis_solo.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samples table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Registo, N¬∫ Campo, Ano, Perfil, Campanha, Col√≥nia_Pais, Distrito, AmostraCrivada, Prov√≠ncia, AmostraNaoCrivada, Prateleira, Sala, Obs]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load original dataset\n",
    "samples = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/AmostrasAngolaTerrario.xlsx\")\n",
    "\n",
    "# This will show all rows that are duplicates (keep=False shows all duplicates, not just subsequent ones)\n",
    "duplicates1 = samples[samples.duplicated('Registo',keep=False)]\n",
    "print(duplicates1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for clarity\n",
    "samples.rename(columns={\n",
    "    'Registo': 'sample_id',\n",
    "    'N¬∫ Campo': 'site_info_id',\n",
    "    'Ano': 'year',\n",
    "    'Perfil': 'profile',\n",
    "    'Campanha': 'campaign',\n",
    "    'Col√≥nia_Pais': 'country',\n",
    "    'Distrito': 'district',\n",
    "    'AmostraCrivada': 'sample_sifted',\n",
    "    'AmostraNaoCrivada': 'sample_not_sifted',\n",
    "    'Prateleira': 'shelf',\n",
    "    'Sala': 'room'\n",
    "}, inplace=True)\n",
    "\n",
    "# Drop unused columns\n",
    "samples_cleaning = samples.drop(columns=[\n",
    "    'campaign', 'country', 'Prov√≠ncia', 'sample_not_sifted', 'sample_sifted', 'Obs'\n",
    "], errors='ignore')\n",
    "\n",
    "# # Add a new Primary Key ID column starting from 1\n",
    "# samples_cleaning.insert(0, 'sample_id', range(1, len(samples_cleaning) + 1))\n",
    "\n",
    "# Add empty FK columns\n",
    "samples_cleaning['lab_info_id'] = pd.NA\n",
    "samples_cleaning['horizon_id'] = pd.NA\n",
    "samples_cleaning['profile_record_id'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>year</th>\n",
       "      <th>profile</th>\n",
       "      <th>district</th>\n",
       "      <th>shelf</th>\n",
       "      <th>room</th>\n",
       "      <th>lab_info_id</th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>profile_record_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>630</td>\n",
       "      <td>172</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>139</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>631</td>\n",
       "      <td>173</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>139</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>174</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>139</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>633</td>\n",
       "      <td>175</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>139</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>687</td>\n",
       "      <td>1034</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>208</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id site_info_id    year profile district shelf room lab_info_id  \\\n",
       "0        630          172  1946.0     139   Huambo     1   22        <NA>   \n",
       "1        631          173  1946.0     139   Huambo     1   22        <NA>   \n",
       "2        632          174  1946.0     139   Huambo     1   22        <NA>   \n",
       "3        633          175  1946.0     139   Huambo     1   22        <NA>   \n",
       "4        687         1034  1946.0     208   Huambo     1   22        <NA>   \n",
       "\n",
       "  horizon_id profile_record_id  \n",
       "0       <NA>              <NA>  \n",
       "1       <NA>              <NA>  \n",
       "2       <NA>              <NA>  \n",
       "3       <NA>              <NA>  \n",
       "4       <NA>              <NA>  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_cleaning.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_id              int64\n",
       "site_info_id          object\n",
       "year                 float64\n",
       "profile               object\n",
       "district              object\n",
       "shelf                 object\n",
       "room                  object\n",
       "lab_info_id           object\n",
       "horizon_id            object\n",
       "profile_record_id     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_cleaning.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Duplicate sample_id values:\n",
      "Empty DataFrame\n",
      "Columns: [sample_id]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Ensure consistent types and formatting\n",
    "samples_cleaning['profile'] = samples_cleaning['profile'].astype(str).str.replace('/', '_').str.strip().str[:20]\n",
    "samples_cleaning['shelf'] = samples_cleaning['shelf'].astype(\"string\")\n",
    "\n",
    "#formatting site_info_id\n",
    "samples_cleaning['site_info_id'] = samples_cleaning['site_info_id'].astype(str).str.replace('/', '_').str.strip().str[:20]\n",
    "\n",
    "# Convert sample_id to string (no truncation unless necessary)\n",
    "samples_cleaning['sample_id'] = samples_cleaning['sample_id'].astype(str).str.strip()\n",
    "\n",
    "# Check for duplicates AFTER conversion\n",
    "duplicate_ids = samples_cleaning[samples_cleaning.duplicated('sample_id', keep=False)]\n",
    "print(\"üîç Duplicate sample_id values:\")\n",
    "print(duplicate_ids[['sample_id']])\n",
    "\n",
    "samples_cleaning1 = samples_cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking which profiles match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>profile_record_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>shelf</th>\n",
       "      <th>room</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>630</td>\n",
       "      <td>172</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1946.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>631</td>\n",
       "      <td>173</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1946.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>174</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1946.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>633</td>\n",
       "      <td>175</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1946.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>687</td>\n",
       "      <td>1034</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>208</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1946.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id site_info_id profile_record_id profile horizon_id shelf room  \\\n",
       "0       630          172              <NA>     139       <NA>     1   22   \n",
       "1       631          173              <NA>     139       <NA>     1   22   \n",
       "2       632          174              <NA>     139       <NA>     1   22   \n",
       "3       633          175              <NA>     139       <NA>     1   22   \n",
       "4       687         1034              <NA>     208       <NA>     1   22   \n",
       "\n",
       "     year  \n",
       "0  1946.0  \n",
       "1  1946.0  \n",
       "2  1946.0  \n",
       "3  1946.0  \n",
       "4  1946.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reorder to match DB schema\n",
    "samples_check = samples_cleaning1[[\n",
    "    'sample_id',\n",
    "    'site_info_id',\n",
    "    'profile_record_id',\n",
    "    'profile',\n",
    "    'horizon_id',\n",
    "    'shelf',\n",
    "    'room',\n",
    "    'year'\n",
    "]]\n",
    "\n",
    "samples_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_id                    object\n",
       "site_info_id                 object\n",
       "profile_record_id            object\n",
       "profile                      object\n",
       "horizon_id                   object\n",
       "shelf                string[python]\n",
       "room                         object\n",
       "year                        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_check.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples_check.to_csv(\"/Users/inesschwartz/Desktop/samples_check.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab_Info  table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Amostra', 'ID', 'Morfo_id', 'Analise-id', 'PERFIL', 'GR', 'COD_PROV',\n",
       "       'NA', 'LS', 'LI', 'EG', 'AG', 'AF', 'L', 'Argila', 'Eq_Hum', 'atm_1/3',\n",
       "       'atm_15', 'CACO3', 'Gesso', 'Fe livre', 'CO', 'N total', 'C/N', 'MO',\n",
       "       'P205 total', 'pH (H2O)', 'pH(KCL)', 'Ca++', 'Mg++', 'K+', 'Na+',\n",
       "       'Soma de bases', 'CTC', 'V', 'Cloretos', 'Sulfatos', 'Condutividade',\n",
       "       'S√≥dio sol√∫vel', 'EqMol (SiO2)', 'EqMol(Al2O3)', 'EqMol(Fe2O3)',\n",
       "       'SiO2/Al2O3', 'SiO2/Fe2O3', 'SiO2/R2O3', 'Fe2O3/Al2O3', 'FE2O3_TARG',\n",
       "       'FE2O3_LARG', 'CEC_ARG', 'Min_<0,002', 'Min_0,05-0,02', 'Min_0,2-0,05',\n",
       "       'Min_2-0,2', 'Confirmar'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyses.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change column names\n",
    "analyses.rename(columns={\n",
    "    'Amostra': 'sample_id',\n",
    "    'Morfo_id': 'horizon_id',\n",
    "    'Analise-id': 'analysis_id',\n",
    "    'PERFIL': 'profile', #change to reference PK profile_record_id\n",
    "    'LS': 'upper_limit',\n",
    "    'LI': 'lower_limit',\n",
    "    'EG': 'EG',\n",
    "    'AG': 'thick_clay',\n",
    "    'AF': 'fine_clay',\n",
    "    'Argila': 'clay',\n",
    "    'L': 'silt',\n",
    "    'Gesso': 'gypsum',\n",
    "    'Fe livre': 'free_iron',\n",
    "    'CO':'organic_carbon',\n",
    "    'N total': 'total_N',\n",
    "    'MO': 'OM',\n",
    "    'Soma de bases': 'exchangable_bases_sum',\n",
    "    'CTC': 'CEC',\n",
    "    'Cloretos':'chlorides',\n",
    "    'Sulfatos':'sulfates',\n",
    "    'Condutividade':'conductivity',\n",
    "    'S√≥dio sol√∫vel':'soluble_sodium',\n",
    "    'P205 total': 'P205',\n",
    "    'pH (H2O)':'pH_H2O',\n",
    "    'pH(KCL)':'pH_KCL',\n",
    "    'MO': 'organic_material',\n",
    "    'atm_1/3': 'atm_1_3',\n",
    "    'Ca++': 'Ca',\n",
    "    'Mg++': 'Mg',\n",
    "    'Na+': 'Na',\n",
    "    'K+': 'K',\n",
    "    'Min_<0,002': 'Min_lt_0002',\n",
    "    'Min_0,05-0,02': 'Min_005_002',\n",
    "    'Min_0,2-0,05': 'Min_02_005',\n",
    "    'Min_2-0,2': 'Min_2_02',\n",
    "}, inplace=True)\n",
    "\n",
    "# Drop columns not needed\n",
    "analyses_drop = analyses.drop([\n",
    "    'EqMol (SiO2)', 'EqMol(Al2O3)', 'EqMol(Fe2O3)', 'SiO2/Al2O3', \n",
    "    'SiO2/Fe2O3', 'SiO2/R2O3', 'Fe2O3/Al2O3', 'FE2O3_TARG', \n",
    "    'FE2O3_LARG', 'CEC_ARG', 'GR', 'COD_PROV'\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new Primary Key ID column starting from 1\n",
    "#drop lab_sample_id if already exist error comes up\n",
    "# Check if 'lab_sample_id' exists and drop it\n",
    "if 'lab_sample_id' in analyses_drop.columns:\n",
    "    analyses_drop = analyses_drop.drop(columns=['lab_sample_id'])\n",
    "\n",
    "# Insert a new column, e.g., 'lab_sample_id' as a primary key starting from 1\n",
    "analyses_drop.insert(0, 'lab_sample_id', range(1, len(analyses_drop) + 1))\n",
    "\n",
    "\n",
    "#add minerology_id column\n",
    "analyses_drop['minerology_id'] = pd.NA\n",
    "#add soil_biology_id column (just in case)\n",
    "analyses_drop['soil_biology_id'] = pd.NA\n",
    "\n",
    "\n",
    "#Define the desired final column order\n",
    "final_columns = [\n",
    "    'lab_sample_id',\n",
    "    'ID',\n",
    "    'analysis_id',\n",
    "    'horizon_id',\n",
    "    'sample_id',\n",
    "    'profile',\n",
    "    'minerology_id',\n",
    "    'soil_biology_id',\n",
    "    'EG',\n",
    "    'thick_clay',\n",
    "    'fine_clay',\n",
    "    'silt',\n",
    "    'clay',\n",
    "    'Eq_Hum',\n",
    "    'atm_1_3',\n",
    "    'atm_15',\n",
    "    'CACO3',\n",
    "    'gypsum',\n",
    "    'free_iron',\n",
    "    'organic_carbon',\n",
    "    'total_N',\n",
    "    'P205',\n",
    "    'organic_material',\n",
    "    'pH_H2O',\n",
    "    'pH_KCL',\n",
    "    'Ca',\n",
    "    'Mg',\n",
    "    'Na',\n",
    "    'K',\n",
    "    'exchangable_bases_sum',\n",
    "    'CEC',\n",
    "    'V',\n",
    "    'conductivity',\n",
    "    'soluble_sodium',\n",
    "    'Min_lt_0002',\n",
    "    'Min_005_002',\n",
    "    'Min_02_005',\n",
    "    'Min_2_02',\n",
    "]\n",
    "\n",
    "analyses1 = analyses_drop[final_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Columns with NaN values:\n",
      "sample_id                  19\n",
      "minerology_id            7847\n",
      "soil_biology_id          7847\n",
      "EG                       4910\n",
      "thick_clay                448\n",
      "fine_clay                 390\n",
      "silt                      430\n",
      "clay                      420\n",
      "Eq_Hum                   2297\n",
      "atm_1_3                  7469\n",
      "atm_15                   5689\n",
      "CACO3                    7334\n",
      "gypsum                   7815\n",
      "free_iron                2792\n",
      "organic_carbon           2395\n",
      "total_N                  6259\n",
      "P205                     5464\n",
      "organic_material         2413\n",
      "pH_H2O                    684\n",
      "pH_KCL                   1749\n",
      "Ca                       3018\n",
      "Mg                       3025\n",
      "Na                       3024\n",
      "K                        2995\n",
      "exchangable_bases_sum    6740\n",
      "CEC                      2423\n",
      "V                        2476\n",
      "conductivity             7201\n",
      "soluble_sodium           1270\n",
      "Min_lt_0002              7517\n",
      "Min_005_002              7719\n",
      "Min_02_005               7723\n",
      "Min_2_02                 7810\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#address nulls \n",
    "# Show count of NaN values in each column\n",
    "nan_counts = analyses1.isna().sum()\n",
    "\n",
    "# Filter to show only columns with at least one NaN\n",
    "nan_columns = nan_counts[nan_counts > 0]\n",
    "\n",
    "# Print them\n",
    "print(\"üîç Columns with NaN values:\")\n",
    "print(nan_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting and checking for duplicates\n",
    "# Make a safe copy\n",
    "analyses1 = analyses_drop[final_columns].copy()\n",
    "\n",
    "# Convert sample_id to numeric, coercing errors to NaN\n",
    "analyses1['sample_id'] = pd.to_numeric(analyses1['sample_id'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN in sample_id to avoid int conversion error\n",
    "analyses1 = analyses1.dropna(subset=['sample_id'])\n",
    "\n",
    "# Convert to int, then to str\n",
    "analyses1['sample_id'] = analyses1['sample_id'].astype(int).astype(str)\n",
    "\n",
    "# Clean the profile column\n",
    "analyses1['profile'] = (\n",
    "    analyses1['profile']\n",
    "    .astype(str)\n",
    "    .str.replace('/', '_')\n",
    "    .str.strip()\n",
    "    .str[:20]\n",
    ")\n",
    "\n",
    "# Alias to another variable\n",
    "analyses2 = analyses1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10337' '11608' '3497' '3498' '2923' '2924' '2925' '2926' '2927' '2928'\n",
      " '2929' '2930' '16355' '16356' '419' '420' '15682' '13817' '75' '13965'\n",
      " '3418' '9230' '8639' '3529']\n"
     ]
    }
   ],
   "source": [
    "# Filter out and print duplicated samples (including the first occurrence)\n",
    "duplicated_values = analyses2['sample_id'][analyses2['sample_id'].duplicated()].unique()\n",
    "print(duplicated_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sample ID corrections applied.\n"
     ]
    }
   ],
   "source": [
    "# 1. Ensure sample_id and analysis_id are strings\n",
    "analyses2['sample_id'] = analyses2['sample_id'].astype(str).str.strip()\n",
    "analyses2['analysis_id'] = analyses2['analysis_id'].astype(str).str.strip()\n",
    "\n",
    "# 2. Correct duplicates for sample_id '11608'\n",
    "mask_11608 = analyses2['sample_id'] == '11608'\n",
    "analyses2.loc[mask_11608, 'sample_id'] = [\n",
    "    f\"11608_{i+1}\" for i in range(mask_11608.sum())\n",
    "]\n",
    "\n",
    "# 3. Remove duplicate for sample_id '3497' (keep first occurrence only)\n",
    "duplicates_3497 = analyses2[analyses2['sample_id'] == '3497']\n",
    "if len(duplicates_3497) > 1:\n",
    "    indices_to_drop = duplicates_3497.index[1:]\n",
    "    analyses2 = analyses2.drop(indices_to_drop)\n",
    "\n",
    "# 4. Correct sample_id ‚Üí analysis_id assignments for specific sample_ids\n",
    "id_map = {\n",
    "    'UZ_24c/60_1_1': '8639',\n",
    "    'UZ_24c/60_2_1': '8640',\n",
    "    'UZ_24c/60_3_1': '8641',\n",
    "    'UZ_24c/60_4_1': '8642',\n",
    "    'UZ_24c/60_5_1': '8643'\n",
    "}\n",
    "\n",
    "# Apply mapping to update analysis_id based on sample_id\n",
    "analyses2['analysis_id'] = analyses2.apply(\n",
    "    lambda row: id_map.get(row['sample_id'], row['analysis_id']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Sample ID corrections applied.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóë Dropped second occurrence of sample_id 2923\n",
      "üóë Dropped second occurrence of sample_id 2924\n",
      "üóë Dropped second occurrence of sample_id 2925\n",
      "üóë Dropped second occurrence of sample_id 2926\n",
      "üóë Dropped second occurrence of sample_id 2927\n",
      "üóë Dropped second occurrence of sample_id 2928\n",
      "üóë Dropped second occurrence of sample_id 2929\n",
      "üóë Dropped second occurrence of sample_id 2930\n"
     ]
    }
   ],
   "source": [
    "#drop unwanted/un-needed duplicates or erroneous reccords\n",
    "\n",
    "# Drop second row where sample_id == '3497'\n",
    "indices = analyses2.index[analyses2['sample_id'] == '3497'].tolist()\n",
    "if len(indices) > 1:\n",
    "    analyses2 = analyses2.drop(indices[1])\n",
    "\n",
    "# Drop second row where sample_id == '3497'\n",
    "indices = analyses2.index[analyses2['sample_id'] == '13817'].tolist()\n",
    "if len(indices) > 1:\n",
    "    analyses2 = analyses2.drop(indices[1])\n",
    "\n",
    "# drop second row where sample_id == '8639'\n",
    "indices = analyses2.index[analyses2['sample_id'] == '8639'].tolist()\n",
    "if len(indices) > 1:\n",
    "    analyses2 = analyses2.drop(indices[1])\n",
    "\n",
    "#Drop second row where sample_id =='2923-2930'\n",
    "# Ensure sample_id is string for consistent comparison\n",
    "analyses2['sample_id'] = analyses2['sample_id'].astype(str)\n",
    "\n",
    "# Define the sample_ids to clean duplicates for\n",
    "target_ids = [str(i) for i in range(2923, 2931)]\n",
    "\n",
    "# Identify and drop the second occurrence of each\n",
    "for sid in target_ids:\n",
    "    matches = analyses2.index[analyses2['sample_id'] == sid].tolist()\n",
    "    if len(matches) > 1:\n",
    "        # Drop the second occurrence (index 1 in the list)\n",
    "        analyses2 = analyses2.drop(matches[1])\n",
    "        print(f\"üóë Dropped second occurrence of sample_id {sid}\")\n",
    "\n",
    "\n",
    "# Ensure sample_id is string for correct matching\n",
    "analyses2['sample_id'] = analyses2['sample_id'].astype(str)\n",
    "\n",
    "# List of sample_ids to drop (as strings)\n",
    "sample_ids_to_drop = [\n",
    "    '4247', '4248', '4249', '4250', '4251', '4252', '4253', '4254',  # invalid samples\n",
    "    '5533', '6636', '6637', '6638', '6639',\n",
    "    '6875', '7361',\n",
    "    '3012', '3013', '3014', '3015', '3016',\n",
    "    '3049', '3050', '3051', '3052', '3053', '3054', '419', '420', '421', '422'\n",
    "]\n",
    "\n",
    "# Drop rows where sample_id is in the list\n",
    "analyses2 = analyses2[~analyses2['sample_id'].isin(sample_ids_to_drop)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Manual sample_id corrections applied.\n",
      "Remaining rows: 7798\n",
      "Missing sample_id entries: 0\n"
     ]
    }
   ],
   "source": [
    "# Ensure both 'lab_sample_id' and 'sample_id' columns exist\n",
    "assert 'lab_sample_id' in analyses2.columns, \"Missing column: lab_sample_id\"\n",
    "assert 'sample_id' in analyses2.columns, \"Missing column: sample_id\"\n",
    "\n",
    "# Step 1: Ensure lab_sample_id is string so comparison works\n",
    "analyses2['lab_sample_id'] = analyses2['lab_sample_id'].astype(str)\n",
    "\n",
    "# Step 2: Manual corrections mapping\n",
    "manual_corrections = {\n",
    "    '66': 10376,\n",
    "    '4219': 3497,\n",
    "    '4221': 3498,   # same sample, different depths\n",
    "    '4222': 3499,\n",
    "    '4747': 16255,\n",
    "    '4748': 16256,\n",
    "    '5010': 15862,\n",
    "    '6802': 13695,\n",
    "    '7255': 8230,\n",
    "    '7686': 8529,\n",
    "    '6875': 3419,\n",
    "    '6638': 75_1,\n",
    "}\n",
    "\n",
    "# Step 3: Apply manual corrections\n",
    "for lab_id, sample_id in manual_corrections.items():\n",
    "    matches = analyses2['lab_sample_id'] == lab_id\n",
    "    if matches.sum() == 0:\n",
    "        print(f\"‚ö†Ô∏è Warning: lab_sample_id {lab_id} not found in data.\")\n",
    "    else:\n",
    "        analyses2.loc[matches, 'sample_id'] = sample_id\n",
    "\n",
    "# Step 4: Summary after correction\n",
    "print(\"‚úÖ Manual sample_id corrections applied.\")\n",
    "print(f\"Remaining rows: {len(analyses2)}\")\n",
    "print(f\"Missing sample_id entries: {analyses2['sample_id'].isna().sum()}\")\n",
    "\n",
    "# Optional: Show which rows are still missing sample_id\n",
    "missing_sample_ids = analyses2[analyses2['sample_id'].isna()]\n",
    "if not missing_sample_ids.empty:\n",
    "    print(\"üîç Rows with missing sample_id after correction:\")\n",
    "    print(missing_sample_ids[['lab_sample_id', 'sample_id']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Missing sample_id in 0 rows.\n",
      "üîç lab_sample_id values with missing sample_id:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Find rows with missing sample_id\n",
    "missing_sample_id_rows = analyses2[analyses2['sample_id'].isna()]\n",
    "\n",
    "# Print how many are missing\n",
    "print(f\"‚ùå Missing sample_id in {len(missing_sample_id_rows)} rows.\")\n",
    "\n",
    "# Show the unique lab_sample_id values that are missing a sample_id\n",
    "print(\"üîç lab_sample_id values with missing sample_id:\")\n",
    "print(missing_sample_id_rows['lab_sample_id'].unique())\n",
    "\n",
    "# Optional: Save to CSV for review\n",
    "#missing_sample_id_rows.to_csv(\"missing_sample_ids.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN sample_id rows: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"NaN sample_id rows:\", analyses2['sample_id'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing sample_id (NaN only)\n",
    "analyses2 = analyses2[analyses2['sample_id'].notna()].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Updated analyses2 has 7798 rows.\n"
     ]
    }
   ],
   "source": [
    "print(f\"‚úÖ Updated analyses2 has {len(analyses2)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses3 = analyses2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab_sample_id</th>\n",
       "      <th>ID</th>\n",
       "      <th>analysis_id</th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>minerology_id</th>\n",
       "      <th>soil_biology_id</th>\n",
       "      <th>EG</th>\n",
       "      <th>thick_clay</th>\n",
       "      <th>...</th>\n",
       "      <th>K</th>\n",
       "      <th>exchangable_bases_sum</th>\n",
       "      <th>CEC</th>\n",
       "      <th>V</th>\n",
       "      <th>conductivity</th>\n",
       "      <th>soluble_sodium</th>\n",
       "      <th>Min_lt_0002</th>\n",
       "      <th>Min_005_002</th>\n",
       "      <th>Min_02_005</th>\n",
       "      <th>Min_2_02</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>10999</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.700001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.83</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>11000</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.799999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.98</td>\n",
       "      <td>10.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>11001</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.62</td>\n",
       "      <td>5.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>11002</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.599998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.91</td>\n",
       "      <td>8.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>B_101/62_5_21</td>\n",
       "      <td>B_101/62_5_2</td>\n",
       "      <td>11003</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.799999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.04</td>\n",
       "      <td>7.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  lab_sample_id  ID    analysis_id    horizon_id sample_id profile  \\\n",
       "0             1  21   B_101/62_1_1  B_101/62_1_1     10999  101_62   \n",
       "1             2  22   B_101/62_2_1  B_101/62_2_1     11000  101_62   \n",
       "2             3  23   B_101/62_3_1  B_101/62_3_1     11001  101_62   \n",
       "3             4  24   B_101/62_4_1  B_101/62_4_1     11002  101_62   \n",
       "4             5  25  B_101/62_5_21  B_101/62_5_2     11003  101_62   \n",
       "\n",
       "  minerology_id soil_biology_id  EG  thick_clay  ...     K  \\\n",
       "0          <NA>            <NA> NaN   61.700001  ...  0.03   \n",
       "1          <NA>            <NA> NaN   52.799999  ...  0.03   \n",
       "2          <NA>            <NA> NaN   42.500000  ...  0.01   \n",
       "3          <NA>            <NA> NaN   42.599998  ...  0.01   \n",
       "4          <NA>            <NA> NaN   36.799999  ...  0.01   \n",
       "\n",
       "   exchangable_bases_sum   CEC     V  conductivity  soluble_sodium  \\\n",
       "0                    NaN  1.83  23.0           NaN             0.0   \n",
       "1                    NaN  1.98  10.6           NaN             0.0   \n",
       "2                    NaN  1.62   5.6           NaN             0.0   \n",
       "3                    NaN  0.91   8.8           NaN             0.0   \n",
       "4                    NaN  1.04   7.7           NaN             0.0   \n",
       "\n",
       "   Min_lt_0002  Min_005_002  Min_02_005  Min_2_02  \n",
       "0          NaN          NaN         NaN       NaN  \n",
       "1          NaN          NaN         NaN       NaN  \n",
       "2          NaN          NaN         NaN       NaN  \n",
       "3          NaN          NaN         NaN       NaN  \n",
       "4          NaN          NaN         NaN       NaN  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyses3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Filter out and print duplicated samples (including the first occurrence)\n",
    "duplicated_values = analyses3['sample_id'][analyses3['sample_id'].duplicated()].unique()\n",
    "print(duplicated_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Number of null-like sample_id values: 0\n",
      "‚ö†Ô∏è Unique null-like sample_id values found:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#double checking null sample_id's\n",
    "# Step 1: Ensure sample_id is a string for consistent comparison\n",
    "analyses3['sample_id'] = analyses3['sample_id'].astype(str).str.strip()\n",
    "\n",
    "# Step 2: Define mask for all \"null-like\" values\n",
    "null_like_mask = analyses3['sample_id'].isin(['', '0', 'NULL', 'NaN', 'nan', 'None']) | analyses3['sample_id'].isna()\n",
    "\n",
    "# Step 3: Count and list unique problematic values\n",
    "null_sample_ids = analyses3[null_like_mask]['sample_id'].unique()\n",
    "null_count = null_like_mask.sum()\n",
    "\n",
    "# Step 4: Output results\n",
    "print(f\"üîç Number of null-like sample_id values: {null_count}\")\n",
    "print(\"‚ö†Ô∏è Unique null-like sample_id values found:\")\n",
    "print(null_sample_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## joining elemental analyses to analyses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Found 0 rows with NaN sample_id:\n",
      "Empty DataFrame\n",
      "Columns: [sample_id, Soil Profile, field_sample_code, Code.1, Depht, Sampling Date, Mg, Al, Si, P, S, Cl, K, Ca, Ti, V, Cr, Mn, Fe, Co, Ni, Cu, Zn, As, Se, Rb, Sr, Zr, Nb, Mo, Cd, Sn, Sb, Ba, Ta, W, Pt, Au, Hg, Tl, Pb, Bi, Th, U]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "# Rename and clean sample_id as before\n",
    "elemental_analyses.rename(columns={'Lab. Code': 'sample_id'}, inplace=True)\n",
    "\n",
    "# Convert to string to clean text\n",
    "sample_id_cleaned = elemental_analyses['sample_id'].astype(str)\n",
    "sample_id_cleaned = sample_id_cleaned.str.replace('C-', '', regex=False)\n",
    "sample_id_cleaned = sample_id_cleaned.str.replace(' MNL', '', regex=False)\n",
    "\n",
    "# Convert to numeric with coercion (NaNs will appear here)\n",
    "elemental_analyses['sample_id'] = pd.to_numeric(sample_id_cleaned, errors='coerce')\n",
    "\n",
    "# üîç Show rows where sample_id is NaN\n",
    "nan_sample_ids = elemental_analyses[elemental_analyses['sample_id'].isna()]\n",
    "print(f\"‚ùå Found {len(nan_sample_ids)} rows with NaN sample_id:\")\n",
    "print(nan_sample_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'Lab. Code' to 'sample_id'\n",
    "elemental_analyses.rename(columns={'Lab. Code': 'sample_id'}, inplace=True)\n",
    "\n",
    "# Temporarily convert to string to do the replacements\n",
    "sample_id_cleaned = elemental_analyses['sample_id'].astype(str)\n",
    "sample_id_cleaned = sample_id_cleaned.str.replace('C-', '', regex=False)\n",
    "sample_id_cleaned = sample_id_cleaned.str.replace(' MNL', '', regex=False)\n",
    "\n",
    "# Convert cleaned values to numeric safely\n",
    "elemental_analyses['sample_id'] = pd.to_numeric(sample_id_cleaned, errors='coerce')\n",
    "\n",
    "\n",
    "# Convert to integer and then string for uniformity\n",
    "elemental_analyses['sample_id'] = elemental_analyses['sample_id'].astype(int).astype(str)\n",
    "\n",
    "# Rename 'Code' to 'field_sample_code'\n",
    "elemental_analyses.rename(columns={'Code': 'field_sample_code'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses2['sample_id'] = analyses3['sample_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "print (analyses3['sample_id'].dtype)\n",
    "print (elemental_analyses['sample_id'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert sample_id to string (no truncation unless necessary)\n",
    "elemental_analyses['sample_id'] = elemental_analyses['sample_id'].astype(str).str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample IDs in elemental_analyses not found in analyses2:\n",
      "['16105' '16217' '16033' '16231' '16208' '16274' '16225' '15731' '16078'\n",
      " '15786' '15693' '15678' '15959' '16372' '16418' '16459' '16477' '15498'\n",
      " '15437' '12749' '13109' '14337' '14319' '15484' '15508' '15614' '15580'\n",
      " '15582' '15606' '15463' '14496' '11343' '17892' '16976' '17269' '18248'\n",
      " '16946' '17686' '17728' '17415' '17642' '18212' '17016' '17282' '18429'\n",
      " '17340' '18824' '18448' '8721' '7313' '5502' '5392' '7317' '8626' '7193'\n",
      " '7254' '7293' '8240' '15397' '14403' '14983' '16126' '15059' '15322'\n",
      " '15268' '14808' '15106']\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "# Check which sample_ids in elemental_analyses are NOT in analyses_ready\n",
    "missing_ids = elemental_analyses[~elemental_analyses['sample_id'].isin(analyses3['sample_id'])]\n",
    "\n",
    "# Display them\n",
    "print(\"Sample IDs in elemental_analyses not found in analyses2:\")\n",
    "print(missing_ids['sample_id'].unique())\n",
    "\n",
    "print(len(missing_ids['sample_id'].unique()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEFT JOIN ELEMENTAL ANALYSE TO ANALYSE TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab_sample_id</th>\n",
       "      <th>ID</th>\n",
       "      <th>analysis_id</th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>minerology_id</th>\n",
       "      <th>soil_biology_id</th>\n",
       "      <th>EG</th>\n",
       "      <th>thick_clay</th>\n",
       "      <th>...</th>\n",
       "      <th>Ta</th>\n",
       "      <th>W</th>\n",
       "      <th>Pt</th>\n",
       "      <th>Au</th>\n",
       "      <th>Hg</th>\n",
       "      <th>Tl</th>\n",
       "      <th>Pb</th>\n",
       "      <th>Bi</th>\n",
       "      <th>Th</th>\n",
       "      <th>U</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>10999</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.700001</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>11000</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.799999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>11001</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>11002</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.599998</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>B_101/62_5_21</td>\n",
       "      <td>B_101/62_5_2</td>\n",
       "      <td>11003</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.799999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  lab_sample_id  ID    analysis_id    horizon_id sample_id profile  \\\n",
       "0             1  21   B_101/62_1_1  B_101/62_1_1     10999  101_62   \n",
       "1             2  22   B_101/62_2_1  B_101/62_2_1     11000  101_62   \n",
       "2             3  23   B_101/62_3_1  B_101/62_3_1     11001  101_62   \n",
       "3             4  24   B_101/62_4_1  B_101/62_4_1     11002  101_62   \n",
       "4             5  25  B_101/62_5_21  B_101/62_5_2     11003  101_62   \n",
       "\n",
       "  minerology_id soil_biology_id  EG  thick_clay  ...  Ta   W  Pt  Au  Hg  Tl  \\\n",
       "0          <NA>            <NA> NaN   61.700001  ... NaN NaN NaN NaN NaN NaN   \n",
       "1          <NA>            <NA> NaN   52.799999  ... NaN NaN NaN NaN NaN NaN   \n",
       "2          <NA>            <NA> NaN   42.500000  ... NaN NaN NaN NaN NaN NaN   \n",
       "3          <NA>            <NA> NaN   42.599998  ... NaN NaN NaN NaN NaN NaN   \n",
       "4          <NA>            <NA> NaN   36.799999  ... NaN NaN NaN NaN NaN NaN   \n",
       "\n",
       "   Pb  Bi  Th   U  \n",
       "0 NaN NaN NaN NaN  \n",
       "1 NaN NaN NaN NaN  \n",
       "2 NaN NaN NaN NaN  \n",
       "3 NaN NaN NaN NaN  \n",
       "4 NaN NaN NaN NaN  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged1 = analyses3.merge(elemental_analyses, on='sample_id', how='left')\n",
    "merged1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     lab_sample_id    ID    analysis_id     horizon_id sample_id profile  \\\n",
      "1361          1362  8471  Cb_134/59_6_1  Cb_134/59_6_1      6940  134_59   \n",
      "\n",
      "     minerology_id soil_biology_id  EG  thick_clay  ...   K  \\\n",
      "1361          <NA>            <NA> NaN        31.1  ... NaN   \n",
      "\n",
      "      exchangable_bases_sum  CEC   V  conductivity  soluble_sodium  \\\n",
      "1361                    NaN  NaN NaN           NaN             0.0   \n",
      "\n",
      "      Min_lt_0002  Min_005_002  Min_02_005  Min_2_02  \n",
      "1361          NaN          NaN         NaN       NaN  \n",
      "\n",
      "[1 rows x 38 columns]\n",
      "   sample_id Soil Profile field_sample_code Code.1      Depht Sampling Date  \\\n",
      "80      6940      P134/59          A-320/59  S-492  1.30-1.70    1954-08-06   \n",
      "\n",
      "             Mg             Al             Si      P  ...    Ta   W  Pt  Au  \\\n",
      "80  1457.666667  106266.333333  308859.666667  468.0  ...  26.0 NaN NaN NaN   \n",
      "\n",
      "           Hg   Tl         Pb  Bi         Th   U  \n",
      "80  10.666667  4.0  25.333333 NaN  12.333333 NaN  \n",
      "\n",
      "[1 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "print(analyses3[analyses3['sample_id'] == '6940'])\n",
    "print(elemental_analyses[elemental_analyses['sample_id'] == '6940'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of each column in merged1:\n",
      "\n",
      "lab_sample_id     object\n",
      "analysis_id       object\n",
      "horizon_id        object\n",
      "sample_id         object\n",
      "profile           object\n",
      "                  ...   \n",
      "Tl               float64\n",
      "Pb               float64\n",
      "Bi               float64\n",
      "Th               float64\n",
      "U                float64\n",
      "Length: 71, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Define final column list\n",
    "final_columns = [\n",
    "    'lab_sample_id', 'analysis_id', 'horizon_id','sample_id', 'profile', 'EG', 'thick_clay', 'fine_clay', 'silt', 'clay', 'Eq_Hum', 'atm_1/3', 'atm_15',\n",
    "    'CACO3', 'gypsum', 'free_iron', 'organic_carbon', 'total_N', 'P205', 'organic_material', 'pH_H2O', 'pH_KCL',\n",
    "    'Ca++', 'Mg++', 'Na+', 'K+', 'exchangable_bases_sum', 'CEC', 'V', 'conductivity', 'soluble_sodium', 'Min_<0,002',\n",
    "    'Min_0,05-0,02', 'Min_0,2-0,05', 'Min_2-0,2', \n",
    "    'field_sample_code', 'Depth', 'Al', 'Si', 'P', 'S', 'Cl', 'Ti', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn',\n",
    "    'As', 'Se', 'Rb', 'Sr', 'Zr', 'Nb', 'Mo', 'Cd', 'Sn', 'Sb', 'Ba', 'Ta', 'W', 'Pt', 'Au', 'Hg', 'Tl', 'Pb', 'Bi',\n",
    "    'Th', 'U'\n",
    "]\n",
    "\n",
    "# Ensure all columns exist in the DataFrame\n",
    "for col in final_columns:\n",
    "    if col not in merged1.columns:\n",
    "        merged1[col] = pd.NA  # or np.nan if preferred\n",
    "\n",
    "# Reorder DataFrame columns\n",
    "merged1 = merged1[final_columns]\n",
    "\n",
    "# Check and print the datatypes of each column\n",
    "print(\"Data types of each column in merged1:\\n\")\n",
    "print(merged1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check/Change datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab_sample_id     object\n",
      "analysis_id       object\n",
      "horizon_id        object\n",
      "sample_id         object\n",
      "profile           object\n",
      "                  ...   \n",
      "Tl               float64\n",
      "Pb               float64\n",
      "Bi               float64\n",
      "Th               float64\n",
      "U                float64\n",
      "Length: 71, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Change PK (sample_id) to a consistent datatype: Converted to string and truncate to 20 characters (similar to VARCHAR(20))\n",
    "merged1['sample_id'] = merged1['sample_id'].astype(str).str.strip().str[:20]\n",
    "merged1['lab_sample_id'] = merged1['lab_sample_id'].astype(str).str.strip().str[:20]\n",
    "\n",
    "print(merged1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and/or add FK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'lab_sample_id' exists and drop it\n",
    "if 'lab_sample_id' in merged1.columns:\n",
    "    merged1 = merged1.drop(columns=['lab_sample_id'])\n",
    "\n",
    "# Insert a new column, e.g., 'lab_sample_id' as a primary key starting from 1\n",
    "merged1.insert(0, 'lab_sample_id', range(1, len(merged1) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab_sample_id</th>\n",
       "      <th>analysis_id</th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>EG</th>\n",
       "      <th>thick_clay</th>\n",
       "      <th>fine_clay</th>\n",
       "      <th>silt</th>\n",
       "      <th>clay</th>\n",
       "      <th>...</th>\n",
       "      <th>Ta</th>\n",
       "      <th>W</th>\n",
       "      <th>Pt</th>\n",
       "      <th>Au</th>\n",
       "      <th>Hg</th>\n",
       "      <th>Tl</th>\n",
       "      <th>Pb</th>\n",
       "      <th>Bi</th>\n",
       "      <th>Th</th>\n",
       "      <th>U</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>10999</td>\n",
       "      <td>101_62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.700001</td>\n",
       "      <td>32.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>11000</td>\n",
       "      <td>101_62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.799999</td>\n",
       "      <td>35.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>11.4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>11001</td>\n",
       "      <td>101_62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>46.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>11.1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>11002</td>\n",
       "      <td>101_62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.599998</td>\n",
       "      <td>41.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15.4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B_101/62_5_21</td>\n",
       "      <td>B_101/62_5_2</td>\n",
       "      <td>11003</td>\n",
       "      <td>101_62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.799999</td>\n",
       "      <td>47.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>14.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lab_sample_id    analysis_id    horizon_id sample_id profile  EG  \\\n",
       "0              1   B_101/62_1_1  B_101/62_1_1     10999  101_62 NaN   \n",
       "1              2   B_101/62_2_1  B_101/62_2_1     11000  101_62 NaN   \n",
       "2              3   B_101/62_3_1  B_101/62_3_1     11001  101_62 NaN   \n",
       "3              4   B_101/62_4_1  B_101/62_4_1     11002  101_62 NaN   \n",
       "4              5  B_101/62_5_21  B_101/62_5_2     11003  101_62 NaN   \n",
       "\n",
       "   thick_clay  fine_clay  silt  clay  ...  Ta   W  Pt  Au  Hg  Tl  Pb  Bi  Th  \\\n",
       "0   61.700001       32.8   0.2   5.3  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
       "1   52.799999       35.1   0.7  11.4  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
       "2   42.500000       46.2   0.2  11.1  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
       "3   42.599998       41.8   0.2  15.4  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
       "4   36.799999       47.5   1.2  14.5  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
       "\n",
       "    U  \n",
       "0 NaN  \n",
       "1 NaN  \n",
       "2 NaN  \n",
       "3 NaN  \n",
       "4 NaN  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged1.to_csv(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_clean/analyses_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter all samples and profiles down to the matching samples and profiles in merged1 (analysis_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match samples and merged1 by sample_id\n",
    "# filter down samples to a new df 'samples_filtered' so that samples_filtered only has the data that exists in both sample_id of merged1 and samples\n",
    "\n",
    "# Step 1: Find the common sample_ids\n",
    "common_ids = samples_check['sample_id'].isin(merged1['sample_id'])\n",
    "\n",
    "# Step 2: Filter 'samples' to only include rows with sample_ids in 'merged1'\n",
    "samples_filtered = samples_check[common_ids].copy()\n",
    "\n",
    "# Optional: Reset index if desired\n",
    "# samples_filtered.reset_index(drop=True, inplace=True)\n",
    "# Now 'samples_filtered' contains only rows with matching sample_id values in both DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ merged1 has 7798 rows.\n"
     ]
    }
   ],
   "source": [
    "print(f\"‚úÖ merged1 has {len(merged1)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ samples_filtered has 6686 rows.\n"
     ]
    }
   ],
   "source": [
    "print(f\"‚úÖ samples_filtered has {len(samples_filtered)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sample_ids in samples: 14715\n",
      "Unique sample_ids in merged1: 7798\n",
      "Sample_ids in merged1 not found in samples: 1112\n"
     ]
    }
   ],
   "source": [
    "# How many unique sample_ids are in each DataFrame?\n",
    "print(f\"Unique sample_ids in samples: {samples_check['sample_id'].nunique()}\")\n",
    "print(f\"Unique sample_ids in merged1: {merged1['sample_id'].nunique()}\")\n",
    "\n",
    "# How many sample_ids in merged1 are missing from samples?\n",
    "missing_ids = merged1[~merged1['sample_id'].isin(samples_check['sample_id'])]\n",
    "print(f\"Sample_ids in merged1 not found in samples: {missing_ids['sample_id'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sample_ids in samples_check: 14715\n",
      "Unique sample_ids in merged1: 7798\n",
      "Sample_ids in merged1 not found in samples_check: 1112\n",
      "‚ùó Missing sample_ids saved to missing_sample_ids.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Find the common sample_ids\n",
    "common_ids = samples_check['sample_id'].isin(merged1['sample_id'])\n",
    "\n",
    "# Step 2: Filter 'samples_check' to only include rows with sample_ids in 'merged1'\n",
    "samples_filtered = samples_check[common_ids].copy()\n",
    "\n",
    "# Step 3: Diagnostics\n",
    "print(f\"Unique sample_ids in samples_check: {samples_check['sample_id'].nunique()}\")\n",
    "print(f\"Unique sample_ids in merged1: {merged1['sample_id'].nunique()}\")\n",
    "\n",
    "# Step 4: Find sample_ids in merged1 that are not in samples_check\n",
    "missing_ids = merged1[~merged1['sample_id'].isin(samples_check['sample_id'])]\n",
    "print(f\"Sample_ids in merged1 not found in samples_check: {missing_ids['sample_id'].nunique()}\")\n",
    "\n",
    "# Step 5: Export missing sample_ids to CSV\n",
    "missing_ids.to_csv(\"missing_sample_ids.csv\", index=False)\n",
    "print(\"‚ùó Missing sample_ids saved to missing_sample_ids.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>ID</th>\n",
       "      <th>profile</th>\n",
       "      <th>X_coord</th>\n",
       "      <th>Y_coord</th>\n",
       "      <th>land_cover_id</th>\n",
       "      <th>climate_id</th>\n",
       "      <th>geology_id</th>\n",
       "      <th>topo_feature_id</th>\n",
       "      <th>sampling_date</th>\n",
       "      <th>districts_id</th>\n",
       "      <th>district</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2770</td>\n",
       "      <td>1_57</td>\n",
       "      <td>12.161278</td>\n",
       "      <td>-15.222598</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Namibe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>1_59</td>\n",
       "      <td>12.575775</td>\n",
       "      <td>-4.866986</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Cabinda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1618</td>\n",
       "      <td>1_61</td>\n",
       "      <td>15.098840</td>\n",
       "      <td>-11.225411</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Cuanza Sul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>881</td>\n",
       "      <td>1_63</td>\n",
       "      <td>17.081955</td>\n",
       "      <td>-9.274587</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Malanje</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1750</td>\n",
       "      <td>1_64</td>\n",
       "      <td>20.788116</td>\n",
       "      <td>-11.568683</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Moxico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   site_info_id    ID profile    X_coord    Y_coord land_cover_id climate_id  \\\n",
       "0             1  2770    1_57  12.161278 -15.222598          <NA>       <NA>   \n",
       "1             2    48    1_59  12.575775  -4.866986          <NA>       <NA>   \n",
       "2             3  1618    1_61  15.098840 -11.225411          <NA>       <NA>   \n",
       "3             4   881    1_63  17.081955  -9.274587          <NA>       <NA>   \n",
       "4             5  1750    1_64  20.788116 -11.568683          <NA>       <NA>   \n",
       "\n",
       "  geology_id topo_feature_id sampling_date districts_id    district  \n",
       "0       <NA>            <NA>          <NA>         <NA>      Namibe  \n",
       "1       <NA>            <NA>          <NA>         <NA>     Cabinda  \n",
       "2       <NA>            <NA>          <NA>         <NA>  Cuanza Sul  \n",
       "3       <NA>            <NA>          <NA>         <NA>     Malanje  \n",
       "4       <NA>            <NA>          <NA>         <NA>      Moxico  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_info_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique profile in site_info_clean: 4321\n",
      "Unique profile in merged1: 1587\n",
      "profile in merged1 not found in site_info_clean: 8\n",
      "‚ùó Missing sample_ids saved to missing_profs.csv\n"
     ]
    }
   ],
   "source": [
    "# How many unique sample_ids are in each DataFrame?\n",
    "print(f\"Unique profile in site_info_clean: {site_info_clean['profile'].nunique()}\")\n",
    "print(f\"Unique profile in merged1: {merged1['profile'].nunique()}\")\n",
    "\n",
    "# How many sample_ids in merged1 are missing from samples?\n",
    "missing_profs = merged1[~merged1['profile'].isin(site_info_clean['profile'])]\n",
    "print(f\"profile in merged1 not found in site_info_clean: {missing_profs['profile'].nunique()}\")\n",
    "\n",
    "#Export missing_profs  to CSV\n",
    "missing_profs.to_csv(\"missing_profss.csv\", index=False)\n",
    "print(\"‚ùó Missing sample_ids saved to missing_profss.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## correct  profile data points in merged1 to profiles in site_info_clean based on assumptions\n",
    "# make all merged1 profile become this \n",
    "#profile to change 185a_58: update to this 185_58\n",
    "#Mj 26 update to 310c/63\n",
    "#31_6 update to 31_63\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soil Profile Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "##rename columns for clarity add FK/PK\n",
    "#create copy of samples\n",
    "soil_profile_record = samples_filtered.copy()\n",
    "# Rename columns in the DataFrame\n",
    "samples_filtered.rename(columns={\n",
    "    'Registo': 'sample_id',\n",
    "    'N¬∫ Campo': 'site_info_id',\n",
    "    'Ano': 'year',\n",
    "    'Perfil': 'profile',\n",
    "    'Campanha': 'campaign',\n",
    "    'Col√≥nia_Pais': 'country',\n",
    "    'Distrito': 'district',\n",
    "    'AmostraCrivada': 'sample_sifted',\n",
    "    'AmostraNaoCrivada': 'sample_not_sifted',\n",
    "    'Prateleira': 'shelf',\n",
    "    'Sala': 'room'\n",
    "}, inplace=True)\n",
    "\n",
    "soil_profile_cleaning2 = soil_profile_record\n",
    "# Add missing columns\n",
    "soil_profile_cleaning2['site_info_id'] = pd.NA\n",
    "soil_profile_cleaning2['sample_id'] = pd.NA \n",
    "soil_profile_cleaning2['soil_type_id'] = pd.NA \n",
    "soil_profile_cleaning2['site_info_id'] = soil_profile_cleaning2['site_info_id'].astype(str)\n",
    "\n",
    "# Add a new Primary Key ID column starting from 1\n",
    "# Check if 'lab_sample_id' exists and drop it\n",
    "if 'profile_record_id' in soil_profile_cleaning2.columns:\n",
    "    soil_profile_cleaning2 = soil_profile_cleaning2.drop(columns=['profile_record_id'])\n",
    "\n",
    "# Insert a new column, e.g., 'lab_sample_id' as a primary key starting from 1\n",
    "soil_profile_cleaning2.insert(0, 'profile_record_id', range(1, len(soil_profile_cleaning2) + 1))\n",
    "\n",
    "# Reorder columns to match SAMPLES schema\n",
    "profile_record_clean = soil_profile_cleaning2[[\n",
    "    'profile_record_id',\n",
    "    'profile',\n",
    "    'site_info_id',\n",
    "    'sample_id',\n",
    "    'soil_type_id'\n",
    "]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure consistent data types and formatting\n",
    "\n",
    "# correct datatypes\n",
    "#profile to string\n",
    "profile_record_clean['profile'] = profile_record_clean['profile'].astype(str).str.strip().str[:20]\n",
    "\n",
    "#sample_id to string\n",
    "# Convert cleaned values to numeric safely\n",
    "# Convert sample_id to numeric first (coerce errors to NaN)\n",
    "profile_record_clean['sample_id'] = pd.to_numeric(profile_record_clean['sample_id'], errors='coerce')\n",
    "# Then convert to string (will keep NaNs as <NA>)\n",
    "profile_record_clean['sample_id'] = profile_record_clean['sample_id'].astype('Int64').astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<NA>']\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates and nulls\n",
    "# Filter out and print duplicated samples (including the first occurrence)\n",
    "duplicated_values = profile_record_clean['sample_id'][profile_record_clean['sample_id'].duplicated()].unique()\n",
    "print(duplicated_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "profile_record_id     int64\n",
       "profile              object\n",
       "site_info_id         object\n",
       "sample_id            object\n",
       "soil_type_id         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_record_clean.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_record_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>soil_type_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>208</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>208</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   profile_record_id profile site_info_id sample_id soil_type_id\n",
       "0                  1     139         <NA>      <NA>         <NA>\n",
       "1                  2     139         <NA>      <NA>         <NA>\n",
       "2                  3     139         <NA>      <NA>         <NA>\n",
       "4                  4     208         <NA>      <NA>         <NA>\n",
       "5                  5     208         <NA>      <NA>         <NA>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the result\n",
    "profile_record_clean.head()\n",
    "#profile_clean.to_csv(\"/Users/inesschwartz/Desktop/Thesis/tables_clean/profile_record_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Site info table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>ID</th>\n",
       "      <th>profile</th>\n",
       "      <th>X_coord</th>\n",
       "      <th>Y_coord</th>\n",
       "      <th>land_cover_id</th>\n",
       "      <th>climate_id</th>\n",
       "      <th>geology_id</th>\n",
       "      <th>topo_feature_id</th>\n",
       "      <th>sampling_date</th>\n",
       "      <th>districts_id</th>\n",
       "      <th>district</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2770</td>\n",
       "      <td>1_57</td>\n",
       "      <td>12.161278</td>\n",
       "      <td>-15.222598</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Namibe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>1_59</td>\n",
       "      <td>12.575775</td>\n",
       "      <td>-4.866986</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Cabinda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1618</td>\n",
       "      <td>1_61</td>\n",
       "      <td>15.098840</td>\n",
       "      <td>-11.225411</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Cuanza Sul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>881</td>\n",
       "      <td>1_63</td>\n",
       "      <td>17.081955</td>\n",
       "      <td>-9.274587</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Malanje</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1750</td>\n",
       "      <td>1_64</td>\n",
       "      <td>20.788116</td>\n",
       "      <td>-11.568683</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Moxico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   site_info_id    ID profile    X_coord    Y_coord land_cover_id climate_id  \\\n",
       "0             1  2770    1_57  12.161278 -15.222598          <NA>       <NA>   \n",
       "1             2    48    1_59  12.575775  -4.866986          <NA>       <NA>   \n",
       "2             3  1618    1_61  15.098840 -11.225411          <NA>       <NA>   \n",
       "3             4   881    1_63  17.081955  -9.274587          <NA>       <NA>   \n",
       "4             5  1750    1_64  20.788116 -11.568683          <NA>       <NA>   \n",
       "\n",
       "  geology_id topo_feature_id sampling_date districts_id    district  \n",
       "0       <NA>            <NA>          <NA>         <NA>      Namibe  \n",
       "1       <NA>            <NA>          <NA>         <NA>     Cabinda  \n",
       "2       <NA>            <NA>          <NA>         <NA>  Cuanza Sul  \n",
       "3       <NA>            <NA>          <NA>         <NA>     Malanje  \n",
       "4       <NA>            <NA>          <NA>         <NA>      Moxico  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "site_info_cleaning = profile_loc\n",
    "\n",
    "site_info_cleaning.rename(columns={\n",
    "    'PERFIL': 'profile',\n",
    "    'X_COORD': 'X_coord',\n",
    "    'Y_COORD': 'Y_coord', \n",
    "    'PRO': 'district'\n",
    "}, inplace=True)\n",
    "\n",
    "# Add a new Primary Key ID column starting from 1\n",
    "site_info_cleaning.insert(0, 'site_info_id', range(1, len(site_info_cleaning) + 1))\n",
    "\n",
    "\n",
    "# Add missing columns\n",
    "site_info_cleaning['land_cover_id'] = pd.NA\n",
    "site_info_cleaning['climate_id'] = pd.NA  \n",
    "site_info_cleaning['geology_id'] = pd.NA\n",
    "site_info_cleaning['topo_feature_id'] = pd.NA  \n",
    "site_info_cleaning['sampling_date'] = pd.NA # might leave out...\n",
    "site_info_cleaning['districts_id'] = pd.NA  \n",
    "\n",
    "# Function to remove accents\n",
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        return ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    return text\n",
    "\n",
    "# Apply to all object (string) columns\n",
    "for col in site_info_cleaning.select_dtypes(include='object').columns:\n",
    "    site_info_cleaning[col] = site_info_cleaning[col].apply(remove_accents)\n",
    "\n",
    "# Replace / with _ and strip/shorten 'profile' BEFORE slicing into site_info_clean\n",
    "site_info_cleaning['profile'] = (\n",
    "    site_info_cleaning['profile']\n",
    "    .astype(str)\n",
    "    .str.replace('/', '_')\n",
    "    .str.strip()\n",
    "    .str[:20]\n",
    ")\n",
    "\n",
    "# Then select columns\n",
    "site_info_clean = site_info_cleaning[[\n",
    "    'site_info_id',\n",
    "    'ID',\n",
    "    'profile',\n",
    "    'X_coord',\n",
    "    'Y_coord',\n",
    "    'land_cover_id',\n",
    "    'climate_id',\n",
    "    'geology_id',\n",
    "    'topo_feature_id',\n",
    "    'sampling_date',\n",
    "    'districts_id',\n",
    "    'district'\n",
    "]]\n",
    "\n",
    "\n",
    "site_info_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphology Horizon Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "morphology.rename(columns={\n",
    "    'Morfo_id':'horizon_id',\n",
    "    'Amostra': 'sample_id',\n",
    "    'Perfil': 'profile',\n",
    "    'CM':'horizon_layer',\n",
    "    'Limite Superior': 'upper_depth',\n",
    "    'Limite inferior': 'lower_depth',\n",
    "    'Grau de humidade': 'moisture_degree',\n",
    "    'Quantidade de ra√≠zes': 'root_quantity',\n",
    "    'Di√¢metro de ra√≠zes': 'root_diameter',\n",
    "    'Textura': 'texture',\n",
    "    'Tipo de estrutura': 'structure_type',\n",
    "    'Classes de estrutura': 'structure_class',\n",
    "    'Grau de estrutura': 'structure_degree',\n",
    "    'Di√¢metro de poros': 'pore_diameter',\n",
    "    'Quantidade de poros': 'pore_quantity',\n",
    "    'Forma de poros': 'pore_shape',\n",
    "    'Cor (s)': 'dry_color_name',\n",
    "    'Matiz (s)': 'dry_hue',\n",
    "    'Valor (s)':'dry_value',\n",
    "    'Croma (s)': 'dry_chroma',\n",
    "    'Cor (h)': 'moist_color_name',\n",
    "    'Matiz (h)': 'moist_hue',\n",
    "    'Valor (h)': 'moist_value',\n",
    "    'Croma (h)': 'moist_chroma',\n",
    "    'Compacidade':'compaction',\n",
    "    'Dureza': 'durability'\n",
    "}, inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "morphology_cleaning = morphology.drop(columns=[\n",
    "    'ID1', 'Agrupamento', 'REF', 'Pro', 'Observa√ßoes', 'Horizonte de diagn√≥stico', 'Propriedade de diagn√≥stico', 'Nitidez do limite', 'Designa√ß√£o do horizonte', 'Observa√ßoes', 'Confirmar', 'Adesividade', 'Plasticidade', 'Efervesc√™ncia com HCl', 'Friabilidade', 'Orienta√ß√£o das Fendas', 'Largura das fendas', 'Quantidade de fendas'\n",
    "])\n",
    "\n",
    "#add profile_record_id to populate later from soil profile table\n",
    "morphology_cleaning['profile_record_id'] = pd.NA\n",
    "\n",
    "#drop accents\n",
    "import unicodedata\n",
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        # Normalize and remove diacritics\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "# Apply to all cells in the DataFrame\n",
    "morphology_cleaning = morphology_cleaning.applymap(remove_accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>horizon_layer</th>\n",
       "      <th>upper_depth</th>\n",
       "      <th>lower_depth</th>\n",
       "      <th>dry_color_name</th>\n",
       "      <th>dry_hue</th>\n",
       "      <th>dry_value</th>\n",
       "      <th>dry_chroma</th>\n",
       "      <th>...</th>\n",
       "      <th>compaction</th>\n",
       "      <th>durability</th>\n",
       "      <th>pore_quantity</th>\n",
       "      <th>pore_diameter</th>\n",
       "      <th>pore_shape</th>\n",
       "      <th>root_quantity</th>\n",
       "      <th>root_diameter</th>\n",
       "      <th>moisture_degree</th>\n",
       "      <th>Unnamed: 46</th>\n",
       "      <th>profile_record_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>10999.0</td>\n",
       "      <td>101/62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Pardo-acinzentado a pardo</td>\n",
       "      <td>10YR</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>Pequena a minima</td>\n",
       "      <td>Brando</td>\n",
       "      <td>Pouco poroso</td>\n",
       "      <td>Muito finos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Muitas finas e bastantes medias</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>101/62</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Pardo</td>\n",
       "      <td>10YR</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Brando</td>\n",
       "      <td>Pouco poroso</td>\n",
       "      <td>Muito finos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bastantes finas e medias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>11001.0</td>\n",
       "      <td>101/62</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>Pardo-amarelado-claro</td>\n",
       "      <td>10YR</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Pequena a minima</td>\n",
       "      <td>Brando</td>\n",
       "      <td>Pouco poroso</td>\n",
       "      <td>Muito finos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algumas finas e medias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>11002.0</td>\n",
       "      <td>101/62</td>\n",
       "      <td>4.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Amarelo a amarelo-avermelhado</td>\n",
       "      <td>8,75YR</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Pequena a minima</td>\n",
       "      <td>Brando</td>\n",
       "      <td>Pouco a medianamente poroso</td>\n",
       "      <td>Muito finos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Poucas finas, algumas medias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B_101/62_5_2</td>\n",
       "      <td>11003.2</td>\n",
       "      <td>101/62</td>\n",
       "      <td>5.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>Pardo-avermelhado</td>\n",
       "      <td>7,5YR</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Brando</td>\n",
       "      <td>Pouco a medianamente poroso</td>\n",
       "      <td>Muito finos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Raras</td>\n",
       "      <td>Medias e grossas</td>\n",
       "      <td>Seco a humido</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     horizon_id  sample_id profile  horizon_layer  upper_depth  lower_depth  \\\n",
       "0  B_101/62_1_1    10999.0  101/62            1.0          0.0         11.0   \n",
       "1  B_101/62_2_1    11000.0  101/62            2.0         11.0         28.0   \n",
       "2  B_101/62_3_1    11001.0  101/62            3.0         28.0         54.0   \n",
       "3  B_101/62_4_1    11002.0  101/62            4.0         54.0         90.0   \n",
       "4  B_101/62_5_2    11003.2  101/62            5.0         90.0        160.0   \n",
       "\n",
       "                  dry_color_name dry_hue  dry_value  dry_chroma  ...  \\\n",
       "0      Pardo-acinzentado a pardo    10YR        5.0         2.5  ...   \n",
       "1                          Pardo    10YR        5.0         3.0  ...   \n",
       "2          Pardo-amarelado-claro    10YR        6.0         4.0  ...   \n",
       "3  Amarelo a amarelo-avermelhado  8,75YR        7.0         6.0  ...   \n",
       "4              Pardo-avermelhado   7,5YR        7.0         6.0  ...   \n",
       "\n",
       "         compaction durability                pore_quantity  pore_diameter  \\\n",
       "0  Pequena a minima     Brando                 Pouco poroso    Muito finos   \n",
       "1           Pequena     Brando                 Pouco poroso    Muito finos   \n",
       "2  Pequena a minima     Brando                 Pouco poroso    Muito finos   \n",
       "3  Pequena a minima     Brando  Pouco a medianamente poroso    Muito finos   \n",
       "4           Pequena     Brando  Pouco a medianamente poroso    Muito finos   \n",
       "\n",
       "  pore_shape                                 root_quantity     root_diameter  \\\n",
       "0        NaN               Muitas finas e bastantes medias               NaN   \n",
       "1        NaN      Bastantes finas e medias e raras grossas               NaN   \n",
       "2        NaN        Algumas finas e medias e raras grossas               NaN   \n",
       "3        NaN  Poucas finas, algumas medias e raras grossas               NaN   \n",
       "4        NaN                                         Raras  Medias e grossas   \n",
       "\n",
       "  moisture_degree Unnamed: 46 profile_record_id  \n",
       "0            Seco         NaN              <NA>  \n",
       "1            Seco         NaN              <NA>  \n",
       "2            Seco         NaN              <NA>  \n",
       "3            Seco         NaN              <NA>  \n",
       "4   Seco a humido         NaN              <NA>  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reorder columns to match SAMPLES schema\n",
    "morphology_cleaning[[\n",
    "    'horizon_id',\n",
    "    'sample_id',\n",
    "    'profile_record_id',\n",
    "    'profile',\n",
    "    'horizon_layer',\n",
    "    'upper_depth',\n",
    "    'lower_depth',\n",
    "    'moisture_degree',\n",
    "    'root_quantity',\n",
    "    'root_diameter',\n",
    "    'texture',\n",
    "    'structure_type',\n",
    "    'structure_class',\n",
    "    'structure_degree',\n",
    "    'pore_diameter',\n",
    "    'pore_quantity',\n",
    "    'pore_shape',\n",
    "    'dry_color_name',\n",
    "    'dry_hue',\n",
    "    'dry_value',\n",
    "    'dry_chroma',\n",
    "    'moist_color_name',\n",
    "    'moist_hue',\n",
    "    'moist_value',\n",
    "    'moist_chroma',\n",
    "    'compaction',\n",
    "    'durability'\n",
    "]]\n",
    "\n",
    "# Show first few rows\n",
    "morphology_cleaning.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "horizon_id                             object\n",
       "sample_id                             float64\n",
       "profile                                object\n",
       "horizon_layer                         float64\n",
       "upper_depth                           float64\n",
       "lower_depth                           float64\n",
       "dry_color_name                         object\n",
       "dry_hue                                object\n",
       "dry_value                             float64\n",
       "dry_chroma                            float64\n",
       "moist_color_name                       object\n",
       "moist_hue                              object\n",
       "moist_value                           float64\n",
       "moist_chroma                          float64\n",
       "Manchas                                object\n",
       "texture                                object\n",
       "Abund√¢ncia de elementos grosseiros     object\n",
       "Forma de elementos grosseiros          object\n",
       "Natureza de elementos grosseiros       object\n",
       "structure_type                         object\n",
       "structure_class                        object\n",
       "structure_degree                       object\n",
       "compaction                             object\n",
       "durability                             object\n",
       "pore_quantity                          object\n",
       "pore_diameter                          object\n",
       "pore_shape                             object\n",
       "root_quantity                          object\n",
       "root_diameter                          object\n",
       "moisture_degree                        object\n",
       "Unnamed: 46                           float64\n",
       "profile_record_id                      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morphology_cleaning.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure consistent data types and formatting\n",
    "morphology_cleaning['profile'] = morphology_cleaning['profile'].astype(str).str.replace('/', '_').str.strip().str[:20]\n",
    "\n",
    "#convert sample_to to string of the integer if it's a float like 11003.0\n",
    "#The original string if it's not an integer or has decimal places\n",
    "#\"NULL\" if the value is missing\n",
    "def clean_sample_id(x):\n",
    "    if pd.isnull(x):\n",
    "        return \"NULL\"\n",
    "    try:\n",
    "        float_val = float(x)\n",
    "        if float_val.is_integer():\n",
    "            return str(int(float_val))\n",
    "        else:\n",
    "            return str(float_val)\n",
    "    except:\n",
    "        return str(x)\n",
    "\n",
    "morphology_cleaning['sample_id'] = morphology_cleaning['sample_id'].apply(clean_sample_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NULL']\n",
      "üîç Number of sample_id values equal to 'NULL': 6639\n"
     ]
    }
   ],
   "source": [
    "# Check for nulls in 'sample_id' column\n",
    "null_string_values = morphology_cleaning[morphology_cleaning['sample_id'] == \"NULL\"]['sample_id'].unique()\n",
    "print(null_string_values)\n",
    "\n",
    "null_string_count = (morphology_cleaning['sample_id'] == \"NULL\").sum()\n",
    "print(f\"üîç Number of sample_id values equal to 'NULL': {null_string_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 1260 sample_id(s) in morphology_cleaning but not in samples_check:\n",
      "['10064.2', '1007', '1022', '1024', '10338', '10339', '10340', '10341', '10342', '10343', '10344', '10345', '10346', '10347', '10348', '10377', '104', '10428.2', '10443', '10444', '10445', '10446.2', '10448', '10449', '10450', '10451', '10467', '10468', '10469', '10470.2', '10472', '10480.2', '10485.2', '105', '10509.2', '10536.2', '10558', '10562.2', '10576.2', '106', '1067', '1068', '1069', '107', '10843.2', '10849.2', '10885.2', '1089', '1090', '10901', '10902', '10903', '10904.2', '1091', '1092', '10922.2', '1093', '10933.2', '1094', '1099', '1100.2', '11003.2', '1108.2', '1110', '1111.2', '1117', '11176.2', '1118', '1119.2', '11285.2', '11327.2', '11356.2', '11373', '11374', '1138', '1139', '11393.2', '1140.2', '1142', '1143', '1144.3', '11454.2', '1147', '1148', '11485.2', '1149.3', '11492', '11493', '11494', '1152', '1153', '1154.2', '1156', '11560.2', '1157', '1158.3', '1160', '11794.2', '1182', '1183', '1184.2', '1186', '1187', '1188', '1189', '1190', '1191.2', '1198', '1199', '1200.3', '1204', '1205', '1206.2', '1208', '1209.2', '1211', '1212', '1213', '1214', '1215.2', '1217', '1219', '1220.2', '1222', '1223', '1224', '1225.2', '1227', '1228', '1229.2', '1231', '1232', '1233', '1234', '1235', '1236.2', '12450', '12451', '12452', '12494', '12495', '12496', '1250', '1251', '1252.3', '1255', '1256', '12630.2', '12636.2', '12653.2', '12692.2', '12715.2', '1274', '1275.3', '1286', '1287', '1288', '1291', '1292', '1298', '1299', '1300', '1303', '1304', '1323', '1324', '1325.3', '1327', '13274.2', '1328', '1329', '13299.2', '1333', '1334', '1335.3', '1338', '13387.2', '1339', '13394.2', '134', '1340', '13404.2', '1341', '1343.2', '13475.2', '13481.2', '13493.2', '135', '1350', '1351', '1352.3', '13521.2', '13532.2', '13549.2', '1355', '13556.2', '1356.3', '1358', '13581.2', '13599.2', '136', '1360', '13606.2', '1361', '13621.2', '13638.2', '1364', '1365', '1366.3', '13661.2', '1368', '13695.2', '1375', '1376.4', '13765.2', '13779.2', '1379', '13793.2', '13804.2', '1381', '13810.2', '13817.2', '1382', '1383.2', '1386', '1387', '1388.2', '13883.2', '13901.2', '13924.2', '13929', '13930', '13931.2', '13933', '13934', '13949', '1396', '1397', '1398', '13988.2', '1399', '13999.2', '140', '1401', '14018.2', '1402', '1403', '1406', '14064.2', '1407', '14071.2', '1408.3', '1411', '14112.2', '14119.2', '1412', '1413.2', '14135.2', '1415', '14156.2', '142', '14213.2', '1424', '14249.2', '1425', '1426.3', '14266.2', '14278.2', '1429', '14296.2', '1434', '14351', '14352', '14353', '1438', '1439', '144', '1440', '1441', '1442.2', '14451', '14452', '14453', '14454', '14455.2', '14508', '14509', '14510', '1452', '1453', '1454', '1455', '1456', '1457', '1458', '1459', '1460', '1461', '1462', '1463.2', '1465', '1466', '1473', '1474', '1475.2', '1482', '1483', '1484', '1485.2', '14857.2', '1487', '1488', '1489', '1490.3', '14900', '14901', '14920.2', '1493', '1494', '1495', '1496', '1497', '14982', '15005.2', '15031.2', '15032.2', '15049.2', '15132.2', '1525', '15286.2', '15305.2', '15314', '15315', '15316', '15337.2', '15352.2', '15370.2', '1538', '1539', '1542', '1543', '1544', '1546.2', '1548.3', '1557.2', '1560.2', '1562.2', '15641.2', '1568.2', '15769', '15770', '15771', '15772', '15773', '15774', '15775', '15776', '15777', '15778.2', '1580.2', '1585.2', '1589.3', '15896.2', '1592.2', '15951.2', '15957.2', '1598.2', '1602.2', '16031.2', '1604.2', '16266.2', '1627.2', '1629.2', '16400', '16401', '16402', '16403', '16404', '16405', '16424', '16425', '16426', '16427', '16428', '16429', '16494.2', '1653.4', '16633', '16634', '16637', '16638', '1678', '1679', '1680', '1681', '16826', '16827', '16828', '16829', '16830', '16845', '16846', '16847', '16848', '16849', '16850', '16851', '16852', '16853.2', '16855', '16867', '16868', '16869', '16870.2', '16872', '16873', '1699', '1700', '1701', '1702', '1703', '1704', '1711', '1712', '1713', '1718', '1747.3', '1752.2', '17536.2', '1754.2', '17607.2', '17794', '17795', '17796', '17797', '17798', '17799', '1785.3', '17884', '17885', '17886', '17887', '17888', '17889', '17890', '1792.2', '17994', '17995', '17996', '17997', '1802.2', '18025', '18026', '18027', '18028', '18058', '18059', '18060', '18061', '18062', '18063', '18077', '18078', '18079', '18080', '18100', '18101', '18102', '18103', '18104', '1843.2', '18441', '18442', '18443', '18444', '18445', '18446', '18447', '1845.2', '18498.2', '1850.2', '18513.2', '18575.2', '18588.2', '18671.2', '18703.2', '18736', '18737', '18738', '18739', '1874.2', '18740.2', '18759.2', '18774', '18775', '18776', '18777', '18778', '18779', '1880.3', '18814.2', '19176', '19177', '19178', '19179', '19180', '19181', '19182', '19183', '19184', '19185', '19186', '19187', '19188', '19189', '19190', '19192', '19193', '19194', '19195', '19196', '1924', '19240', '19241', '19242', '19243', '19244.2', '19251', '19252', '19253', '19254', '19255.2', '19267', '19268', '19269', '19270', '19271', '1928', '19283', '19284', '19285', '19286', '19287', '19298', '19299', '19301', '19302', '19303', '19304', '1932', '19330', '19331', '19332', '19333', '19334', '19371', '19372', '19373', '19374', '19375', '1938.2', '19391', '19392', '19393', '19394', '19404', '19405', '19406', '19407', '19408', '19431', '19432', '19433', '19434', '19435', '19436', '19437', '19438', '19439', '19440', '19448', '19449', '19450', '19451', '19467', '19468', '19469', '19537', '19538', '19539', '19541', '19542', '19543', '19544', '19545', '19550', '19551', '19552', '19553', '19554.2', '19556', '19557', '19558', '1956.3', '19565', '19566', '19567', '19568', '19569', '19570', '19571', '19572', '19573', '19574', '19575', '19576', '19577', '19579', '19580', '19581', '19582', '19583', '19585', '19586', '19587', '19588', '19654', '19655', '19656', '1968.2', '1991.2', '1993.2', '2002.2', '2007.3', '2013.4', '2019.2', '2023.2', '2051.2', '2055.2', '2057.2', '2069.3', '2075.2', '20880', '20881', '20882', '20883', '2089.2', '20891', '20892', '20893', '20894', '20895', '20896', '20897', '20898', '20899', '20905', '20906', '20907', '20908', '20909', '20910', '20911', '20912.2', '20914', '20915', '20916', '20917', '20918', '20919', '20920', '20921.2', '20925', '20926', '20927', '2093.2', '20955', '20956', '20957', '20958', '20959', '20970', '20971', '20972', '20973', '21002', '21003', '21004', '21005', '21006', '21007', '2107.4', '2115.2', '2136.2', '2149.3', '21528', '21529', '21530', '21531', '21532', '21533', '21534', '21535', '21536', '21537', '21542', '21543', '21544', '21545', '21546', '21547', '21548', '21549', '21555', '21556', '21557', '21558', '21559', '21575', '21576', '21577', '21603', '21604', '21605', '21606', '21613', '21614', '21615', '21616', '21617', '21618', '21619', '21620', '21627', '21628', '21629', '21630', '21634', '21635', '21636', '21637', '21642', '21643', '21644', '2165', '21652', '21653', '21654', '21666', '21667', '21668', '21669', '21670', '21671', '21672', '21673', '21674', '2169.2', '21704', '21705', '21706', '21707', '21708', '21711', '21712', '21713', '21714', '21715', '21716', '21718', '21719', '21720', '21721', '21724', '21725', '21726', '21727', '21728', '21732', '21733', '21734', '21735', '21736', '21744', '21745', '21746', '21747', '2248.2', '2253.2', '2263.2', '2268.2', '2282.2', '22839', '22840', '22841', '22842', '22843', '22844', '22845', '22846', '2371.2', '2373.3', '2378.2', '2380.2', '2382.2', '2393.2', '2398.2', '2400.2', '2408.2', '2410.3', '2415.3', '2463.2', '2475.2', '2546', '2547', '2549', '2550', '2560', '2561', '2562', '2563', '2630', '2631', '2632', '2633', '2663', '2664', '2665', '2666', '2668', '2670', '2671', '2672', '2674', '2675', '2677', '2685', '2686', '2687', '2689', '2693', '2694', '2695', '2709', '2711.2', '2716.3', '2730.2', '2738.2', '2743.3', '2747.2', '2750.3', '2757.3', '2763.2', '2765.2', '2776.2', '2785.2', '2842.3', '2847.2', '2869.2', '2877.2', '2888.2', '2890.3', '2906.2', '2908.2', '2910.2', '2921.2', '2927.2', '2945.2', '2978.2', '2981.2', '2987.2', '299.2', '2994.2', '3', '3006.2', '3008.2', '301', '3014.2', '3016.3', '302', '3021.3', '3027', '3029', '303', '3031.2', '3045.2', '3050.2', '3052.2', '3103.2', '3109.2', '3119.2', '3145.2', '3151.2', '3156.2', '3161.2', '3163.2', '3165.2', '3179.2', '3194.2', '3213.2', '3216.2', '3227', '3229', '3231', '3259.2', '3290.2', '3309.2', '3348.2', '3412.2', '3417.2', '3492.2', '3506.2', '3526.2', '3528.3', '3535.2', '3568', '3569', '3570.2', '3572.2', '3574.2', '3606.2', '3636.2', '3651', '3652', '3653', '3654', '3655', '3671', '3672', '3673', '3674', '3676', '3703', '3704', '3705', '3706', '3721', '3722', '3723', '3734', '3735', '3736', '3737', '3745', '3746', '3768', '3770', '3785', '3787', '3788', '3798', '3800', '3801', '3835', '3837', '3838', '3840', '3841', '3850', '3852', '3853', '3854', '3855', '3869', '3870', '3871', '3872', '3873', '3874', '3876', '3878', '3879', '3881', '3923', '3924', '3925', '4241', '437', '438', '439', '473', '474', '475', '476', '483', '4840.3', '4867.3', '489', '4899.2', '490', '4908.2', '4916.2', '4920.2', '4957', '4958.2', '4972.3', '4981.2', '4989.2', '4996.2', '4998', '4999', '50', '5000', '5006.2', '5010.2', '5026.2', '5036', '5045.2', '5052', '5053.2', '5082.2', '5088.2', '51.2', '5101.2', '5107.2', '5123.2', '5174.2', '5176.2', '5192.2', '5202.2', '5207.2', '5214.2', '5218.2', '5225.3', '5270.2', '5315.2', '5327.2', '5332.2', '5342.2', '5348.2', '5359.2', '5389', '5404.2', '5417.2', '5431', '5434.2', '5442.2', '5444.2', '5447.2', '5476.2', '5478.2', '5686', '5717.2', '5737.2', '5745.2', '5751.2', '5767', '577', '5770.2', '5777.2', '578.2', '5780.2', '579', '5792', '5810.2', '5822.2', '5839.2', '5883.2', '5886', '5897.3', '5902.2', '5914.2', '5916.2', '5945.2', '5963.2', '5973.2', '598', '5983.2', '599', '5990.2', '5995.2', '5997.2', '600', '6014.2', '6022.2', '6032.2', '607', '608', '609.2', '611', '612.2', '6314.2', '6317.2', '6323.2', '6332.2', '6334', '6335', '6336.3', '6339', '634.2', '6340', '6341', '6342.3', '6348.2', '6350.2', '6356', '6357', '6358.2', '6373', '6374', '6375', '6376.3', '6381.2', '6383.3', '6386', '6387', '6388.2', '6390', '6391.2', '6402.2', '6405.2', '6416.2', '6436', '6437', '6438', '6439.3', '6475', '6476', '6477', '6478', '6479', '6481', '6549.2', '6633.2', '6635.2', '6652.2', '6667', '6668', '6669.2', '6675', '6684.2', '6686.2', '6704', '6705', '6707', '6710', '6726.2', '6754.2', '6768.2', '6787.2', '6815.2', '6824', '6836', '6840', '6853.2', '6872.2', '6890.2', '6895.2', '6897.2', '6903.2', '6908.2', '6921.2', '6924.2', '6931.2', '694', '695', '6964.2', '6974.2', '6988.2', '7011.2', '7026', '7047.2', '709', '7108.2', '7127', '7140', '7141.2', '7184.2', '7255.2', '7257', '7261', '7265', '7268.2', '7278', '7284', '7289.2', '7291.2', '734', '735', '7352', '7390.2', '7395', '7397.2', '74', '7408.2', '745', '746', '747', '75', '75.1', '76', '761', '7815', '7817.2', '7838.2', '7849.2', '7859.2', '7931', '7932.2', '7935.2', '7944.2', '7958.2', '8000', '8012.2', '8057', '8058', '8059', '8066', '8067', '8068', '8077', '8119.2', '8271.2', '8274.2', '832', '8332', '8336', '8339', '836.2', '8360.2', '8431', '8493', '8565', '86', '8681.2', '87', '88', '8830.2', '8865.2', '89', '890', '891', '8959.2', '90', '9039.2', '91', '9125.2', '9143.2', '9155.2', '9183.2', '9202', '9208.2', '9281.2', '9297.2', '9302.2', '9442.2', '9449.2', '9455.2', '95', '9561', '9564.2', '96', '9604.2', '97', '98', '985', '989', '996', '9986', 'NULL']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/79mdnyy56_xc3g1jvp9wf4_80000gn/T/ipykernel_2227/2245887227.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples_check['sample_id'] = samples_check['sample_id'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "#sample_id(s) in morphology_cleaning but not in samples_check:\n",
    "# Ensure both sample_id columns are strings for accurate comparison\n",
    "morphology_cleaning['sample_id'] = morphology_cleaning['sample_id'].astype(str)\n",
    "samples_check['sample_id'] = samples_check['sample_id'].astype(str)\n",
    "\n",
    "# Get unique sample_id values in both DataFrames\n",
    "morph_sample_ids = set(morphology_cleaning['sample_id'].dropna())\n",
    "samples_check_ids = set(samples_check['sample_id'].dropna())\n",
    "\n",
    "# Find sample_ids in morphology_cleaning but not in samples_check\n",
    "sample_ids_not_in_check = sorted(morph_sample_ids - samples_check_ids)\n",
    "\n",
    "# Print results\n",
    "print(f\"‚úÖ Found {len(sample_ids_not_in_check)} sample_id(s) in morphology_cleaning but not in samples_check:\")\n",
    "print(sample_ids_not_in_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing sample IDs in horizon:\n",
      "['633' '700' '707' ... '18869' '18870' '18871']\n",
      "9635\n"
     ]
    }
   ],
   "source": [
    "#check which sample_id values from samples_check are missing from morphology_cleaning\n",
    "\n",
    "missing_ids_horizon = samples_check[~samples_check['sample_id'].isin(morphology_cleaning['sample_id'])]\n",
    "print(\"Missing sample IDs in horizon:\")\n",
    "print(missing_ids_horizon['sample_id'].unique())\n",
    "print(len(missing_ids_horizon['sample_id'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal** = drop any horizon_id from morphology_cleaning that does not have both:\n",
    "\n",
    "1. Key horizon characteristics\n",
    "\n",
    "2. A valid profile (w location and site characteristics) that appears in site_info_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Found 2597 rows with at least one of the following issues:\n",
      "   ‚Ä¢ missing key horizon data\n",
      "   ‚Ä¢ invalid profile (not in site_info_clean)\n",
      "   ‚Ä¢ missing both upper_depth and lower_depth\n",
      "‚úÖ Remaining rows in morphology_cleaning: 10438\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------\n",
    "# 1. Standardize 'profile' column formatting\n",
    "# -----------------------------------------\n",
    "site_info_clean.loc[:, 'profile'] = (\n",
    "    site_info_clean['profile']\n",
    "    .astype(str)\n",
    "    .str.replace('/', '_')\n",
    "    .str.strip()\n",
    "    .str.upper()\n",
    "    .str[:20]\n",
    ")\n",
    "\n",
    "morphology_cleaning.loc[:, 'profile'] = (\n",
    "    morphology_cleaning['profile']\n",
    "    .astype(str)\n",
    "    .str.replace('/', '_')\n",
    "    .str.strip()\n",
    "    .str.upper()\n",
    "    .str[:20]\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Define valid profiles\n",
    "# ---------------------------\n",
    "valid_profiles = set(site_info_clean['profile'])\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Define helper to check for missing/zero\n",
    "# ---------------------------\n",
    "def is_missing(val):\n",
    "    return pd.isna(val) or val == 0 or val == '0'\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Build masks for filtering\n",
    "# ---------------------------\n",
    "\n",
    "# A. Missing key horizon data\n",
    "missing_key_data_mask = (\n",
    "    morphology_cleaning['horizon_id'].notna() &\n",
    "    morphology_cleaning['moist_color_name'].apply(is_missing) &\n",
    "    morphology_cleaning['texture'].apply(is_missing) &\n",
    "    morphology_cleaning['structure_type'].apply(is_missing)\n",
    ")\n",
    "\n",
    "# B. Invalid profile\n",
    "invalid_profile_mask = ~morphology_cleaning['profile'].isin(valid_profiles)\n",
    "\n",
    "# C. Missing both upper and lower depth\n",
    "missing_depth_mask = (\n",
    "    morphology_cleaning['upper_depth'].apply(is_missing) &\n",
    "    morphology_cleaning['lower_depth'].apply(is_missing)\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Combine all masks\n",
    "# ---------------------------\n",
    "rows_to_flag_mask = missing_key_data_mask | invalid_profile_mask | missing_depth_mask\n",
    "rows_to_flag = morphology_cleaning[rows_to_flag_mask]\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Report and drop flagged rows\n",
    "# ---------------------------\n",
    "print(f\"‚ö†Ô∏è Found {len(rows_to_flag)} rows with at least one of the following issues:\")\n",
    "print(\"   ‚Ä¢ missing key horizon data\")\n",
    "print(\"   ‚Ä¢ invalid profile (not in site_info_clean)\")\n",
    "print(\"   ‚Ä¢ missing both upper_depth and lower_depth\")\n",
    "\n",
    "# Optional: Save flagged rows for review\n",
    "# rows_to_flag.to_csv(\"/Users/inesschwartz/Desktop/morph_flagged_rows_combined.csv\", index=False)\n",
    "\n",
    "# Drop flagged rows from main DataFrame\n",
    "morphology_cleaning = morphology_cleaning[~rows_to_flag_mask].copy()\n",
    "\n",
    "# Summary after cleanup\n",
    "print(f\"‚úÖ Remaining rows in morphology_cleaning: {len(morphology_cleaning)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11016' '11017' 'NULL' '11352' '11353' '17532' '17533' '17534' '2438'\n",
      " '7192' '13178' '20883' '9027' '9231' '8232' '9255' '8639' '8536' '4918']\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates\n",
    "# Make sure sample_id and horizon_id are strings\n",
    "morphology_cleaning['sample_id'] = morphology_cleaning['sample_id'].astype(str).str.strip()\n",
    "morphology_cleaning['horizon_id'] = morphology_cleaning['horizon_id'].astype(str).str.strip()\n",
    "# Filter out and print duplicated samples (including the first occurrence)\n",
    "duplicated_values = morphology_cleaning['sample_id'][morphology_cleaning['sample_id'].duplicated()].unique()\n",
    "print(duplicated_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Morphology data cleaned and sample_id updated via horizon_id.\n"
     ]
    }
   ],
   "source": [
    "#addressing duplicates\n",
    "\n",
    "# -----------------------------------\n",
    "# 1. Correct sample_id using horizon_id map\n",
    "# -----------------------------------\n",
    "# Step 1: Make sure IDs are strings and trimmed\n",
    "morphology_cleaning['horizon_id'] = morphology_cleaning['horizon_id'].astype(str).str.strip()\n",
    "morphology_cleaning['sample_id'] = morphology_cleaning['sample_id'].astype(str).str.strip()\n",
    "\n",
    "# Step 2: Your mapping (horizon_id ‚Üí new sample_id)\n",
    "horizon_to_sample_map = {\n",
    "    'UZ_24c/60_1_1': '8639',\n",
    "    'UZ_24c/60_2_1': '8640',\n",
    "    'UZ_24c/60_3_1': '8641',\n",
    "    'UZ_24c/60_4_1': '8642',\n",
    "    'UZ_24c/60_5_1': '8643',\n",
    "    'UZ_66c/60_1_1': '8777',\n",
    "    'UZ_66c/60_2_1': '8778',\n",
    "    'UZ_66c/60_3_1': '8779',\n",
    "    'UZ_66c/60_4_1': '8780',\n",
    "    'CC_473/66_1_1': '17596',\n",
    "    'CC_473/66_2_1': '17597',\n",
    "    'CC_473/66_3_1': '17598',\n",
    "    'N_85/57_1_0': '4947',\n",
    "    'N_85/57_2_0': '4948',\n",
    "    'N_85/57_3_0': '4949',\n",
    "    'UZ_231/60_4_1': '8255',\n",
    "    'B_139/61_5_1': '10353',\n",
    "    'H_480/55_1_1': '3919',\n",
    "    'H_91/54_1_1': '1255',\n",
    "    'H_90/54_3_3': '1252',\n",
    "    'Hb_136/56_1_1':'2961',\n",
    "    'Hb_136/56_2_1': '2962',\n",
    "    'Hb_136/56_3_1': '2963',\n",
    "    'Hb_136/56_4_1': '2964',\n",
    "    'UZ_216/60_5_1': '8231',\n",
    "    'Bg_217/46_1_1': '694',\n",
    "    'Bg_217/46_2_1': '695',\n",
    "    'Bg_217/46_3_1': '696',\n",
    "    'UZ_215c/60_1_1':'9255',\n",
    "    'UZ_215c/60_2_1': '9256',\n",
    "    'UZ_215c/60_3_1': '9257',\n",
    "    'UZ_215c/60_4_1': '9258',\n",
    "    'UZ_215c/60_5_1': '9259',\n",
    "    'UZ_215c/60_6_1': '9260',\n",
    "    'N_30/57_1_1':'4848',\n",
    "    'N_30/57_2_2':'4849',\n",
    "    'N_30/57_3_1':'4850',\n",
    "    'UZ_66c/60_1_1': '8777',\n",
    "    'UZ_66c/60_2_1': '8778',\n",
    "    'UZ_66c/60_3_1': '8779',\n",
    "    'UZ_66c/60_4_1': '8780',\n",
    "    'UZ_66c/60_5_1': '8781',\n",
    "    'UZ_66c/60_6_1': '8783',\n",
    "    'UZ_66c/60_7_1': '8782',\n",
    "    'H_91/54_1_1': '1255',\n",
    "    'H_91/54_2_1': '1256',\n",
    "    'H_91/54_3_1': '1257'\n",
    "}\n",
    "\n",
    "# Step 3: Apply the mapping to replace sample_id based on horizon_id\n",
    "morphology_cleaning['sample_id'] = morphology_cleaning.apply(\n",
    "    lambda row: horizon_to_sample_map.get(row['horizon_id'], row['sample_id']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# 2. Drop specific horizon_ids (might have already been dropped)\n",
    "# -----------------------------------\n",
    "horizons_to_drop = [\n",
    "    'B_139/61_4_1', 'H_113/55_1_1', 'H_1306/52_1_1', 'H_1611/52_1_1',\n",
    "    'H_227/46_1_1', 'H_227/46_2_1', 'H_227/46_3_1', 'H_227/46_4_1',\n",
    "    'Mj_178c/63_4_1', 'Mj_321c/63_4_1', 'B_110/62_5_1', 'B_110/62_6_1'\n",
    "]\n",
    "morphology_cleaning = morphology_cleaning[~morphology_cleaning['horizon_id'].isin(horizons_to_drop)]\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# 3. Done ‚Äî Optional: Save to file\n",
    "# -----------------------------------\n",
    "#df_morpho_cleaned.to_csv(\"/Users/inesschwartz/Desktop/df_morpho_cleaned_corrected.csv\", index=False)\n",
    "print(\"‚úÖ Morphology data cleaned and sample_id updated via horizon_id.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NULL']\n",
      "üîç Number of sample_id values equal to 'NULL': 4713\n"
     ]
    }
   ],
   "source": [
    "#re-count null sample_id's\n",
    "morphology_cleaning\n",
    "# Check for nulls in 'sample_id' column\n",
    "null_string_values2 = morphology_cleaning[morphology_cleaning['sample_id'] == \"NULL\"]['sample_id'].unique()\n",
    "print(null_string_values2)\n",
    "\n",
    "null_string_count2 = (morphology_cleaning['sample_id'] == \"NULL\").sum()\n",
    "print(f\"üîç Number of sample_id values equal to 'NULL': {null_string_count2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Found 0 duplicated horizon_id(s).\n"
     ]
    }
   ],
   "source": [
    "num_duplicates = morphology_cleaning['horizon_id'].duplicated().sum()\n",
    "print(f\"üîÅ Found {num_duplicates} duplicated horizon_id(s).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_morpho_cleaned = morphology_cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dealing w decimals in sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Found 440 sample_id(s) with decimal values.\n",
      "\n",
      "üßæ Affected sample_ids:\n",
      "['11003.2' '11285.2' '10843.2' '10428.2' '10470.2' '10480.2' '10485.2'\n",
      " '10446.2' '10849.2' '10509.2' '10562.2' '10536.2' '11327.2' '11356.2'\n",
      " '11393.2' '11176.2' '11454.2' '10885.2' '10904.2' '10922.2' '11485.2'\n",
      " '10933.2' '4972.3' '4981.2' '5810.2' '5045.2' '5822.2' '4989.2' '5006.2'\n",
      " '5010.2' '5839.2' '4996.2' '5082.2' '5026.2' '6314.2' '6317.2' '6323.2'\n",
      " '6332.2' '6336.3' '5883.2' '6342.3' '6348.2' '6350.2' '6358.2' '5902.2'\n",
      " '5897.3' '6376.3' '6381.2' '6383.3' '6388.2' '6391.2' '6402.2' '6405.2'\n",
      " '6416.2' '5914.2' '5916.2' '6439.3' '5088.2' '5053.2' '5101.2' '5107.2'\n",
      " '5123.2' '5945.2' '5174.2' '5176.2' '5963.2' '5973.2' '5202.2' '5207.2'\n",
      " '5192.2' '5214.2' '5218.2' '5225.3' '5983.2' '5990.2' '5270.2' '5995.2'\n",
      " '5997.2' '5447.2' '6014.2' '6022.2' '5417.2' '5332.2' '6032.2' '5476.2'\n",
      " '5478.2' '5434.2' '5359.2' '5442.2' '5444.2' '5327.2' '5342.2' '5315.2'\n",
      " '5348.2' '7390.2' '7397.2' '5717.2' '5745.2' '5751.2' '5737.2' '5777.2'\n",
      " '7408.2' '5770.2' '5780.2' '20912.2' '20921.2' '19554.2' '19255.2'\n",
      " '18814.2' '18740.2' '6787.2' '6903.2' '6924.2' '6684.2' '6686.2' '6921.2'\n",
      " '7047.2' '6726.2' '6815.2' '6754.2' '6633.2' '6635.2' '6890.2' '6908.2'\n",
      " '6895.2' '6897.2' '6652.2' '6964.2' '7108.2' '6872.2' '6669.2' '6768.2'\n",
      " '6931.2' '6853.2' '7268.2' '7141.2' '6988.2' '7011.2' '7184.2' '6974.2'\n",
      " '7289.2' '7291.2' '7255.2' '16853.2' '16870.2' '18498.2' '18513.2'\n",
      " '17536.2' '18575.2' '18588.2' '18671.2' '17607.2' '18703.2' '10064.2'\n",
      " '11560.2' '11794.2' '1100.2' '1557.2' '1560.2' '1752.2' '1754.2' '1747.3'\n",
      " '1275.3' '1785.3' '1792.2' '1343.2' '1352.3' '1356.3' '1802.2' '1366.3'\n",
      " '1325.3' '836.2' '1376.4' '1383.2' '1335.3' '1388.2' '1108.2' '1562.2'\n",
      " '1413.2' '1408.3' '1111.2' '1568.2' '2248.2' '1843.2' '1845.2' '2253.2'\n",
      " '1426.3' '1850.2' '1580.2' '1442.2' '1602.2' '1604.2' '2263.2' '1463.2'\n",
      " '1119.2' '1475.2' '1490.3' '1485.2' '1874.2' '2268.2' '1589.3' '2282.2'\n",
      " '1140.2' '1585.2' '1598.2' '1144.3' '1938.2' '1149.3' '1956.3' '1968.2'\n",
      " '1154.2' '1158.3' '1991.2' '1993.2' '1548.3' '2002.2' '2013.4' '2007.3'\n",
      " '2371.2' '2373.3' '2378.2' '2380.2' '2382.2' '2019.2' '2023.2' '2055.2'\n",
      " '2057.2' '2051.2' '2393.2' '2398.2' '2400.2' '2075.2' '2069.3' '2408.2'\n",
      " '2410.3' '2415.3' '2093.2' '2089.2' '2107.4' '2115.2' '2136.2' '1627.2'\n",
      " '1629.2' '1546.2' '2463.2' '1184.2' '2169.2' '2475.2' '1191.2' '1200.3'\n",
      " '1653.4' '1206.2' '1209.2' '1215.2' '1220.2' '1225.2' '1229.2' '1236.2'\n",
      " '2921.2' '2888.2' '2890.3' '2906.2' '2908.2' '2910.2' '2927.2' '609.2'\n",
      " '612.2' '3506.2' '2945.2' '2716.3' '2994.2' '2987.2' '2978.2' '2981.2'\n",
      " '3021.3' '3006.2' '3008.2' '3014.2' '3016.3' '3535.2' '299.2' '3031.2'\n",
      " '3526.2' '3528.3' '3045.2' '3050.2' '3052.2' '3103.2' '3109.2' '3119.2'\n",
      " '2730.2' '3145.2' '3179.2' '3570.2' '3572.2' '3574.2' '3156.2' '3151.2'\n",
      " '3161.2' '3163.2' '3165.2' '3194.2' '3213.2' '3216.2' '3606.2' '3259.2'\n",
      " '3636.2' '2750.3' '3290.2' '3309.2' '2743.3' '2738.2' '2763.2' '2765.2'\n",
      " '2757.3' '2776.2' '2785.2' '2711.2' '3492.2' '2847.2' '2842.3' '2877.2'\n",
      " '2869.2' '10576.2' '19244.2' '15778.2' '15305.2' '16266.2' '15005.2'\n",
      " '15286.2' '15049.2' '15337.2' '15031.2' '15032.2' '15957.2' '15951.2'\n",
      " '15896.2' '15352.2' '15370.2' '16494.2' '15641.2' '15132.2' '14857.2'\n",
      " '14455.2' '14920.2' '16031.2' '13521.2' '13779.2' '13765.2' '13493.2'\n",
      " '13817.2' '13793.2' '13810.2' '13804.2' '13475.2' '13883.2' '13901.2'\n",
      " '13481.2' '13924.2' '14018.2' '13999.2' '13988.2' '13931.2' '14112.2'\n",
      " '14119.2' '13394.2' '14071.2' '13549.2' '14135.2' '14064.2' '13387.2'\n",
      " '14156.2' '13556.2' '13274.2' '12692.2' '13532.2' '14249.2' '13299.2'\n",
      " '14278.2' '14213.2' '14266.2' '14296.2' '13404.2' '12715.2' '51.2'\n",
      " '12636.2' '578.2' '12630.2' '13599.2' '75.1' '13581.2' '12653.2'\n",
      " '13621.2' '13606.2' '13638.2' '13661.2' '13695.2' '4840.3' '3348.2'\n",
      " '3417.2' '3412.2' '4867.3' '4899.2' '4908.2' '4916.2' '4920.2' '5404.2'\n",
      " '4958.2' '9039.2' '8959.2' '9125.2' '8119.2' '9183.2' '9208.2' '9143.2'\n",
      " '9155.2' '7838.2' '9281.2' '7849.2' '9297.2' '9302.2' '9449.2' '8274.2'\n",
      " '8271.2' '9442.2' '6549.2' '9564.2' '9455.2' '9604.2' '8360.2' '7859.2'\n",
      " '7817.2' '8681.2' '7932.2' '7935.2' '7944.2' '7958.2' '8012.2' '8865.2'\n",
      " '8830.2']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Ensure sample_id is a string\n",
    "df_morpho_cleaned['sample_id'] = df_morpho_cleaned['sample_id'].astype(str).str.strip()\n",
    "\n",
    "# Step 2: Identify sample_ids that contain a decimal point\n",
    "decimal_sample_ids = df_morpho_cleaned[df_morpho_cleaned['sample_id'].str.contains(r'\\.\\d+$', regex=True)]\n",
    "\n",
    "# Step 3: Output results\n",
    "print(f\"üîç Found {len(decimal_sample_ids)} sample_id(s) with decimal values.\\n\")\n",
    "\n",
    "# Optional: Print the unique sample_ids with decimals\n",
    "print(\"üßæ Affected sample_ids:\")\n",
    "print(decimal_sample_ids['sample_id'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "df_morpho_cleaned.to_csv(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_clean/morpho_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## take out all decimals in sample_id (keep as just whole number)\n",
    "# Function to truncate decimal part and return string\n",
    "def truncate_to_int_string(x):\n",
    "    try:\n",
    "        return str(int(float(x)))\n",
    "    except:\n",
    "        return str(x).strip()  # fallback to string version if it can't convert\n",
    "\n",
    "# Apply to sample_id column\n",
    "df_morpho_cleaned['sample_id'] = df_morpho_cleaned['sample_id'].apply(truncate_to_int_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 708 sample_id(s) in morphology_cleaning but not in samples_check:\n",
      "['1024', '10338', '10339', '10340', '10341', '10342', '10343', '10344', '10345', '10346', '10347', '10348', '10377', '104', '10443', '10444', '10445', '10448', '10449', '10450', '10451', '10467', '10468', '10469', '10470', '10472', '105', '10558', '10562', '106', '107', '1089', '10901', '10902', '10903', '10904', '1091', '1092', '1094', '1099', '1100', '1108', '1110', '1111', '1117', '1119', '11373', '1138', '1140', '1142', '1144', '1147', '1149', '11492', '11493', '11494', '1152', '1154', '1156', '1158', '1160', '1182', '1183', '1184', '1186', '1187', '1188', '1189', '1190', '1191', '1198', '1200', '1204', '1205', '1206', '1208', '1209', '1211', '1212', '1213', '1215', '1217', '1219', '1220', '1222', '1223', '1224', '1225', '1227', '1229', '1231', '1233', '1234', '1235', '1236', '12450', '12451', '12452', '12494', '12495', '12496', '1250', '1252', '1255', '1256', '1257', '1274', '1275', '1286', '1287', '1288', '1291', '1292', '1298', '1299', '1303', '1323', '1324', '1325', '1328', '1329', '1333', '1335', '1338', '1339', '134', '1340', '1341', '1343', '135', '1350', '1352', '1355', '1356', '136', '1360', '1361', '1364', '1365', '1366', '1375', '1376', '1381', '1382', '1383', '1386', '1388', '13929', '13930', '13931', '13933', '13934', '13949', '1396', '1397', '1398', '1399', '140', '1401', '1403', '1406', '1408', '1412', '1413', '1415', '142', '1424', '1425', '1426', '1434', '14351', '14352', '14353', '1438', '144', '1440', '1441', '1442', '14451', '14452', '14453', '14454', '14455', '14508', '14509', '1452', '1453', '1455', '1456', '1458', '1459', '1460', '1461', '1462', '1463', '1465', '1466', '1473', '1475', '1482', '1483', '1484', '1485', '1487', '1488', '1489', '1490', '14900', '14901', '1493', '1494', '1495', '1496', '1497', '14982', '15314', '15315', '1542', '1543', '1544', '1546', '1548', '15769', '15770', '15771', '15772', '15773', '15774', '15775', '15776', '15777', '15778', '16400', '16401', '16402', '16403', '16404', '16405', '16424', '16425', '16426', '16427', '16428', '16429', '16633', '16634', '16637', '16638', '1678', '1680', '1681', '16826', '16827', '16828', '16829', '16830', '16845', '16846', '16847', '16848', '16849', '16850', '16851', '16852', '16853', '16855', '16867', '16868', '16869', '16870', '16872', '16873', '1699', '1700', '1701', '1702', '1703', '1704', '1711', '1712', '1713', '1718', '17794', '17795', '17796', '17797', '17798', '17799', '17884', '17885', '17886', '17887', '17888', '17889', '17890', '17994', '17995', '17996', '17997', '18025', '18026', '18027', '18028', '18058', '18059', '18060', '18061', '18062', '18063', '18077', '18078', '18079', '18080', '18100', '18101', '18102', '18103', '18104', '18441', '18442', '18443', '18444', '18445', '18446', '18447', '18736', '18737', '18738', '18739', '18740', '18774', '18775', '18776', '18777', '18778', '18779', '19176', '19177', '19178', '19179', '19180', '19181', '19182', '19183', '19184', '19185', '19186', '19187', '19188', '19189', '19190', '19192', '19193', '19194', '19195', '19196', '1924', '19240', '19241', '19242', '19243', '19244', '19251', '19252', '19253', '19254', '19255', '19267', '19268', '19269', '19270', '19271', '19283', '19284', '19285', '19286', '19287', '19298', '19299', '19301', '19302', '19303', '19304', '1932', '19330', '19331', '19332', '19333', '19334', '19371', '19372', '19373', '19374', '19375', '19431', '19432', '19433', '19434', '19435', '19436', '19437', '19438', '19439', '19440', '19448', '19449', '19450', '19451', '19467', '19468', '19469', '19537', '19538', '19539', '19541', '19542', '19543', '19544', '19545', '19550', '19551', '19552', '19553', '19554', '19556', '19557', '19558', '19565', '19566', '19567', '19568', '19569', '19570', '19571', '19572', '19573', '19574', '19575', '19576', '19577', '19579', '19580', '19581', '19582', '19583', '19585', '19586', '19587', '19588', '19654', '19655', '19656', '20880', '20881', '20882', '20883', '20891', '20892', '20893', '20894', '20895', '20896', '20897', '20898', '20899', '20905', '20906', '20907', '20908', '20909', '20910', '20911', '20912', '20914', '20915', '20916', '20917', '20918', '20919', '20920', '20921', '20925', '20926', '20927', '20955', '20956', '20957', '20958', '20959', '20970', '20971', '20972', '20973', '21002', '21003', '21004', '21005', '21006', '21007', '21528', '21529', '21530', '21531', '21533', '21534', '21535', '21536', '21542', '21543', '21544', '21545', '21546', '21547', '21548', '21555', '21556', '21557', '21558', '21575', '21576', '21577', '21603', '21604', '21605', '21606', '21613', '21614', '21615', '21616', '21617', '21618', '21619', '21620', '21627', '21629', '21630', '21634', '21635', '21636', '21637', '21642', '21643', '21644', '2165', '21652', '21653', '21654', '21666', '21667', '21668', '21669', '21670', '21671', '21672', '21673', '21674', '21704', '21705', '21706', '21707', '21708', '21711', '21712', '21713', '21714', '21715', '21716', '21718', '21719', '21720', '21721', '21724', '21725', '21726', '21727', '21728', '21732', '21733', '21734', '21735', '21736', '21744', '21745', '21746', '21747', '2410', '2668', '2709', '299', '3027', '3029', '303', '3227', '3229', '3231', '3506', '3568', '3919', '4241', '437', '438', '439', '4957', '4999', '50', '5000', '5010', '5036', '5045', '5052', '51', '5389', '5431', '5686', '5767', '577', '578', '579', '5792', '5886', '598', '599', '600', '607', '608', '609', '611', '612', '6332', '6335', '6336', '6339', '6341', '6342', '6356', '6357', '6358', '6373', '6375', '6376', '6386', '6387', '6388', '6390', '6391', '6436', '6438', '6439', '6475', '6476', '6477', '6478', '6479', '6481', '6667', '6668', '6675', '6704', '6705', '6707', '6710', '6824', '6836', '6840', '694', '695', '696', '7026', '7127', '7140', '7257', '7265', '7278', '7284', '7352', '7395', '74', '75', '76', '7815', '7931', '8000', '8057', '8058', '8066', '8067', '8068', '8077', '8332', '8336', '8339', '8431', '8493', '8565', '86', '87', '88', '89', '90', '91', '9202', '95', '9561', '96', '97', '98', '9986', 'NULL']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/79mdnyy56_xc3g1jvp9wf4_80000gn/T/ipykernel_2227/2573390024.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples_check['sample_id'] = samples_check['sample_id'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "#sample_id(s) in morphology_cleaning but not in samples_check:\n",
    "# Ensure both sample_id columns are strings for accurate comparison\n",
    "df_morpho_cleaned['sample_id'] = df_morpho_cleaned['sample_id'].astype(str)\n",
    "samples_check['sample_id'] = samples_check['sample_id'].astype(str)\n",
    "\n",
    "# Get unique sample_id values in both DataFrames\n",
    "morph_sample_ids = set(df_morpho_cleaned['sample_id'].dropna())\n",
    "samples_check_ids = set(samples_check['sample_id'].dropna())\n",
    "\n",
    "# Find sample_ids in morphology_cleaning but not in samples_check\n",
    "sample_ids_not_in_check = sorted(morph_sample_ids - samples_check_ids)\n",
    "\n",
    "# Print results\n",
    "print(f\"‚úÖ Found {len(sample_ids_not_in_check)} sample_id(s) in morphology_cleaning but not in samples_check:\")\n",
    "print(sample_ids_not_in_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**consider just adding these 708 sample_id's to samples df (since they all have a related location)**\n",
    "\n",
    "as composite keys?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deciding what to do about missing sample_id's in morphology\n",
    "--> assumption: these horizons were not sampled, but just reviewed visually\n",
    "--> incorrect bc there are over 9000 sample_id's from sample df that do not have a morpho_id\n",
    "\n",
    "--> will probably just use profile record_id for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab_sample_id</th>\n",
       "      <th>analysis_id</th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>EG</th>\n",
       "      <th>thick_clay</th>\n",
       "      <th>fine_clay</th>\n",
       "      <th>silt</th>\n",
       "      <th>clay</th>\n",
       "      <th>Eq_Hum</th>\n",
       "      <th>...</th>\n",
       "      <th>Ta</th>\n",
       "      <th>W</th>\n",
       "      <th>Pt</th>\n",
       "      <th>Au</th>\n",
       "      <th>Hg</th>\n",
       "      <th>Tl</th>\n",
       "      <th>Pb</th>\n",
       "      <th>Bi</th>\n",
       "      <th>Th</th>\n",
       "      <th>U</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>10999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.700001</td>\n",
       "      <td>32.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>11000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.799999</td>\n",
       "      <td>35.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>11.4</td>\n",
       "      <td>6.4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>11001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>46.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>11.1</td>\n",
       "      <td>6.3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>11002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.599998</td>\n",
       "      <td>41.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15.4</td>\n",
       "      <td>5.2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B_101/62_5_21</td>\n",
       "      <td>B_101/62_5_2</td>\n",
       "      <td>11003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.799999</td>\n",
       "      <td>47.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>14.5</td>\n",
       "      <td>7.3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lab_sample_id    analysis_id    horizon_id sample_id  EG  thick_clay  \\\n",
       "0              1   B_101/62_1_1  B_101/62_1_1     10999 NaN   61.700001   \n",
       "1              2   B_101/62_2_1  B_101/62_2_1     11000 NaN   52.799999   \n",
       "2              3   B_101/62_3_1  B_101/62_3_1     11001 NaN   42.500000   \n",
       "3              4   B_101/62_4_1  B_101/62_4_1     11002 NaN   42.599998   \n",
       "4              5  B_101/62_5_21  B_101/62_5_2     11003 NaN   36.799999   \n",
       "\n",
       "   fine_clay  silt  clay  Eq_Hum  ...  Ta   W  Pt  Au  Hg  Tl  Pb  Bi  Th   U  \n",
       "0       32.8   0.2   5.3     4.6  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "1       35.1   0.7  11.4     6.4  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "2       46.2   0.2  11.1     6.3  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "3       41.8   0.2  15.4     5.2  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "4       47.5   1.2  14.5     7.3  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows missing analysis_id after merge: 3965\n",
      "           horizon_id sample_id_x  profile  horizon_layer  upper_depth  \\\n",
      "1399    Cb_127/59_3_0        NULL   127_59            3.0         17.0   \n",
      "7752   Mj_267c/63_1_0        NULL  267C_63            1.0          0.0   \n",
      "1969    Cb_226/59_2_0        NULL   226_59            2.0         12.0   \n",
      "7125    Mj_215/63_5_0        NULL   215_63            5.0        103.0   \n",
      "6118    Mj_122/63_4_0        NULL   122_63            4.0         36.0   \n",
      "5929   Mj_107c/63_1_0        NULL  107C_63            1.0          0.0   \n",
      "10356  UZ_244c/60_2_0        NULL  244C_60            2.0          7.0   \n",
      "6799    Mj_189/63_1_0        NULL   189_63            1.0          0.0   \n",
      "7199    Mj_221/63_1_0        NULL   221_63            1.0          0.0   \n",
      "8674    Mj_352/63_7_0        NULL   352_63            7.0        129.0   \n",
      "\n",
      "       lower_depth                                     dry_color_name dry_hue  \\\n",
      "1399          31.0                                              Pardo    10YR   \n",
      "7752          20.0                               Pardo a pardo-escuro   7,5YR   \n",
      "1969          30.0                           Pardo-acinzentado-escuro    10YR   \n",
      "7125         146.0                                Pardo-claro a pardo   7,5YR   \n",
      "6118          60.0                                            Laranja     5YR   \n",
      "5929          17.0                                Pardo a pardo-claro    10YR   \n",
      "10356         18.0                       Pardo a pardo-escuro a pardo    10YR   \n",
      "6799          15.0                                    Cinzento-escuro    10YR   \n",
      "7199          11.0  Pardo-acinzentado com tom pardo-acinzentado-es...    10YR   \n",
      "8674         185.0                                                NaN     NaN   \n",
      "\n",
      "       dry_value  dry_chroma  ...         pore_quantity        pore_diameter  \\\n",
      "1399         5.0         3.0  ...         Quase fechado                  NaN   \n",
      "7752         4.0         2.0  ...          Pouco poroso          Muito finos   \n",
      "1969         4.0         2.0  ...          Pouco poroso                  NaN   \n",
      "7125         6.0         4.0  ...          Pouco poroso          Muito finos   \n",
      "6118         5.0         6.0  ...  Moderadamente poroso  Muito finos e finos   \n",
      "5929         5.0         3.0  ...  Moderadamente poroso  Muito finos e finos   \n",
      "10356        4.5         3.0  ...          Pouco poroso                  NaN   \n",
      "6799         4.0         1.0  ...          Pouco poroso          Muito finos   \n",
      "7199         5.0         2.0  ...          Pouco poroso  Muito finos e finos   \n",
      "8674         NaN         NaN  ...               Fechado                  NaN   \n",
      "\n",
      "       pore_shape                              root_quantity  \\\n",
      "1399          NaN               Algumas finas e raras medias   \n",
      "7752          NaN              Muitas finas e algumas medias   \n",
      "1969          NaN                                    Algumas   \n",
      "7125          NaN                                     Poucas   \n",
      "6118          NaN                Poucas finas e raras medias   \n",
      "5929          NaN    Muitas finas e algumas medias e grossas   \n",
      "10356         NaN              Muitas finas e algumas medias   \n",
      "6799          NaN                                     Poucas   \n",
      "7199          NaN  Bastantes finas e poucas medias e grossas   \n",
      "8674          NaN                                 Sem raizes   \n",
      "\n",
      "                 root_diameter  \\\n",
      "1399                       NaN   \n",
      "7752                       NaN   \n",
      "1969            Finas (< 1 mm)   \n",
      "7125            Finas (< 1 mm)   \n",
      "6118                       NaN   \n",
      "5929                       NaN   \n",
      "10356                      NaN   \n",
      "6799   Finas, medias e grossas   \n",
      "7199                       NaN   \n",
      "8674                       NaN   \n",
      "\n",
      "                                         moisture_degree Unnamed: 46  \\\n",
      "1399                                          Quase seco         NaN   \n",
      "7752                                                Seco         NaN   \n",
      "1969                                                Seco         NaN   \n",
      "7125                                              Fresco         NaN   \n",
      "6118                                                Seco         NaN   \n",
      "5929                                                Seco         NaN   \n",
      "10356                                         Quase seco         NaN   \n",
      "6799                                                Seco         NaN   \n",
      "7199                                                Seco         NaN   \n",
      "8674   Muito humido a humido e muito humido com a pro...         NaN   \n",
      "\n",
      "      profile_record_id analysis_id sample_id_y  \n",
      "1399               <NA>         NaN         NaN  \n",
      "7752               <NA>         NaN         NaN  \n",
      "1969               <NA>         NaN         NaN  \n",
      "7125               <NA>         NaN         NaN  \n",
      "6118               <NA>         NaN         NaN  \n",
      "5929               <NA>         NaN         NaN  \n",
      "10356              <NA>         NaN         NaN  \n",
      "6799               <NA>         NaN         NaN  \n",
      "7199               <NA>         NaN         NaN  \n",
      "8674               <NA>         NaN         NaN  \n",
      "\n",
      "[10 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ensure sample_id columns are strings and stripped\n",
    "df_morpho_cleaned['sample_id'] = df_morpho_cleaned['sample_id'].astype(str).str.strip()\n",
    "merged1['sample_id'] = merged1['sample_id'].astype(str).str.strip()\n",
    "\n",
    "# Merge to add analysis_id, sample_id (from merged1) onto df_morpho_cleaned based on horizon_id\n",
    "df_morpho_enriched = df_morpho_cleaned.merge(\n",
    "    merged1[['horizon_id', 'analysis_id', 'sample_id']],\n",
    "    on='horizon_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Rows with missing analysis_id mean df_morpho_cleaned sample_id is not in merged1\n",
    "missing_analysis = df_morpho_enriched[df_morpho_enriched['analysis_id'].isna()]\n",
    "print(f\"Number of rows missing analysis_id after merge: {len(missing_analysis)}\")\n",
    "\n",
    "# Inspect some missing rows to understand the issue\n",
    "print(missing_analysis.sample(min(10, len(missing_analysis))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Duplicated horizon_ids in merged1: 452\n",
      "     lab_sample_id    analysis_id    horizon_id sample_id   EG  thick_clay  \\\n",
      "4                5  B_101/62_5_21  B_101/62_5_2     11003  NaN   36.799999   \n",
      "5                6  B_101/62_5_22  B_101/62_5_2     11004  NaN   45.099998   \n",
      "75              76  B_148/62_2_21  B_148/62_2_2     11285  NaN   21.799999   \n",
      "76              77  B_148/62_2_22  B_148/62_2_2     11286  NaN   22.000000   \n",
      "85              86   B_15/62_5_21   B_15/62_5_2     10843  NaN   27.100000   \n",
      "86              87   B_15/62_5_22   B_15/62_5_2     10844  NaN   27.200001   \n",
      "105            106  B_154/61_3_21  B_154/61_3_2     10335  NaN   17.799999   \n",
      "106            107  B_154/61_3_22  B_154/61_3_2     10336  NaN   18.700001   \n",
      "143            144  B_187/61_4_21  B_187/61_4_2     10428  1.0   31.900000   \n",
      "144            145  B_187/61_4_22  B_187/61_4_2     10429  1.0   33.000000   \n",
      "\n",
      "     fine_clay  silt  clay  Eq_Hum  ...  Ta   W  Pt  Au  Hg  Tl  Pb  Bi  Th  \\\n",
      "4         47.5   1.2  14.5     7.3  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "5         41.8   1.4  11.7     0.0  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "75        73.7   2.3   2.1     0.0  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "76        73.4   3.1   1.5     0.0  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "85        45.1   1.6  26.1     0.0  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "86        44.7   2.2  25.9     0.0  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "105       10.2   5.6  66.4    27.6  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "106       11.7   4.6  65.1     0.0  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "143       49.0   2.9  16.2     0.0  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "144       46.8   3.0  17.2     0.0  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "\n",
      "      U  \n",
      "4   NaN  \n",
      "5   NaN  \n",
      "75  NaN  \n",
      "76  NaN  \n",
      "85  NaN  \n",
      "86  NaN  \n",
      "105 NaN  \n",
      "106 NaN  \n",
      "143 NaN  \n",
      "144 NaN  \n",
      "\n",
      "[10 rows x 70 columns]\n"
     ]
    }
   ],
   "source": [
    "#452 duplicate horizon_id in analysis df\n",
    "# are there multiple horizon_id's per analysis_id?\n",
    "# is it an error that can resolved w new unique id\n",
    "dupes = merged1[merged1['horizon_id'].duplicated(keep=False)]\n",
    "print(f\"‚ö†Ô∏è Duplicated horizon_ids in merged1: {dupes['horizon_id'].nunique()}\")\n",
    "dupes_sorted = dupes.sort_values('horizon_id')\n",
    "print(dupes_sorted.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   total_in_morpho  total_in_merged1  dupes_in_merged1  missing_in_merged1\n",
      "0            10434              7292               506                3965\n"
     ]
    }
   ],
   "source": [
    "missing_in_merged1 = df_morpho_cleaned[~df_morpho_cleaned['horizon_id'].isin(merged1['horizon_id'])]\n",
    "\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'total_in_morpho': [df_morpho_cleaned['horizon_id'].nunique()],\n",
    "    'total_in_merged1': [merged1['horizon_id'].nunique()],\n",
    "    'dupes_in_merged1': [merged1['horizon_id'].duplicated().sum()],\n",
    "    'missing_in_merged1': [len(missing_in_merged1)]\n",
    "})\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soil Type Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soil_type_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>CEP_GR</th>\n",
       "      <th>CEP_NAME</th>\n",
       "      <th>FAO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1_51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1_57</td>\n",
       "      <td>Aridicos</td>\n",
       "      <td>Aridicos com calcario Pardo-cinzentos</td>\n",
       "      <td>CLha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   soil_type_id profile    CEP_GR                               CEP_NAME   FAO\n",
       "0             1    1_51       NaN                                    NaN   NaN\n",
       "1             2    1_57  Aridicos  Aridicos com calcario Pardo-cinzentos  CLha"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns\n",
    "soil_type = soil_profile.copy()\n",
    "\n",
    "soil_type.rename(columns={\n",
    "    'Perfil': 'profile',\n",
    "    'Agrupamento': 'grouping',\n",
    "    'Pro': 'province',\n",
    "    'Pa√≠s': 'country',\n",
    "    'Local': 'location',\n",
    "    'DATA': 'date',\n",
    "    'CEP_NOME': 'CEP_NAME',\n",
    "}, inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "soil_type_cleaning = soil_type.drop(columns=[\n",
    "    'REF', 'province', 'country', 'location', 'DESCRITOR1', 'DESCRITOR2', 'DESCRITOR3',\n",
    "    'date', 'Fase', 'D_INSER√áAO', 'Publica√ß√£o', 'WRB_old', 'Miss√£o'\n",
    "], errors='ignore')  # use errors='ignore' in case some columns were already missing\n",
    "\n",
    "# Add a new Primary Key ID column starting from 1\n",
    "soil_type_cleaning.insert(0, 'soil_type_id', range(1, len(soil_type_cleaning) + 1))\n",
    "\n",
    "# Drop accents\n",
    "import unicodedata\n",
    "\n",
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        return ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    return text\n",
    "\n",
    "# Apply to all cells in the DataFrame\n",
    "soil_type_cleaning = soil_type_cleaning.applymap(remove_accents)\n",
    "\n",
    "# Replace / with _ and strip/shorten 'profile' BEFORE slicing into site_info_clean\n",
    "soil_type_cleaning['profile'] = (\n",
    "    soil_type_cleaning['profile']\n",
    "    .astype(str)\n",
    "    .str.replace('/', '_')\n",
    "    .str.strip()\n",
    "    .str[:20]\n",
    ")\n",
    "\n",
    "# Keep only relevant columns\n",
    "soil_type_clean = soil_type_cleaning[[\n",
    "    'soil_type_id',\n",
    "    'profile',\n",
    "    'CEP_GR',\n",
    "    'CEP_NAME',\n",
    "    'FAO'\n",
    "]]\n",
    "\n",
    "# Preview\n",
    "soil_type_clean.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# check duplicates\n",
    "# Filter out and print duplicated samples (including the first occurrence)\n",
    "duplicated_values_st = soil_type_clean['profile'][soil_type_clean['profile'].duplicated()].unique()\n",
    "print(duplicated_values_st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Unique 'profile' values equal to string 'NULL': []\n",
      "üî¢ Number of 'profile' values equal to string 'NULL': 0\n"
     ]
    }
   ],
   "source": [
    "# Check for profile values that are the string \"NULL\"\n",
    "null_string_values_st = soil_type_clean[soil_type_clean['profile'] == \"NULL\"]['profile'].unique()\n",
    "print(\"üîç Unique 'profile' values equal to string 'NULL':\", null_string_values_st)\n",
    "\n",
    "null_string_count_st = (soil_type_clean['profile'] == \"NULL\").sum()\n",
    "print(f\"üî¢ Number of 'profile' values equal to string 'NULL': {null_string_count_st}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total profiles in site_info_clean: 4321\n",
      "Total profiles in soil_type_clean: 2518\n",
      "\n",
      "‚úÖ Profiles in both: 2376\n",
      "‚ùå Profiles only in site_info_clean: 1945 (45.0%)\n",
      "‚ùå Profiles only in soil_type_clean: 142 (5.6%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1. Define cleaning function for profile\n",
    "# ---------------------------------------\n",
    "def clean_profile_column(series):\n",
    "    return (\n",
    "        series.astype(str)\n",
    "        .str.replace('/', '_')\n",
    "        .str.strip()\n",
    "        .str.upper()\n",
    "        .str[:20]\n",
    "    )\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2. Safely clean 'profile' columns\n",
    "# ---------------------------------------\n",
    "site_info_clean = site_info_clean.copy()\n",
    "soil_type_clean = soil_type_clean.copy()\n",
    "\n",
    "site_info_clean.loc[:, 'profile'] = clean_profile_column(site_info_clean['profile'])\n",
    "soil_type_clean.loc[:, 'profile'] = clean_profile_column(soil_type_clean['profile'])\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3. Create sets of unique profiles\n",
    "# ---------------------------------------\n",
    "site_profiles = set(site_info_clean['profile'].unique())\n",
    "soil_profiles = set(soil_type_clean['profile'].unique())\n",
    "\n",
    "# ---------------------------------------\n",
    "# 4. Compare overlaps and mismatches\n",
    "# ---------------------------------------\n",
    "only_in_site_info = site_profiles - soil_profiles\n",
    "only_in_soil_type = soil_profiles - site_profiles\n",
    "in_both = site_profiles & soil_profiles\n",
    "\n",
    "# ---------------------------------------\n",
    "# 5. Report results\n",
    "# ---------------------------------------\n",
    "print(f\"Total profiles in site_info_clean: {len(site_profiles)}\")\n",
    "print(f\"Total profiles in soil_type_clean: {len(soil_profiles)}\")\n",
    "print()\n",
    "print(f\"‚úÖ Profiles in both: {len(in_both)}\")\n",
    "print(f\"‚ùå Profiles only in site_info_clean: {len(only_in_site_info)} \"\n",
    "      f\"({len(only_in_site_info) / len(site_profiles) * 100:.1f}%)\")\n",
    "print(f\"‚ùå Profiles only in soil_type_clean: {len(only_in_soil_type)} \"\n",
    "      f\"({len(only_in_soil_type) / len(soil_profiles) * 100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export for investigation\n",
    "\n",
    "unmatched_site_info = site_info_clean[site_info_clean['profile'].isin(only_in_site_info)]\n",
    "unmatched_site_info.to_csv(\"unmatched_profiles_site_info.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save clean table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>climate_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>ID</th>\n",
       "      <th>mean_annual_temp</th>\n",
       "      <th>mean_annual_precip</th>\n",
       "      <th>koppen_climate</th>\n",
       "      <th>thornthwaite_climate</th>\n",
       "      <th>hydric_regime</th>\n",
       "      <th>thermal_regime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1_57</td>\n",
       "      <td>2770</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arido</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1_59</td>\n",
       "      <td>48</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Humido</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1_61</td>\n",
       "      <td>1618</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1_63</td>\n",
       "      <td>881</td>\n",
       "      <td>21.5</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>Tropical chuvoso com estacao seca no Inverno, ...</td>\n",
       "      <td>Humido</td>\n",
       "      <td>Tropustico udico</td>\n",
       "      <td>Iso-Hipertermico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1_64</td>\n",
       "      <td>1750</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   climate_id profile    ID mean_annual_temp mean_annual_precip  \\\n",
       "0           1    1_57  2770                                       \n",
       "1           2    1_59    48                                       \n",
       "2           3    1_61  1618                                       \n",
       "3           4    1_63   881             21.5             1500.0   \n",
       "4           5    1_64  1750                                       \n",
       "\n",
       "                                      koppen_climate thornthwaite_climate  \\\n",
       "0                                                NaN                Arido   \n",
       "1                                                NaN               Humido   \n",
       "2                                                NaN                  NaN   \n",
       "3  Tropical chuvoso com estacao seca no Inverno, ...               Humido   \n",
       "4                                                NaN                  NaN   \n",
       "\n",
       "      hydric_regime    thermal_regime  \n",
       "0               NaN               NaN  \n",
       "1               NaN               NaN  \n",
       "2               NaN               NaN  \n",
       "3  Tropustico udico  Iso-Hipertermico  \n",
       "4               NaN               NaN  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load original Excel data\n",
    "profile_loc = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Perfis_local.xlsx\")\n",
    "\n",
    "# Create a copy for cleaning\n",
    "climate_features_cleaning = profile_loc.copy()\n",
    "\n",
    "# Mapping from climate codes to descriptions\n",
    "code_to_description = {\n",
    "    \"B1\": \"H√∫mido\",\n",
    "    \"B2\": \"H√∫mido\",\n",
    "    \"B3\": \"H√∫mido\",\n",
    "    \"B4\": \"H√∫mido\",\n",
    "    \"C1\": \"Sub-h√∫mido seco\",\n",
    "    \"C2\": \"Sub-h√∫mido chuvoso\",\n",
    "    \"D\": \"Semi-√°rido\",\n",
    "    \"E\": \"√Årido\",\n",
    "    \"Aw\": \"Tropical chuvoso com esta√ß√£o seca no Inverno, de savana\",\n",
    "    \"BSw\": \"Seco de estepe, com chuva predominante no Ver√£o\",\n",
    "    \"BWw\": \"Seco de deserto, com chuva predominante no Ver√£o\",\n",
    "    \"Cw\": \"Mesot√©rmico h√∫mido com esta√ß√£o seca no Inverno\",\n",
    "    \"ARe\": \"Ar√≠dico extremo\",\n",
    "    \"ARf\": \"Ar√≠dico fraco\",\n",
    "    \"ARt\": \"Ar√≠dico t√≠pico\",\n",
    "    \"tUDs\": \"Temp√∫dico seco\",\n",
    "    \"tUSh\": \"Temp√∫stico h√∫mido\",\n",
    "    \"tUSt\": \"Temp√∫stico t√≠pico\",\n",
    "    \"TUDs\": \"Trop√∫dico seco\",\n",
    "    \"TUSa\": \"Trop√∫stico ar√≠dico\",\n",
    "    \"TUSu\": \"Trop√∫stico √∫dico\",\n",
    "    \"TUSt\": \"Tropustico t√≠pico\",\n",
    "    \"H\": \"Hipert√©rmico\",\n",
    "    \"iH\": \"Iso-Hipert√©rmico\",\n",
    "    \"iT\": \"Iso-T√©rmico\",\n",
    "    \"T\": \"T√©rmico\"\n",
    "}\n",
    "\n",
    "# Replace climate codes with descriptions\n",
    "for col in [\"CL_THORNTH\", \"CL_KOPPEN\", \"REG_H√çDRIC\", \"REG_T√âRMIC\"]:\n",
    "    climate_features_cleaning[col] = climate_features_cleaning[col].replace(code_to_description)\n",
    "\n",
    "# Function to average values like \"21-22\" -> 21.5\n",
    "def average_range(value):\n",
    "    if isinstance(value, str) and '-' in value:\n",
    "        try:\n",
    "            nums = [float(x.strip()) for x in value.split('-')]\n",
    "            return sum(nums) / len(nums)\n",
    "        except:\n",
    "            return None\n",
    "    try:\n",
    "        return float(value)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply to temperature and precipitation columns\n",
    "climate_features_cleaning[\"TMA\"] = climate_features_cleaning[\"TMA\"].apply(average_range)\n",
    "climate_features_cleaning[\"PMA\"] = climate_features_cleaning[\"PMA\"].apply(average_range)\n",
    "\n",
    "# Rename columns for clarity\n",
    "climate_features_cleaning.rename(columns={\n",
    "    'PERFIL': 'profile',\n",
    "    'CL_THORNTH': 'thornthwaite_climate',\n",
    "    'CL_KOPPEN': 'koppen_climate',\n",
    "    'TMA': 'mean_annual_temp',\n",
    "    'PMA': 'mean_annual_precip',\n",
    "    'REG_H√çDRIC': 'hydric_regime',\n",
    "    'REG_T√âRMIC': 'thermal_regime'\n",
    "}, inplace=True)\n",
    "\n",
    "# ensure consistent profile formatting\n",
    "climate_features_cleaning['profile'] = climate_features_cleaning['profile'].astype(str).str.replace('/', '_').str.strip().str[:20]\n",
    "\n",
    "# Drop accents from text values\n",
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        return ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    return text\n",
    "\n",
    "climate_features_cleaning = climate_features_cleaning.applymap(remove_accents)\n",
    "\n",
    "# Add primary key column\n",
    "climate_features_cleaning.insert(0, 'climate_id', range(1, len(climate_features_cleaning) + 1))\n",
    "\n",
    "# Replace empty strings or nulls in numeric columns with \\N (Postgres null)\n",
    "for col in ['mean_annual_temp', 'mean_annual_precip']:\n",
    "    climate_features_cleaning[col] = climate_features_cleaning[col].replace(\n",
    "        ['', ' ', 'NULL', None, pd.NA, pd.NaT, 'nan', float('nan')], ''\n",
    "    )\n",
    "\n",
    "# Select and reorder relevant columns for export\n",
    "climate_features_clean = climate_features_cleaning[[\n",
    "    'climate_id',\n",
    "    'profile',\n",
    "    'ID',\n",
    "    'mean_annual_temp',\n",
    "    'mean_annual_precip',\n",
    "    'koppen_climate',\n",
    "    'thornthwaite_climate',\n",
    "    'hydric_regime',\n",
    "    'thermal_regime'\n",
    "]]\n",
    "\n",
    "# Preview\n",
    "climate_features_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check FK relationships and datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of each column in profile_clean:\n",
      "\n",
      "climate_id               int64\n",
      "profile                 object\n",
      "ID                       int64\n",
      "mean_annual_temp        object\n",
      "mean_annual_precip      object\n",
      "koppen_climate          object\n",
      "thornthwaite_climate    object\n",
      "hydric_regime           object\n",
      "thermal_regime          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check and print the datatypes of each column\n",
    "print(\"Data types of each column in profile_clean:\\n\")\n",
    "print(climate_features_clean.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topo features table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topo_features_id</th>\n",
       "      <th>ID</th>\n",
       "      <th>profile</th>\n",
       "      <th>slope_code</th>\n",
       "      <th>altitude</th>\n",
       "      <th>aspect</th>\n",
       "      <th>land_surface_temp</th>\n",
       "      <th>dem_elevation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2770</td>\n",
       "      <td>1_57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>1_59</td>\n",
       "      <td>D5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1618</td>\n",
       "      <td>1_61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>881</td>\n",
       "      <td>1_63</td>\n",
       "      <td>D1</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1750</td>\n",
       "      <td>1_64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topo_features_id    ID profile slope_code  altitude aspect  \\\n",
       "0                 1  2770    1_57        NaN      32.0   <NA>   \n",
       "1                 2    48    1_59         D5       NaN   <NA>   \n",
       "2                 3  1618    1_61        NaN       NaN   <NA>   \n",
       "3                 4   881    1_63         D1    1210.0   <NA>   \n",
       "4                 5  1750    1_64        NaN       NaN   <NA>   \n",
       "\n",
       "  land_surface_temp dem_elevation  \n",
       "0              <NA>          <NA>  \n",
       "1              <NA>          <NA>  \n",
       "2              <NA>          <NA>  \n",
       "3              <NA>          <NA>  \n",
       "4              <NA>          <NA>  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a copy for cleaning\n",
    "topo_features_cleaning = profile_loc.copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "topo_features_cleaning.rename(columns={\n",
    "    'PERFIL': 'profile',\n",
    "    'TOPOGRAFIA': 'slope_code',\n",
    "    'ALTITUDE': 'altitude',\n",
    "}, inplace=True)\n",
    "\n",
    "# ensure consistent profile formatting\n",
    "topo_features_cleaning['profile'] = topo_features_cleaning['profile'].astype(str).str.replace('/', '_').str.strip().str[:20]\n",
    "\n",
    "# Add missing columns\n",
    "topo_features_cleaning['aspect'] = pd.NA  \n",
    "topo_features_cleaning['land_surface_temp'] = pd.NA\n",
    "topo_features_cleaning['dem_elevation'] = pd.NA  \n",
    "\n",
    "# Add primary key column\n",
    "topo_features_cleaning.insert(0, 'topo_features_id', range(1, len(topo_features_cleaning) + 1))\n",
    "\n",
    "# Create slope class mapping dictionary\n",
    "slope_code_to_description = {\n",
    "    \"D1\": \"Plano (Declives < 2%)\",\n",
    "    \"D2\": \"Ondulado muito suave (Declives > 2% e < 3%)\",\n",
    "    \"D3\": \"Ondulado suave (Declives > 3% e < 5%)\",\n",
    "    \"D4\": \"Ondulado (Declives > 5% e < 8%)\",\n",
    "    \"D5\": \"Acidentado (Declives > 8% e < 15%)\",\n",
    "    \"D6\": \"Escarpado (Declives >15% e < 30%)\",\n",
    "    \"D7\": \"Montanhoso (Declives > 30%)\"\n",
    "}\n",
    "\n",
    "# Create a mapping DataFrame for slope classes\n",
    "slope_classes_df = pd.DataFrame([\n",
    "    {\"slope_code\": code, \"slope_description\": desc}\n",
    "    for code, desc in slope_code_to_description.items()\n",
    "])\n",
    "\n",
    "#drop accents\n",
    "import unicodedata\n",
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        # Normalize and remove diacritics\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "# Apply to all cells in the DataFrame\n",
    "topo_features_cleaning = topo_features_cleaning.applymap(remove_accents)\n",
    "\n",
    "# Final cleaned topo features table (referencing slope_code, not description)\n",
    "topo_features_clean = topo_features_cleaning[[\n",
    "    'topo_features_id',\n",
    "    'ID',\n",
    "    'profile', \n",
    "    'slope_code',\n",
    "    'altitude',\n",
    "    'aspect',\n",
    "    'land_surface_temp',\n",
    "    'dem_elevation'\n",
    "]]\n",
    "\n",
    "# Preview\n",
    "topo_features_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geological features table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/79mdnyy56_xc3g1jvp9wf4_80000gn/T/ipykernel_2227/2818143663.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  geo_features_clean['profile'] = geo_features_clean['profile'].astype(str).str.replace('/', '_').str.strip().str[:20]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_features_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>ID</th>\n",
       "      <th>geology_id</th>\n",
       "      <th>lithology_id</th>\n",
       "      <th>lithology_1954_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1_57</td>\n",
       "      <td>2770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1_59</td>\n",
       "      <td>48</td>\n",
       "      <td>Oendolongo</td>\n",
       "      <td>pp</td>\n",
       "      <td>Sistema do Maiombe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1_61</td>\n",
       "      <td>1618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1_63</td>\n",
       "      <td>881</td>\n",
       "      <td>Karroo</td>\n",
       "      <td>Cs/Cal</td>\n",
       "      <td>Serie de Cassanje - T2'T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1_64</td>\n",
       "      <td>1750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   geo_features_id profile    ID  geology_id lithology_id  \\\n",
       "0                1    1_57  2770         NaN            d   \n",
       "1                2    1_59    48  Oendolongo           pp   \n",
       "2                3    1_61  1618         NaN          NaN   \n",
       "3                4    1_63   881      Karroo       Cs/Cal   \n",
       "4                5    1_64  1750         NaN          NaN   \n",
       "\n",
       "           lithology_1954_id  \n",
       "0                        NaN  \n",
       "1         Sistema do Maiombe  \n",
       "2                        NaN  \n",
       "3  Serie de Cassanje - T2'T1  \n",
       "4                        NaN  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load cleaned geo features data\n",
    "geo_features_cleaning = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Perfis_local.xlsx\")\n",
    "\n",
    "# Rename columns\n",
    "geo_features_cleaning.rename(columns={\n",
    "    'PERFIL': 'profile',\n",
    "    'GEOLOGIA': 'geology_id',\n",
    "    'LITOLOGIA': 'lithology_id',\n",
    "    'LITOLOGIA_1954': 'lithology_1954_id',\n",
    "}, inplace=True)\n",
    "\n",
    "# Add primary key column\n",
    "geo_features_cleaning.insert(0, 'geo_features_id', range(1, len(geo_features_cleaning) + 1))\n",
    "\n",
    "# Final normalized geo_features table (with codes as foreign keys)\n",
    "geo_features_clean = geo_features_cleaning[[\n",
    "    'geo_features_id',\n",
    "    'profile',\n",
    "    'ID',\n",
    "    'geology_id',\n",
    "    'lithology_id',\n",
    "    'lithology_1954_id'\n",
    "]]\n",
    "\n",
    "# ensure consistent profile formatting\n",
    "geo_features_clean['profile'] = geo_features_clean['profile'].astype(str).str.replace('/', '_').str.strip().str[:20]\n",
    "\n",
    "#drop accents\n",
    "import unicodedata\n",
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        # Normalize and remove diacritics\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "# Apply to all cells in the DataFrame\n",
    "geo_features_clean = geo_features_clean.applymap(remove_accents)\n",
    "\n",
    "#preview\n",
    "geo_features_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mapping tables\n",
    "# Mappings for geology\n",
    "geology_mapping = {\n",
    "    \"Kalahari\": \"Sistema do Kalahari\",\n",
    "    \"Superficiais\": \"Forma√ß√µes Superficiais\",\n",
    "    \"Karroo\": \"Sistema do Karroo\",\n",
    "    \"Bembe\": \"Sistema do Bembe\",\n",
    "    \"Oendolongo\": \"Sistema do Oendolongo\",\n",
    "    \"Base\": \"Complexo de base\",\n",
    "    \"Proteroz√≥ico\": \"Proteroz√≥ico\",\n",
    "    \"Pleistoc√©nico\": \"Pleistoc√©nico\",\n",
    "    \"Terci√°rio\": \"Terci√°rio (m√©dio e inferior)\",\n",
    "    \"TQ\": \"Quatern√°rio e Terci√°rio superior\",\n",
    "    \"Cret√°cio\": \"\",  # no description provided\n",
    "    \"RPKS\": \"Recente Plistoc√©nico e Kalahari Superior\"\n",
    "}\n",
    "\n",
    "# Mappings for lithology_1954\n",
    "lithology_1954_mapping = {\n",
    "    \"Œ≥\": \"Granitos, Granodioritos e Quartzodioritos\",\n",
    "    \"PL\": \"Xistos, metaquartzitos, conglomerados, arcoses, ect.\",\n",
    "    \"Œª\": \"Rochas eruptivas indeterminadas\",\n",
    "    \"Œ¥p\": \"Doleritos, doleritos pigeon√≠ticos\",\n",
    "    \"Œ¥ab\": \"Diabases, diabases albito-cloriticas\",\n",
    "    \"Œµ\": \"Noritos, gabros e peridotitos\",\n",
    "    \"JK\": \"Composto de conglomerados, areias, cascalhos do Kalahari\",\n",
    "    \"C\": \"S√©rie Xisto - calc√°ria\",\n",
    "    \"Cal\": \"Sedimentos arenosos n√£o consolidados\",\n",
    "    \"K\": \"S√©rie xisto - gresosa\",\n",
    "    \"RT\": \"N√£o diferenciado\",\n",
    "    \"œÉ\": \"Sienitos, sienitos nefel√≠nicos\",\n",
    "    \"Q\": \"Dep√≥sitos fossil√≠feros\",\n",
    "    \"CS\": \"Grande conglomerado e s√©rie de Mwashya\"\n",
    "}\n",
    "\n",
    "# Mappings for lithology\n",
    "lithology_mapping = {\n",
    "    \"a\": \"Rochas aren√°ceas consolidadas\",\n",
    "    \"aq\": \"Gr√©s quartz√≠ticos do Oendolongo\",\n",
    "    \"b\": \"Rochas eruptivas b√°sicas\",\n",
    "    \"c\": \"Rochas sedimentares consolidadas calc√°rias\",\n",
    "    \"c'\": \"Rochas sedimentares n√£o consolidadas calc√°rias\",\n",
    "    \"cg\": \"Rochas cristalof√≠licas argil√°ceas\",\n",
    "    \"d\": \"Sedimentos n√£o consolidados de origem marinha\",\n",
    "    \"dc\": \"Dep√≥sitos coluvionares\",\n",
    "    \"dr\": \"Dior√≠tos\",\n",
    "    \"e\": \"Rochas sedimentares consolidadas n√£o calc√°rias\",\n",
    "    \"g\": \"Rochas argil√°ceas consolidadas n√£o calc√°rias\",\n",
    "    \"g'\": \"Rochas argil√°ceas n√£o consolidadas n√£o calc√°rias\",\n",
    "    \"g''\": \"Rochas cristalinas pouco micas em quartzo\",\n",
    "    \"gp\": \"Rochas do  complexo gabro-plagioclast√≠co\",\n",
    "    \"k\": \"Sedimentos n√£o consolidados grosseiros do Kalahari\",\n",
    "    \"m\": \"Rochas sedimentares n√£o consolidadas calco-gips√≠feras\",\n",
    "    \"m'\": \"Dep√≥sitos coluvionares margosos\",\n",
    "    \"mm\": \"Materiais mistos\",\n",
    "    \"n\": \"Sedimentos n√£o consolidados de origem continental\",\n",
    "    \"nd\": \"n√£o descrito\",\n",
    "    \"pp\": \"Sedimentos n√£o consolidados grosseiros plio-plistoc√©nicos\",\n",
    "    \"q\": \"Rochas cristalinas quartz√≠feras\",\n",
    "    \"q'\": \"Materiais redistribu√≠dos provenientes de desagrega√ß√£o rochas crist. quartz√≠feras\",\n",
    "    \"qf\": \"Quartzitos ferruginosos do Oendolongo\",\n",
    "    \"r\": \"Sedimentos grosseiros n√£o especificados\",\n",
    "    \"s\": \"Sienitos\",\n",
    "    \"sx\": \"Forma√ß√µes (ou rochas) sedimentares n√£o especificadas\",\n",
    "    \"sx1\": \"Rochas sedimentares consolidadas com e sem calc√°rio\",\n",
    "    \"sx2\": \"Rochas sedimentares consolidadas\",\n",
    "    \"v\": \"Materiais vulc√¢nicos\",\n",
    "    \"v'\": \"Rochas do complexo alcalino e/ou carboat√≠tico\",\n",
    "    \"x\": \"Rochas consolidadas n√£o especificadas\",\n",
    "    \"xm\": \"Xistos metam√≥rficos\",\n",
    "    \"xq\": \"Rochas cristalinas n√£o especificadas\",\n",
    "    \"z\": \"Rochas metassedimentares\"\n",
    "}\n",
    "\n",
    "# Save each mapping as a DataFrame\n",
    "pd.DataFrame([\n",
    "    {\"geology_code\": k, \"geology_description\": v}\n",
    "    for k, v in geology_mapping.items()\n",
    "]).to_csv(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_clean/geology_mapping.csv\", index=False)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"lithology_code\": k, \"lithology_description\": v}\n",
    "    for k, v in lithology_mapping.items()\n",
    "]).to_csv(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_clean/lithology_mapping.csv\", index=False)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"lithology_1954_code\": k, \"lithology_1954_description\": v}\n",
    "    for k, v in lithology_1954_mapping.items()\n",
    "]).to_csv(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_clean/lithology1954_mapping.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# District table\n",
    "\n",
    "**don't need**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do I need to make a separate table??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minerology info table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biology Info table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking CSV Datatypes before DB export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ File: analyses.csv\n",
      "lab_sample_id      int64\n",
      "sample_id        float64\n",
      "EG               float64\n",
      "thick_clay       float64\n",
      "fine_clay        float64\n",
      "                  ...   \n",
      "Tl               float64\n",
      "Pb               float64\n",
      "Bi               float64\n",
      "Th               float64\n",
      "U                float64\n",
      "Length: 68, dtype: object\n",
      "\n",
      "üìÑ File: soil_type_clean.csv\n",
      "soil_type_id     int64\n",
      "profile         object\n",
      "CEP_GR          object\n",
      "CEP_NAME        object\n",
      "FAO             object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: analyses_clean.csv\n",
      "lab_sample_id      int64\n",
      "analysis_id       object\n",
      "horizon_id        object\n",
      "sample_id         object\n",
      "EG               float64\n",
      "                  ...   \n",
      "Tl               float64\n",
      "Pb               float64\n",
      "Bi               float64\n",
      "Th               float64\n",
      "U                float64\n",
      "Length: 70, dtype: object\n",
      "\n",
      "üìÑ File: profile_record.csv\n",
      "profile_record_id      int64\n",
      "profile               object\n",
      "site_info_id         float64\n",
      "soil_type_id         float64\n",
      "sample_id              int64\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: soil_type.csv\n",
      "soil_type_id     int64\n",
      "profile         object\n",
      "CEP_GR          object\n",
      "CEP_NAME        object\n",
      "FAO             object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: morpho_cleaned.csv\n",
      "Unnamed: 0                              int64\n",
      "horizon_id                             object\n",
      "sample_id                             float64\n",
      "profile                                object\n",
      "horizon_layer                         float64\n",
      "upper_depth                           float64\n",
      "lower_depth                           float64\n",
      "dry_color_name                         object\n",
      "dry_hue                                object\n",
      "dry_value                             float64\n",
      "dry_chroma                            float64\n",
      "moist_color_name                       object\n",
      "moist_hue                              object\n",
      "moist_value                           float64\n",
      "moist_chroma                          float64\n",
      "Manchas                                object\n",
      "texture                                object\n",
      "Abund√¢ncia de elementos grosseiros     object\n",
      "Forma de elementos grosseiros          object\n",
      "Natureza de elementos grosseiros       object\n",
      "structure_type                         object\n",
      "structure_class                        object\n",
      "structure_degree                       object\n",
      "compaction                             object\n",
      "durability                             object\n",
      "pore_quantity                          object\n",
      "pore_diameter                          object\n",
      "pore_shape                            float64\n",
      "root_quantity                          object\n",
      "root_diameter                          object\n",
      "moisture_degree                        object\n",
      "Unnamed: 46                           float64\n",
      "profile_record_id                     float64\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: geology_mapping.csv\n",
      "geology_code           object\n",
      "geology_description    object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: topo_feat_clean.csv\n",
      "topo_features_id       int64\n",
      "slope_code            object\n",
      "altitude             float64\n",
      "aspect               float64\n",
      "land_surface_temp    float64\n",
      "dem_elevation        float64\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: lithology1954_mapping.csv\n",
      "lithology_1954_code           object\n",
      "lithology_1954_description    object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: samples.csv\n",
      "sample_id         int64\n",
      "site_info_id     object\n",
      "profile          object\n",
      "horizon_id       object\n",
      "year            float64\n",
      "shelf            object\n",
      "room             object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: morphology_horizon.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/79mdnyy56_xc3g1jvp9wf4_80000gn/T/ipykernel_2227/3654665737.py:16: DtypeWarning: Columns (13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horizon_id            object\n",
      "sample_id            float64\n",
      "profile_record_id    float64\n",
      "horizon_layer        float64\n",
      "upper_depth          float64\n",
      "lower_depth          float64\n",
      "moisture_degree       object\n",
      "root_quantity         object\n",
      "root_diameter         object\n",
      "texture               object\n",
      "structure_type        object\n",
      "structure_class       object\n",
      "structure_degree      object\n",
      "pore_diameter         object\n",
      "pore_quantity         object\n",
      "pore_shape            object\n",
      "dry_color_name        object\n",
      "dry_hue               object\n",
      "dry_value            float64\n",
      "dry_chroma           float64\n",
      "moist_color_name      object\n",
      "moist_hue             object\n",
      "moist_value          float64\n",
      "moist_chroma         float64\n",
      "compaction            object\n",
      "durability            object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: site_info.csv\n",
      "site_info_id          int64\n",
      "profile              object\n",
      "X_coord             float64\n",
      "Y_coord             float64\n",
      "district             object\n",
      "geo_features_id       int64\n",
      "climate_id            int64\n",
      "topo_features_id      int64\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: profile_record_clean.csv\n",
      "profile_record_id      int64\n",
      "profile               object\n",
      "site_info_id         float64\n",
      "soil_type_id         float64\n",
      "sample_id              int64\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: morphology_horizon_clean.csv\n",
      "horizon_id            object\n",
      "sample_id            float64\n",
      "profile_record_id    float64\n",
      "horizon_layer        float64\n",
      "upper_depth          float64\n",
      "lower_depth          float64\n",
      "moisture_degree       object\n",
      "root_quantity         object\n",
      "root_diameter         object\n",
      "texture               object\n",
      "structure_type        object\n",
      "structure_class       object\n",
      "structure_degree      object\n",
      "pore_diameter         object\n",
      "pore_quantity         object\n",
      "pore_shape            object\n",
      "dry_color_name        object\n",
      "dry_hue               object\n",
      "dry_value            float64\n",
      "dry_chroma           float64\n",
      "moist_color_name      object\n",
      "moist_hue             object\n",
      "moist_value          float64\n",
      "moist_chroma         float64\n",
      "compaction            object\n",
      "durability            object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: lithology_mapping.csv\n",
      "lithology_code           object\n",
      "lithology_description    object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: topo_feat.csv\n",
      "topo_features_id       int64\n",
      "slope_code            object\n",
      "altitude             float64\n",
      "aspect               float64\n",
      "land_surface_temp    float64\n",
      "dem_elevation        float64\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: climate_feat_clean.csv\n",
      "climate_id                int64\n",
      "mean_annual_temp        float64\n",
      "mean_annual_precip      float64\n",
      "koppen_climate           object\n",
      "thornthwaite_climate     object\n",
      "hydric_regime            object\n",
      "thermal_regime           object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: geo_feat.csv\n",
      "geo_features_id       int64\n",
      "geology_id           object\n",
      "lithology_id         object\n",
      "lithology_1954_id    object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: site_info_clean.csv\n",
      "site_info_id          int64\n",
      "profile              object\n",
      "X_coord             float64\n",
      "Y_coord             float64\n",
      "district             object\n",
      "geo_features_id       int64\n",
      "climate_id            int64\n",
      "topo_features_id      int64\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: samples_clean.csv\n",
      "sample_id         int64\n",
      "site_info_id     object\n",
      "profile          object\n",
      "horizon_id       object\n",
      "year            float64\n",
      "shelf            object\n",
      "room             object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: districts_clean.csv\n",
      "district_id     int64\n",
      "district       object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: geo_feat_clean.csv\n",
      "geo_features_id       int64\n",
      "geology_id           object\n",
      "lithology_id         object\n",
      "lithology_1954_id    object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: climate_feat.csv\n",
      "climate_id                int64\n",
      "mean_annual_temp        float64\n",
      "mean_annual_precip      float64\n",
      "koppen_climate           object\n",
      "thornthwaite_climate     object\n",
      "hydric_regime            object\n",
      "thermal_regime           object\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/79mdnyy56_xc3g1jvp9wf4_80000gn/T/ipykernel_2227/3654665737.py:16: DtypeWarning: Columns (13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set the path to your folder containing the 10 CSV files\n",
    "csv_folder = \"/Users/inesschwartz/GreenDataScience/Thesis/tables_clean\"  \n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(csv_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# Loop through each file and display column data types\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(csv_folder, file)\n",
    "    print(f\"\\nüìÑ File: {file}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(df.dtypes)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foreign key imports and datatype consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## standardize datatypes for identifiers\n",
    "def convert_identifiers_to_string(df, id_columns):\n",
    "    \"\"\"\n",
    "    Converts specified identifier columns in a DataFrame to string type,\n",
    "    safely handling float64 values and preserving missing values (NA).\n",
    "    Avoids SettingWithCopyWarning.\n",
    "    \"\"\"\n",
    "    df = df.copy()  # ensure we're working with a copy, not a slice\n",
    "    for col in id_columns:\n",
    "        if col in df.columns:\n",
    "            df.loc[:, col] = df[col].apply(\n",
    "                lambda x: str(int(x)) if pd.notna(x) and isinstance(x, float) and x.is_integer()\n",
    "                else str(x) if pd.notna(x)\n",
    "                else pd.NA\n",
    "            ).astype(\"string\")\n",
    "    return df\n",
    "\n",
    "#usage\n",
    "\n",
    "# Define identifier columns\n",
    "identifier_columns = [\n",
    "    'sample_id', 'site_info_id', 'profile',\n",
    "    'horizon_id', 'lab_sample_id', 'lab_sample_id', 'climate_id', 'geo_features_id', 'topo_features_id', 'profile_record_id'\n",
    "]\n",
    "\n",
    "# Apply to each relevant dataframe\n",
    "samples_check = convert_identifiers_to_string(samples_check, identifier_columns)\n",
    "df_morpho_cleaned = convert_identifiers_to_string(df_morpho_cleaned, identifier_columns)\n",
    "profile_clean = convert_identifiers_to_string(profile_record_clean, identifier_columns)\n",
    "merged1 = convert_identifiers_to_string(merged1, identifier_columns)\n",
    "site_info_clean = convert_identifiers_to_string(site_info_clean, identifier_columns)\n",
    "#district_clean = convert_identifiers_to_string(district_clean, identifier_columns)\n",
    "soil_type_clean = convert_identifiers_to_string(soil_type_clean, identifier_columns)\n",
    "geo_features_clean = convert_identifiers_to_string(geo_features_clean, identifier_columns)\n",
    "topo_features_clean = convert_identifiers_to_string(topo_features_clean, identifier_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "samples column types:\n",
      "  sample_id: object\n",
      "  site_info_id: object\n",
      "  profile: object\n",
      "  horizon_id: object\n",
      "  profile_record_id: object\n",
      "\n",
      "horizon column types:\n",
      "  sample_id: object\n",
      "  profile: object\n",
      "  horizon_id: object\n",
      "  profile_record_id: object\n",
      "\n",
      "profile column types:\n",
      "  sample_id: object\n",
      "  site_info_id: object\n",
      "  profile: object\n",
      "  profile_record_id: string\n",
      "\n",
      "lab_analysis column types:\n",
      "  sample_id: object\n",
      "  horizon_id: object\n",
      "  lab_sample_id: string\n",
      "  lab_sample_id: string\n",
      "\n",
      "site_info column types:\n",
      "  site_info_id: string\n",
      "  profile: object\n",
      "  climate_id: object\n",
      "\n",
      "soil_type column types:\n",
      "  profile: object\n",
      "\n",
      "geo_features column types:\n",
      "  profile: object\n",
      "  geo_features_id: string\n",
      "\n",
      "topo_features column types:\n",
      "  profile: object\n",
      "  topo_features_id: string\n"
     ]
    }
   ],
   "source": [
    "# Check types of identifier columns in each dataframe\n",
    "dfs = {\n",
    "    \"samples\": samples_check,\n",
    "    \"horizon\": df_morpho_cleaned,\n",
    "    \"profile\": profile_clean,\n",
    "    \"lab_analysis\": merged1,\n",
    "    \"site_info\": site_info_clean,\n",
    "    #\"district_clean\": district_clean,\n",
    "    \"soil_type\": soil_type_clean,\n",
    "    \"geo_features\": geo_features_clean,\n",
    "    \"topo_features\": topo_features_clean,\n",
    "}\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{name} column types:\")\n",
    "    for col in identifier_columns:\n",
    "        if col in df.columns:\n",
    "            print(f\"  {col}: {df[col].dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9704 sample_id(s) in samples_check are missing in horizon.\n",
      "      sample_id\n",
      "1           631\n",
      "3           633\n",
      "7           697\n",
      "8           698\n",
      "9           699\n",
      "...         ...\n",
      "14710     18867\n",
      "14711     18868\n",
      "14712     18869\n",
      "14713     18870\n",
      "14714     18871\n",
      "\n",
      "[9704 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "## validate FK references in tables\n",
    "\n",
    "samples_check['sample_id'] = samples_check['sample_id'].astype(str).str.strip().str.replace('.0', '')\n",
    "df_morpho_cleaned['sample_id'] = df_morpho_cleaned['sample_id'].astype(str).str.strip().str.replace('.0', '')\n",
    "\n",
    "# Check for missing FK references in morphology\n",
    "missing_samples = samples_check[~samples_check['sample_id'].isin(df_morpho_cleaned['sample_id'])]\n",
    "\n",
    "print(f\"{len(missing_samples)} sample_id(s) in samples_check are missing in horizon.\")\n",
    "print(missing_samples[['sample_id']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sample_id\n",
      "0       2044\n",
      "1      15936\n",
      "2      17780\n",
      "3       5288\n",
      "4      13422\n",
      "5      10276\n",
      "6      12228\n",
      "7       9094\n",
      "8      15374\n",
      "9       7359\n",
      "10      1762\n",
      "11     17363\n",
      "12     12169\n",
      "13     18754\n",
      "14      2236\n",
      "15      9709\n",
      "16     16539\n",
      "17     17941\n",
      "18     14442\n",
      "19      7699\n",
      "20     11951\n",
      "21     15690\n",
      "22     11558\n",
      "23      1823\n",
      "24      6124\n"
     ]
    }
   ],
   "source": [
    "#inspecting a random 25 samples that are missing in horizon\n",
    "\n",
    "# Print random sample of 25 missing sample_ids\n",
    "sample_subset = missing_samples.sample(n=25, random_state=42)  # Set random_state for reproducibility\n",
    "print(sample_subset[['sample_id']].reset_index(drop=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9704 samples could not be matched with a horizon_id.\n"
     ]
    }
   ],
   "source": [
    "# Samples table horizon_id FK \n",
    "\n",
    "# Drop old horizon_id if it exists (to avoid confusion)\n",
    "samples_check = samples_check.drop(columns=['horizon_id'], errors='ignore')\n",
    "\n",
    "samples_check['sample_id'] = samples_check['sample_id'].astype(str).str.strip().str.replace('.0', '', regex=False)\n",
    "df_morpho_cleaned['sample_id'] = df_morpho_cleaned['sample_id'].astype(str).str.strip().str.replace('.0', '', regex=False)\n",
    "\n",
    "\n",
    "# Merge horizon_id from morph into samples\n",
    "samples_check = samples_check.merge(\n",
    "    df_morpho_cleaned[['sample_id', 'horizon_id']],\n",
    "    on='sample_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "missing = samples_check[samples_check['horizon_id'].isna()]\n",
    "print(f\"{len(missing)} samples could not be matched with a horizon_id.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>year</th>\n",
       "      <th>shelf</th>\n",
       "      <th>room</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>630</td>\n",
       "      <td>172</td>\n",
       "      <td>139</td>\n",
       "      <td>Hb_139/46_1_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>631</td>\n",
       "      <td>173</td>\n",
       "      <td>139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>174</td>\n",
       "      <td>139</td>\n",
       "      <td>Hb_139/46_3_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>633</td>\n",
       "      <td>175</td>\n",
       "      <td>139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>687</td>\n",
       "      <td>1034</td>\n",
       "      <td>208</td>\n",
       "      <td>Hb_208/46_1_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id site_info_id profile     horizon_id    year shelf room\n",
       "0       630          172     139  Hb_139/46_1_1  1946.0     1   22\n",
       "1       631          173     139            NaN  1946.0     1   22\n",
       "2       632          174     139  Hb_139/46_3_1  1946.0     1   22\n",
       "3       633          175     139            NaN  1946.0     1   22\n",
       "4       687         1034     208  Hb_208/46_1_1  1946.0     1   22"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-order and select relevant columns\n",
    "samples_clean1 = samples_check[[\n",
    "    'sample_id',\n",
    "    'site_info_id', \n",
    "    'profile',\n",
    "    'horizon_id',\n",
    "    'year',\n",
    "    'shelf',\n",
    "    'room'  # Ensure this matches the column name in your DataFrame\n",
    "]].copy()\n",
    "\n",
    "# Preview the result\n",
    "samples_clean1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>year</th>\n",
       "      <th>shelf</th>\n",
       "      <th>room</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>630</td>\n",
       "      <td>172</td>\n",
       "      <td>139</td>\n",
       "      <td>Hb_139/46_1_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>631</td>\n",
       "      <td>173</td>\n",
       "      <td>139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>174</td>\n",
       "      <td>139</td>\n",
       "      <td>Hb_139/46_3_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>633</td>\n",
       "      <td>175</td>\n",
       "      <td>139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>687</td>\n",
       "      <td>1034</td>\n",
       "      <td>208</td>\n",
       "      <td>Hb_208/46_1_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id site_info_id profile     horizon_id    year shelf room\n",
       "0       630          172     139  Hb_139/46_1_1  1946.0     1   22\n",
       "1       631          173     139            NaN  1946.0     1   22\n",
       "2       632          174     139  Hb_139/46_3_1  1946.0     1   22\n",
       "3       633          175     139            NaN  1946.0     1   22\n",
       "4       687         1034     208  Hb_208/46_1_1  1946.0     1   22"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_clean1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## completing profile_record_clean table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_record_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>soil_type_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>208</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   profile_record_id profile site_info_id sample_id soil_type_id\n",
       "0                  1     139         <NA>      <NA>         <NA>\n",
       "1                  2     139         <NA>      <NA>         <NA>\n",
       "2                  3     139         <NA>      <NA>         <NA>\n",
       "3                  4     139         <NA>      <NA>         <NA>\n",
       "4                  5     208         <NA>      <NA>         <NA>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_record_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ID'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tp/79mdnyy56_xc3g1jvp9wf4_80000gn/T/ipykernel_2227/388914968.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'site_info_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sample_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'soil_type_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Merge site_info_id (one-to-one or many-to-one) using 'ID'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m profile_record_clean = profile_record_clean.merge(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msite_info_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'profile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'site_info_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   9839\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9840\u001b[0m     ) -> DataFrame:\n\u001b[1;32m   9841\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9843\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m   9844\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9845\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9846\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mindicator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m ) -> DataFrame:\n\u001b[0;32m--> 148\u001b[0;31m     op = _MergeOperation(\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    733\u001b[0m         (\n\u001b[1;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;31m# to avoid incompatible dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1774\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ID'"
     ]
    }
   ],
   "source": [
    "# Drop old foreign key columns if they exist\n",
    "profile_record_clean = profile_record_clean.drop(\n",
    "    columns=['site_info_id', 'sample_id', 'soil_type_id'], errors='ignore'\n",
    ")\n",
    "\n",
    "# Merge site_info_id (one-to-one or many-to-one) using 'ID'\n",
    "profile_record_clean = profile_record_clean.merge(\n",
    "    site_info_clean[['profile', 'site_info_id']],\n",
    "    on='ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge soil_type_id (one-to-one or many-to-one) using 'profile'\n",
    "profile_record_clean = profile_record_clean.merge(\n",
    "    soil_type_clean[['profile', 'soil_type_id']],\n",
    "    on='profile',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge sample_id (one-to-one or many-to-one) using 'profile'\n",
    "profile_record_clean = profile_record_clean.merge(\n",
    "    samples_clean1[['profile', 'sample_id']],\n",
    "    on='profile',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "#drop PK and replace w new profile_record_id PK\n",
    "# Drop old FK columns if they exist\n",
    "profile_record_clean = profile_record_clean.drop(\n",
    "    columns=['profile_record_id' ],\n",
    "    errors='ignore'\n",
    ")\n",
    "\n",
    "# Ensure primary key 'profile_record_id' exists\n",
    "if 'profile_record_id' not in profile_record_clean.columns:\n",
    "    profile_record_clean.insert(0, 'profile_record_id', range(1, len(profile_record_clean) + 1))\n",
    "\n",
    "profile_record_clean['profile_record_id'] = profile_record_clean['profile_record_id'].astype(str)\n",
    "\n",
    "# Final column order\n",
    "column_order = [\n",
    "    'profile_record_id',\n",
    "    'profile',\n",
    "    'site_info_id',\n",
    "    'soil_type_id',\n",
    "    'sample_id'\n",
    "]\n",
    "profile_record_clean = profile_record_clean[column_order]\n",
    "\n",
    "# Check for missing links (optional sanity check)\n",
    "missing_site = profile_record_clean[profile_record_clean['site_info_id'].isna()]\n",
    "missing_soil = profile_record_clean[profile_record_clean['soil_type_id'].isna()]\n",
    "\n",
    "print(f\"{len(missing_site)} profiles missing site_info_id\")\n",
    "print(f\"{len(missing_soil)} profiles missing soil_type_id\")\n",
    "\n",
    "profile_record_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>horizon_layer</th>\n",
       "      <th>upper_depth</th>\n",
       "      <th>lower_depth</th>\n",
       "      <th>dry_color_name</th>\n",
       "      <th>dry_hue</th>\n",
       "      <th>dry_value</th>\n",
       "      <th>dry_chroma</th>\n",
       "      <th>...</th>\n",
       "      <th>compaction</th>\n",
       "      <th>durability</th>\n",
       "      <th>pore_quantity</th>\n",
       "      <th>pore_diameter</th>\n",
       "      <th>pore_shape</th>\n",
       "      <th>root_quantity</th>\n",
       "      <th>root_diameter</th>\n",
       "      <th>moisture_degree</th>\n",
       "      <th>Unnamed: 46</th>\n",
       "      <th>profile_record_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>10999</td>\n",
       "      <td>101_62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Pardo-acinzentado a pardo</td>\n",
       "      <td>10YR</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>Pequena a minima</td>\n",
       "      <td>Brando</td>\n",
       "      <td>Pouco poroso</td>\n",
       "      <td>Muito finos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Muitas finas e bastantes medias</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>11000</td>\n",
       "      <td>101_62</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Pardo</td>\n",
       "      <td>10YR</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Brando</td>\n",
       "      <td>Pouco poroso</td>\n",
       "      <td>Muito finos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bastantes finas e medias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>11001</td>\n",
       "      <td>101_62</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>Pardo-amarelado-claro</td>\n",
       "      <td>10YR</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Pequena a minima</td>\n",
       "      <td>Brando</td>\n",
       "      <td>Pouco poroso</td>\n",
       "      <td>Muito finos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algumas finas e medias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>11002</td>\n",
       "      <td>101_62</td>\n",
       "      <td>4.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Amarelo a amarelo-avermelhado</td>\n",
       "      <td>8,75YR</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Pequena a minima</td>\n",
       "      <td>Brando</td>\n",
       "      <td>Pouco a medianamente poroso</td>\n",
       "      <td>Muito finos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Poucas finas, algumas medias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B_101/62_5_2</td>\n",
       "      <td>11003</td>\n",
       "      <td>101_62</td>\n",
       "      <td>5.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>Pardo-avermelhado</td>\n",
       "      <td>7,5YR</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Brando</td>\n",
       "      <td>Pouco a medianamente poroso</td>\n",
       "      <td>Muito finos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Raras</td>\n",
       "      <td>Medias e grossas</td>\n",
       "      <td>Seco a humido</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     horizon_id sample_id profile  horizon_layer  upper_depth  lower_depth  \\\n",
       "0  B_101/62_1_1     10999  101_62            1.0          0.0         11.0   \n",
       "1  B_101/62_2_1     11000  101_62            2.0         11.0         28.0   \n",
       "2  B_101/62_3_1     11001  101_62            3.0         28.0         54.0   \n",
       "3  B_101/62_4_1     11002  101_62            4.0         54.0         90.0   \n",
       "4  B_101/62_5_2     11003  101_62            5.0         90.0        160.0   \n",
       "\n",
       "                  dry_color_name dry_hue  dry_value  dry_chroma  ...  \\\n",
       "0      Pardo-acinzentado a pardo    10YR        5.0         2.5  ...   \n",
       "1                          Pardo    10YR        5.0         3.0  ...   \n",
       "2          Pardo-amarelado-claro    10YR        6.0         4.0  ...   \n",
       "3  Amarelo a amarelo-avermelhado  8,75YR        7.0         6.0  ...   \n",
       "4              Pardo-avermelhado   7,5YR        7.0         6.0  ...   \n",
       "\n",
       "         compaction durability                pore_quantity  pore_diameter  \\\n",
       "0  Pequena a minima     Brando                 Pouco poroso    Muito finos   \n",
       "1           Pequena     Brando                 Pouco poroso    Muito finos   \n",
       "2  Pequena a minima     Brando                 Pouco poroso    Muito finos   \n",
       "3  Pequena a minima     Brando  Pouco a medianamente poroso    Muito finos   \n",
       "4           Pequena     Brando  Pouco a medianamente poroso    Muito finos   \n",
       "\n",
       "  pore_shape                                 root_quantity     root_diameter  \\\n",
       "0        NaN               Muitas finas e bastantes medias               NaN   \n",
       "1        NaN      Bastantes finas e medias e raras grossas               NaN   \n",
       "2        NaN        Algumas finas e medias e raras grossas               NaN   \n",
       "3        NaN  Poucas finas, algumas medias e raras grossas               NaN   \n",
       "4        NaN                                         Raras  Medias e grossas   \n",
       "\n",
       "  moisture_degree Unnamed: 46 profile_record_id  \n",
       "0            Seco         NaN              <NA>  \n",
       "1            Seco         NaN              <NA>  \n",
       "2            Seco         NaN              <NA>  \n",
       "3            Seco         NaN              <NA>  \n",
       "4   Seco a humido         NaN              <NA>  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_morpho_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to string dtype in both DataFrames before merge\n",
    "df_morpho_cleaned['profile_record_id'] = df_morpho_cleaned['profile_record_id'].astype(str)\n",
    "profile_record_clean['profile_record_id'] = profile_record_clean['profile_record_id'].astype(str)\n",
    "\n",
    "#drop old FK\n",
    "# Drop old FK columns if they exist\n",
    "df_morpho_cleaned = df_morpho_cleaned.drop(\n",
    "    columns=['profile_record_id'],\n",
    "    errors='ignore'\n",
    ")\n",
    "# Now merge on 'profile_record_id'\n",
    "df_morpho_cleaned = df_morpho_cleaned.merge(\n",
    "    profile_record_clean[['profile', 'profile_record_id']],\n",
    "    on='profile',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_morpho_cleaned1 = df_morpho_cleaned[[\n",
    "    'horizon_id',\n",
    "    'sample_id',\n",
    "    #'profile_record_id',\n",
    "    'profile',\n",
    "    'horizon_layer',\n",
    "    'upper_depth',\n",
    "    'lower_depth',\n",
    "    'moisture_degree',\n",
    "    'root_quantity',\n",
    "    'root_diameter',\n",
    "    'texture',\n",
    "    'structure_type',\n",
    "    'structure_class',\n",
    "    'structure_degree',\n",
    "    'pore_diameter',\n",
    "    'pore_quantity',\n",
    "    'pore_shape',\n",
    "    'dry_color_name',\n",
    "    'dry_hue',\n",
    "    'dry_value',\n",
    "    'dry_chroma',\n",
    "    'moist_color_name',\n",
    "    'moist_hue',\n",
    "    'moist_value',\n",
    "    'moist_chroma',\n",
    "    'compaction',\n",
    "    'durability'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>horizon_layer</th>\n",
       "      <th>upper_depth</th>\n",
       "      <th>lower_depth</th>\n",
       "      <th>moisture_degree</th>\n",
       "      <th>root_quantity</th>\n",
       "      <th>root_diameter</th>\n",
       "      <th>texture</th>\n",
       "      <th>...</th>\n",
       "      <th>dry_color_name</th>\n",
       "      <th>dry_hue</th>\n",
       "      <th>dry_value</th>\n",
       "      <th>dry_chroma</th>\n",
       "      <th>moist_color_name</th>\n",
       "      <th>moist_hue</th>\n",
       "      <th>moist_value</th>\n",
       "      <th>moist_chroma</th>\n",
       "      <th>compaction</th>\n",
       "      <th>durability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>10999</td>\n",
       "      <td>101_62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Seco</td>\n",
       "      <td>Muitas finas e bastantes medias</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arenoso</td>\n",
       "      <td>...</td>\n",
       "      <td>Pardo-acinzentado a pardo</td>\n",
       "      <td>10YR</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Pardo-acinzentado-escuro</td>\n",
       "      <td>10YR</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Pequena a minima</td>\n",
       "      <td>Brando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>11000</td>\n",
       "      <td>101_62</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Seco</td>\n",
       "      <td>Bastantes finas e medias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arenoso-franco</td>\n",
       "      <td>...</td>\n",
       "      <td>Pardo</td>\n",
       "      <td>10YR</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Pardo-amarelado-escuro</td>\n",
       "      <td>10YR</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Brando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>11001</td>\n",
       "      <td>101_62</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>Seco</td>\n",
       "      <td>Algumas finas e medias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arenoso-franco</td>\n",
       "      <td>...</td>\n",
       "      <td>Pardo-amarelado-claro</td>\n",
       "      <td>10YR</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Pardo-amarelado-escuro</td>\n",
       "      <td>10YR</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Pequena a minima</td>\n",
       "      <td>Brando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>11002</td>\n",
       "      <td>101_62</td>\n",
       "      <td>4.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Seco</td>\n",
       "      <td>Poucas finas, algumas medias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Franco-arenoso a arenoso-franco</td>\n",
       "      <td>...</td>\n",
       "      <td>Amarelo a amarelo-avermelhado</td>\n",
       "      <td>8,75YR</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Pardo-forte</td>\n",
       "      <td>7,5YR</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Pequena a minima</td>\n",
       "      <td>Brando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B_101/62_5_2</td>\n",
       "      <td>11003</td>\n",
       "      <td>101_62</td>\n",
       "      <td>5.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>Seco a humido</td>\n",
       "      <td>Raras</td>\n",
       "      <td>Medias e grossas</td>\n",
       "      <td>Arenoso-franco</td>\n",
       "      <td>...</td>\n",
       "      <td>Pardo-avermelhado</td>\n",
       "      <td>7,5YR</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Pardo-forte</td>\n",
       "      <td>7,5YR</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Brando</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     horizon_id sample_id profile  horizon_layer  upper_depth  lower_depth  \\\n",
       "0  B_101/62_1_1     10999  101_62            1.0          0.0         11.0   \n",
       "1  B_101/62_2_1     11000  101_62            2.0         11.0         28.0   \n",
       "2  B_101/62_3_1     11001  101_62            3.0         28.0         54.0   \n",
       "3  B_101/62_4_1     11002  101_62            4.0         54.0         90.0   \n",
       "4  B_101/62_5_2     11003  101_62            5.0         90.0        160.0   \n",
       "\n",
       "  moisture_degree                                 root_quantity  \\\n",
       "0            Seco               Muitas finas e bastantes medias   \n",
       "1            Seco      Bastantes finas e medias e raras grossas   \n",
       "2            Seco        Algumas finas e medias e raras grossas   \n",
       "3            Seco  Poucas finas, algumas medias e raras grossas   \n",
       "4   Seco a humido                                         Raras   \n",
       "\n",
       "      root_diameter                          texture  ...  \\\n",
       "0               NaN                          Arenoso  ...   \n",
       "1               NaN                   Arenoso-franco  ...   \n",
       "2               NaN                   Arenoso-franco  ...   \n",
       "3               NaN  Franco-arenoso a arenoso-franco  ...   \n",
       "4  Medias e grossas                   Arenoso-franco  ...   \n",
       "\n",
       "                  dry_color_name dry_hue dry_value dry_chroma  \\\n",
       "0      Pardo-acinzentado a pardo    10YR       5.0        2.5   \n",
       "1                          Pardo    10YR       5.0        3.0   \n",
       "2          Pardo-amarelado-claro    10YR       6.0        4.0   \n",
       "3  Amarelo a amarelo-avermelhado  8,75YR       7.0        6.0   \n",
       "4              Pardo-avermelhado   7,5YR       7.0        6.0   \n",
       "\n",
       "           moist_color_name moist_hue moist_value moist_chroma  \\\n",
       "0  Pardo-acinzentado-escuro      10YR         4.0          2.0   \n",
       "1    Pardo-amarelado-escuro      10YR         3.0          4.0   \n",
       "2    Pardo-amarelado-escuro      10YR         4.0          4.0   \n",
       "3               Pardo-forte     7,5YR         5.0          6.0   \n",
       "4               Pardo-forte     7,5YR         5.0          6.0   \n",
       "\n",
       "         compaction  durability  \n",
       "0  Pequena a minima      Brando  \n",
       "1           Pequena      Brando  \n",
       "2  Pequena a minima      Brando  \n",
       "3  Pequena a minima      Brando  \n",
       "4           Pequena      Brando  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_morpho_cleaned1.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_morpho_cleaned1.to_csv('/Users/inesschwartz/Desktop/morpho_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing site_info_clean table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/79mdnyy56_xc3g1jvp9wf4_80000gn/T/ipykernel_8750/4452788.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  climate_features_clean['ID'] = climate_features_clean['ID'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Convert keys to string before merge to avoid dtype mismatch errors\n",
    "site_info_clean['site_info_id'] = site_info_clean['ID'].astype(str)\n",
    "geo_features_clean['ID'] = geo_features_clean['ID'].astype(str)\n",
    "climate_features_clean['ID'] = climate_features_clean['ID'].astype(str)\n",
    "topo_features_clean['ID'] = topo_features_clean['ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop old FK columns if they exist\n",
    "site_info_clean = site_info_clean.drop(\n",
    "    columns=['geo_features_id', 'climate_id', 'topo_features_id', 'district_id', 'land_cover_id', 'geology_id','topo_feature_id','sampling_date', 'districts_id' ],\n",
    "    errors='ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dropped 'ID' from site_info_clean\n",
      "‚úÖ Dropped 'ID' from geo_features_clean\n",
      "‚úÖ Dropped 'ID' from climate_features_clean\n",
      "‚úÖ Dropped 'ID' from topo_features_clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/79mdnyy56_xc3g1jvp9wf4_80000gn/T/ipykernel_8750/2925909182.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  climate_features_clean['ID'] = climate_features_clean['ID'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Convert 'ID' columns to string ---\n",
    "site_info_clean['ID'] = site_info_clean['ID'].astype(str)\n",
    "geo_features_clean['ID'] = geo_features_clean['ID'].astype(str)\n",
    "climate_features_clean['ID'] = climate_features_clean['ID'].astype(str)\n",
    "topo_features_clean['ID'] = topo_features_clean['ID'].astype(str)\n",
    "\n",
    "# --- Step 2: Merge feature tables into site_info_clean ---\n",
    "site_info_clean = site_info_clean.merge(\n",
    "    geo_features_clean[['ID', 'geo_features_id']],\n",
    "    on='ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "site_info_clean = site_info_clean.merge(\n",
    "    climate_features_clean[['ID', 'climate_id']],\n",
    "    on='ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "site_info_clean = site_info_clean.merge(\n",
    "    topo_features_clean[['ID', 'topo_features_id']],\n",
    "    on='ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# --- Step 3: Drop 'ID' column from all involved tables ---\n",
    "for table in ['site_info_clean', 'geo_features_clean', 'climate_features_clean', 'topo_features_clean']:\n",
    "    df = globals().get(table)\n",
    "    if df is not None and 'ID' in df.columns:\n",
    "        df = df.drop(columns=['ID'])\n",
    "        globals()[table] = df\n",
    "        print(f\"‚úÖ Dropped 'ID' from {table}\")\n",
    "    else:\n",
    "        print(f\"‚è≠Ô∏è Skipping {table}: not found or no 'ID' column\")\n",
    "\n",
    "# --- District: Skip merging district_id, use district name directly ---\n",
    "\n",
    "# don't see need for district_id, just use district name\n",
    "# For district (assuming district columns are already strings)\n",
    "# site_info_clean = site_info_clean.merge(\n",
    "#     district_clean[['district', 'district_id']],\n",
    "#     on='district',\n",
    "#     how='left'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>X_coord</th>\n",
       "      <th>Y_coord</th>\n",
       "      <th>district</th>\n",
       "      <th>geo_features_id</th>\n",
       "      <th>climate_id</th>\n",
       "      <th>topo_features_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2770</td>\n",
       "      <td>1/57</td>\n",
       "      <td>12.161278</td>\n",
       "      <td>-15.222598</td>\n",
       "      <td>Namibe</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>1/59</td>\n",
       "      <td>12.575775</td>\n",
       "      <td>-4.866986</td>\n",
       "      <td>Cabinda</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1618</td>\n",
       "      <td>1/61</td>\n",
       "      <td>15.098840</td>\n",
       "      <td>-11.225411</td>\n",
       "      <td>Cuanza Sul</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>881</td>\n",
       "      <td>1/63</td>\n",
       "      <td>17.081955</td>\n",
       "      <td>-9.274587</td>\n",
       "      <td>Malanje</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1750</td>\n",
       "      <td>1/64</td>\n",
       "      <td>20.788116</td>\n",
       "      <td>-11.568683</td>\n",
       "      <td>Moxico</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  site_info_id profile    X_coord    Y_coord    district  geo_features_id  \\\n",
       "0         2770    1/57  12.161278 -15.222598      Namibe                1   \n",
       "1           48    1/59  12.575775  -4.866986     Cabinda                2   \n",
       "2         1618    1/61  15.098840 -11.225411  Cuanza Sul                3   \n",
       "3          881    1/63  17.081955  -9.274587     Malanje                4   \n",
       "4         1750    1/64  20.788116 -11.568683      Moxico                5   \n",
       "\n",
       "   climate_id  topo_features_id  \n",
       "0           1                 1  \n",
       "1           2                 2  \n",
       "2           3                 3  \n",
       "3           4                 4  \n",
       "4           5                 5  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_info_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to drop ID from climate, geo_features, topo_features, and site_info_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>X_coord</th>\n",
       "      <th>Y_coord</th>\n",
       "      <th>district</th>\n",
       "      <th>geo_features_id</th>\n",
       "      <th>climate_id</th>\n",
       "      <th>topo_features_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2770</td>\n",
       "      <td>1/57</td>\n",
       "      <td>12.161278</td>\n",
       "      <td>-15.222598</td>\n",
       "      <td>Namibe</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>1/59</td>\n",
       "      <td>12.575775</td>\n",
       "      <td>-4.866986</td>\n",
       "      <td>Cabinda</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1618</td>\n",
       "      <td>1/61</td>\n",
       "      <td>15.098840</td>\n",
       "      <td>-11.225411</td>\n",
       "      <td>Cuanza Sul</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>881</td>\n",
       "      <td>1/63</td>\n",
       "      <td>17.081955</td>\n",
       "      <td>-9.274587</td>\n",
       "      <td>Malanje</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1750</td>\n",
       "      <td>1/64</td>\n",
       "      <td>20.788116</td>\n",
       "      <td>-11.568683</td>\n",
       "      <td>Moxico</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  site_info_id profile    X_coord    Y_coord    district  geo_features_id  \\\n",
       "0         2770    1/57  12.161278 -15.222598      Namibe                1   \n",
       "1           48    1/59  12.575775  -4.866986     Cabinda                2   \n",
       "2         1618    1/61  15.098840 -11.225411  Cuanza Sul                3   \n",
       "3          881    1/63  17.081955  -9.274587     Malanje                4   \n",
       "4         1750    1/64  20.788116 -11.568683      Moxico                5   \n",
       "\n",
       "   climate_id  topo_features_id  \n",
       "0           1                 1  \n",
       "1           2                 2  \n",
       "2           3                 3  \n",
       "3           4                 4  \n",
       "4           5                 5  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_info_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_record_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>soil_type_id</th>\n",
       "      <th>sample_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  profile_record_id profile site_info_id  soil_type_id sample_id\n",
       "0                 1     139         <NA>           NaN       630\n",
       "1                 2     139         <NA>           NaN       631\n",
       "2                 3     139         <NA>           NaN       632\n",
       "3                 4     139         <NA>           NaN       633\n",
       "4                 5     139         <NA>           NaN      1817"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_record_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## changing column headers to be SQL friendly\n",
    "# Rename problematic columns\n",
    "merged1 = merged1.rename(columns={\n",
    "    'atm_1/3': 'atm_1_3',\n",
    "    'Ca++': 'Ca',\n",
    "    'Mg++': 'Mg',\n",
    "    'Na+': 'Na',\n",
    "    'K+': 'K',\n",
    "    'Min_<0,002': 'Min_lt_0002',\n",
    "    'Min_0,05-0,02': 'Min_005_002',\n",
    "    'Min_0,2-0,05': 'Min_02_005',\n",
    "    'Min_2-0,2': 'Min_2_02',\n",
    "    'As': 'arsenic',\n",
    "    'P': 'phosphorus',\n",
    "    'S': 'sulfur',\n",
    "    'V': 'vanadium'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVING CLEANED TABLES TO CSV FOR DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepare dictionary of clean tables\n",
    "tables = {\n",
    "    \"samples\": samples_clean1,\n",
    "    \"analyses\": merged1,\n",
    "    \"profile_record\": profile_record_clean,\n",
    "    \"morphology_horizon\": morphology_clean1,\n",
    "    \"soil_type\": soil_type_clean,\n",
    "    \"site_info\": site_info_clean,\n",
    "    \"climate_feat\": climate_features_clean,\n",
    "    \"topo_feat\": topo_features_clean,\n",
    "    \"geo_feat\": geo_features_clean,\n",
    "    #\"districts\": district_clean\n",
    "}\n",
    "\n",
    "# # Save each table to CSV with 'NULL' in empty cells\n",
    "for name, df in tables.items():\n",
    "#     df.replace(\"\", pd.NA, inplace=True)  # Replace empty strings with NA\n",
    "    df.to_csv(f\"/Users/inesschwartz/GreenDataScience/Thesis/tables_clean/{name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save random 100 rows of each table as mini versions for DB test\n",
    "import os\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "mini_path = \"/Users/inesschwartz/GreenDataScience/Thesis/tables_clean_mini\"\n",
    "os.makedirs(mini_path, exist_ok=True)\n",
    "\n",
    "# Save random 100 rows of each table as mini versions\n",
    "samples_clean.sample(n=100, random_state=42).to_csv(f\"{mini_path}/samples.csv\", index=False)\n",
    "merged1.sample(n=100, random_state=42).to_csv(f\"{mini_path}/analyses.csv\", index=False)\n",
    "profile_record_clean.sample(n=100, random_state=42).to_csv(f\"{mini_path}/profile_record.csv\", index=False)\n",
    "morphology_clean1.sample(n=100, random_state=42).to_csv(f\"{mini_path}/morphology_horizon.csv\", index=False)\n",
    "soil_type_clean.sample(n=100, random_state=42).to_csv(f\"{mini_path}/soil_type.csv\", index=False)\n",
    "site_info_clean.sample(n=100, random_state=42).to_csv(f\"{mini_path}/site_info.csv\", index=False)\n",
    "climate_features_clean.sample(n=100, random_state=42).to_csv(f\"{mini_path}/climate_feat.csv\", index=False)\n",
    "topo_features_clean.sample(n=100, random_state=42).to_csv(f\"{mini_path}/topo_feat.csv\", index=False)\n",
    "geo_features_clean.sample(n=100, random_state=42).to_csv(f\"{mini_path}/geo_feat.csv\", index=False)\n",
    "#district_clean.sample(n=100, random_state=42).to_csv(f\"{mini_path}/districts.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "samples (14766 rows):\n",
      "  - sample_id (object)\n",
      "  - site_info_id (object)\n",
      "  - profile (object)\n",
      "  - horizon_id (object)\n",
      "  - year (float64)\n",
      "  - shelf (string)\n",
      "  - room (object)\n",
      "\n",
      "analyses (7847 rows):\n",
      "  - lab_sample_id (object)\n",
      "  - sample_id (object)\n",
      "  - EG (float64)\n",
      "  - thick_clay (float64)\n",
      "  - fine_clay (float64)\n",
      "  - silt (float64)\n",
      "  - clay (float64)\n",
      "  - Eq_Hum (float64)\n",
      "  - atm_1_3 (float64)\n",
      "  - atm_15 (float64)\n",
      "  - CACO3 (float64)\n",
      "  - gypsum (float64)\n",
      "  - free_iron (float64)\n",
      "  - organic_carbon (float64)\n",
      "  - total_N (float64)\n",
      "  - P205 (float64)\n",
      "  - organic_material (float64)\n",
      "  - pH_H2O (float64)\n",
      "  - pH_KCL (float64)\n",
      "  - Ca (float64)\n",
      "  - Mg (float64)\n",
      "  - Na (float64)\n",
      "  - K (float64)\n",
      "  - exchangable_bases_sum (float64)\n",
      "  - CEC (float64)\n",
      "  - vanadium (object)\n",
      "  - conductivity (float64)\n",
      "  - soluble_sodium (float64)\n",
      "  - Min_lt_0002 (object)\n",
      "  - Min_005_002 (object)\n",
      "  - Min_02_005 (object)\n",
      "  - Min_2_02 (object)\n",
      "  - field_sample_code (object)\n",
      "  - Depth (object)\n",
      "  - Al (float64)\n",
      "  - Si (float64)\n",
      "  - phosphorus (float64)\n",
      "  - sulfur (object)\n",
      "  - Cl (float64)\n",
      "  - Ti (float64)\n",
      "  - Cr (float64)\n",
      "  - Mn (float64)\n",
      "  - Fe (float64)\n",
      "  - Co (float64)\n",
      "  - Ni (float64)\n",
      "  - Cu (float64)\n",
      "  - Zn (float64)\n",
      "  - arsenic (float64)\n",
      "  - Se (float64)\n",
      "  - Rb (float64)\n",
      "  - Sr (float64)\n",
      "  - Zr (float64)\n",
      "  - Nb (float64)\n",
      "  - Mo (float64)\n",
      "  - Cd (float64)\n",
      "  - Sn (float64)\n",
      "  - Sb (float64)\n",
      "  - Ba (float64)\n",
      "  - Ta (float64)\n",
      "  - W (float64)\n",
      "  - Pt (float64)\n",
      "  - Au (float64)\n",
      "  - Hg (float64)\n",
      "  - Tl (float64)\n",
      "  - Pb (float64)\n",
      "  - Bi (float64)\n",
      "  - Th (float64)\n",
      "  - U (float64)\n",
      "\n",
      "profile_record (86665 rows):\n",
      "  - profile_record_id (object)\n",
      "  - profile (object)\n",
      "  - site_info_id (string)\n",
      "  - soil_type_id (float64)\n",
      "  - sample_id (object)\n",
      "\n",
      "morphology_horizon (207408 rows):\n",
      "  - horizon_id (object)\n",
      "  - sample_id (string)\n",
      "  - profile_record_id (object)\n",
      "  - horizon_layer (float64)\n",
      "  - upper_depth (float64)\n",
      "  - lower_depth (float64)\n",
      "  - moisture_degree (object)\n",
      "  - root_quantity (object)\n",
      "  - root_diameter (object)\n",
      "  - texture (object)\n",
      "  - structure_type (object)\n",
      "  - structure_class (object)\n",
      "  - structure_degree (object)\n",
      "  - pore_diameter (object)\n",
      "  - pore_quantity (object)\n",
      "  - pore_shape (object)\n",
      "  - dry_color_name (object)\n",
      "  - dry_hue (object)\n",
      "  - dry_value (float64)\n",
      "  - dry_chroma (float64)\n",
      "  - moist_color_name (object)\n",
      "  - moist_hue (object)\n",
      "  - moist_value (float64)\n",
      "  - moist_chroma (float64)\n",
      "  - compaction (object)\n",
      "  - durability (object)\n",
      "\n",
      "soil_type (2518 rows):\n",
      "  - soil_type_id (int64)\n",
      "  - profile (object)\n",
      "  - CEP_GR (object)\n",
      "  - CEP_NAME (object)\n",
      "  - FAO (object)\n",
      "\n",
      "site_info (4321 rows):\n",
      "  - site_info_id (object)\n",
      "  - profile (object)\n",
      "  - X_coord (float64)\n",
      "  - Y_coord (float64)\n",
      "  - district (object)\n",
      "  - geo_features_id (int64)\n",
      "  - climate_id (int64)\n",
      "  - topo_features_id (int64)\n",
      "\n",
      "climate_feat (4321 rows):\n",
      "  - climate_id (int64)\n",
      "  - mean_annual_temp (object)\n",
      "  - mean_annual_precip (object)\n",
      "  - koppen_climate (object)\n",
      "  - thornthwaite_climate (object)\n",
      "  - hydric_regime (object)\n",
      "  - thermal_regime (object)\n",
      "\n",
      "topo_feat (4321 rows):\n",
      "  - topo_features_id (int64)\n",
      "  - slope_code (object)\n",
      "  - altitude (float64)\n",
      "  - aspect (object)\n",
      "  - land_surface_temp (object)\n",
      "  - dem_elevation (object)\n",
      "\n",
      "geo_feat (4321 rows):\n",
      "  - geo_features_id (int64)\n",
      "  - geology_id (object)\n",
      "  - lithology_id (object)\n",
      "  - lithology_1954_id (object)\n"
     ]
    }
   ],
   "source": [
    "# List your DataFrame variables here\n",
    "dataframes = {\n",
    "    \"samples\": samples_clean1,\n",
    "    \"analyses\": merged1,\n",
    "    \"profile_record\": profile_record_clean,\n",
    "    \"morphology_horizon\": morphology_clean1,\n",
    "    \"soil_type\": soil_type_clean,\n",
    "    \"site_info\": site_info_clean,\n",
    "    \"climate_feat\": climate_features_clean,\n",
    "    \"topo_feat\": topo_features_clean,\n",
    "    \"geo_feat\": geo_features_clean\n",
    "    #\"districts\": district_clean\n",
    "}\n",
    "\n",
    "# Print column names and their datatypes\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n{name} ({len(df)} rows):\")\n",
    "    for col in df.columns:\n",
    "        dtype = df[col].dtype\n",
    "        print(f\"  - {col} ({dtype})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      profile_record_id profile site_info_id  soil_type_id sample_id\n",
      "0                     1     139         <NA>           NaN       630\n",
      "1                     2     139         <NA>           NaN       631\n",
      "2                     3     139         <NA>           NaN       632\n",
      "3                     4     139         <NA>           NaN       633\n",
      "4                     5     139         <NA>           NaN      1817\n",
      "...                 ...     ...          ...           ...       ...\n",
      "86660             86661  540/67         3627           NaN     18867\n",
      "86661             86662  540/67         3627           NaN     18868\n",
      "86662             86663  540/67         3627           NaN     18869\n",
      "86663             86664  540/67         3627           NaN     18870\n",
      "86664             86665  538/67         3611           NaN     18871\n",
      "\n",
      "[51532 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "missing = profile_record_clean[~profile_record_clean['soil_type_id'].isin(soil_type_clean['soil_type_id'])]\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>year</th>\n",
       "      <th>shelf</th>\n",
       "      <th>room</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>630</td>\n",
       "      <td>172</td>\n",
       "      <td>139</td>\n",
       "      <td>Hb_139/46_1_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>631</td>\n",
       "      <td>173</td>\n",
       "      <td>139</td>\n",
       "      <td>Hb_139/46_2_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>174</td>\n",
       "      <td>139</td>\n",
       "      <td>Hb_139/46_3_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>633</td>\n",
       "      <td>175</td>\n",
       "      <td>139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>687</td>\n",
       "      <td>1034</td>\n",
       "      <td>208</td>\n",
       "      <td>Hb_208/46_1_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id site_info_id profile     horizon_id    year shelf room\n",
       "0       630          172     139  Hb_139/46_1_1  1946.0     1   22\n",
       "1       631          173     139  Hb_139/46_2_1  1946.0     1   22\n",
       "2       632          174     139  Hb_139/46_3_1  1946.0     1   22\n",
       "3       633          175     139            NaN  1946.0     1   22\n",
       "4       687         1034     208  Hb_208/46_1_1  1946.0     1   22"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_clean1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     site_info_id profile    X_coord    Y_coord    district  geo_features_id  \\\n",
      "0            2770    1/57  12.161278 -15.222598      Namibe                1   \n",
      "2            1618    1/61  15.098840 -11.225411  Cuanza Sul                3   \n",
      "4            1750    1/64  20.788116 -11.568683      Moxico                5   \n",
      "5            3097    1/66  17.666766 -14.655526         NaN                6   \n",
      "8            2675   10/54  14.445188 -14.922688       Huila                9   \n",
      "...           ...     ...        ...        ...         ...              ...   \n",
      "4313         1689   99/63  18.164654 -11.382465     Malanje             4314   \n",
      "4314         3247   99/66  18.994294 -15.814591         NaN             4315   \n",
      "4317         1485  99c/63  17.541534 -10.890781     Malanje             4318   \n",
      "4319         1213   9c/63  16.357859  -9.986349     Malanje             4320   \n",
      "4320         1505   9c/65  20.937136 -10.942842   Lunda Sul             4321   \n",
      "\n",
      "      climate_id  topo_features_id  \n",
      "0              1                 1  \n",
      "2              3                 3  \n",
      "4              5                 5  \n",
      "5              6                 6  \n",
      "8              9                 9  \n",
      "...          ...               ...  \n",
      "4313        4314              4314  \n",
      "4314        4315              4315  \n",
      "4317        4318              4318  \n",
      "4319        4320              4320  \n",
      "4320        4321              4321  \n",
      "\n",
      "[3136 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "missing1 = site_info_clean[~site_info_clean['site_info_id'].isin(samples_clean['site_info_id'])]\n",
    "print(missing1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site_info_id\n",
       "16       16\n",
       "14       15\n",
       "9        15\n",
       "42       15\n",
       "40       15\n",
       "         ..\n",
       "32/57     1\n",
       "31/57     1\n",
       "30/57     1\n",
       "29/57     1\n",
       "657 c     1\n",
       "Name: count, Length: 6439, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_clean1['site_info_id'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>year</th>\n",
       "      <th>shelf</th>\n",
       "      <th>room</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10641</th>\n",
       "      <td>14219</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>318/63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1963.0</td>\n",
       "      <td>458</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12742</th>\n",
       "      <td>16639</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>582</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12743</th>\n",
       "      <td>16640</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>582</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744</th>\n",
       "      <td>16641</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>582</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12745</th>\n",
       "      <td>16642</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>582</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12746</th>\n",
       "      <td>16643</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>582</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12747</th>\n",
       "      <td>16644</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>582</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12748</th>\n",
       "      <td>16645</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>582</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12749</th>\n",
       "      <td>16646</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>582</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12750</th>\n",
       "      <td>16647</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>582</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12751</th>\n",
       "      <td>16648</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12752</th>\n",
       "      <td>16649</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12753</th>\n",
       "      <td>16650</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12754</th>\n",
       "      <td>16651</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12755</th>\n",
       "      <td>16652</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12756</th>\n",
       "      <td>16653</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12757</th>\n",
       "      <td>16654</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12758</th>\n",
       "      <td>16655</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12759</th>\n",
       "      <td>16656</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12760</th>\n",
       "      <td>16657</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12761</th>\n",
       "      <td>16658</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12762</th>\n",
       "      <td>16659</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12763</th>\n",
       "      <td>16660</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12764</th>\n",
       "      <td>16661</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12765</th>\n",
       "      <td>16662</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12766</th>\n",
       "      <td>16663</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12767</th>\n",
       "      <td>16664</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12768</th>\n",
       "      <td>16666</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_id site_info_id profile horizon_id    year shelf room\n",
       "10641     14219         <NA>  318/63        NaN  1963.0   458   26\n",
       "12742     16639         <NA>     nan        NaN  1965.0   582   26\n",
       "12743     16640         <NA>     nan        NaN  1965.0   582   26\n",
       "12744     16641         <NA>     nan        NaN  1965.0   582   26\n",
       "12745     16642         <NA>     nan        NaN  1965.0   582   26\n",
       "12746     16643         <NA>     nan        NaN  1965.0   582   26\n",
       "12747     16644         <NA>     nan        NaN  1965.0   582   26\n",
       "12748     16645         <NA>     nan        NaN  1965.0   582   26\n",
       "12749     16646         <NA>     nan        NaN  1965.0   582   26\n",
       "12750     16647         <NA>     nan        NaN  1965.0   582   26\n",
       "12751     16648         <NA>     nan        NaN  1965.0   583   26\n",
       "12752     16649         <NA>     nan        NaN  1965.0   583   26\n",
       "12753     16650         <NA>     nan        NaN  1965.0   583   26\n",
       "12754     16651         <NA>     nan        NaN  1965.0   583   26\n",
       "12755     16652         <NA>     nan        NaN  1965.0   583   26\n",
       "12756     16653         <NA>     nan        NaN  1965.0   583   26\n",
       "12757     16654         <NA>     nan        NaN  1965.0   583   26\n",
       "12758     16655         <NA>     nan        NaN  1965.0   583   26\n",
       "12759     16656         <NA>     nan        NaN  1965.0   583   26\n",
       "12760     16657         <NA>     nan        NaN  1965.0   583   26\n",
       "12761     16658         <NA>     nan        NaN  1965.0   583   26\n",
       "12762     16659         <NA>     nan        NaN  1965.0   583   26\n",
       "12763     16660         <NA>     nan        NaN  1965.0   583   26\n",
       "12764     16661         <NA>     nan        NaN  1965.0   583   26\n",
       "12765     16662         <NA>     nan        NaN  1965.0   583   26\n",
       "12766     16663         <NA>     nan        NaN  1965.0   583   26\n",
       "12767     16664         <NA>     nan        NaN  1965.0   583   26\n",
       "12768     16666         <NA>     nan        NaN  1965.0   583   26"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_clean1[samples_clean1['site_info_id'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_record_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>soil_type_id</th>\n",
       "      <th>sample_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  profile_record_id profile site_info_id  soil_type_id sample_id\n",
       "0                 1     139         <NA>           NaN       630\n",
       "1                 2     139         <NA>           NaN       631\n",
       "2                 3     139         <NA>           NaN       632\n",
       "3                 4     139         <NA>           NaN       633\n",
       "4                 5     139         <NA>           NaN      1817"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_record_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soil_type_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>CEP_GR</th>\n",
       "      <th>CEP_NAME</th>\n",
       "      <th>FAO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1/51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1/57</td>\n",
       "      <td>Aridicos</td>\n",
       "      <td>Aridicos com calcario Pardo-cinzentos</td>\n",
       "      <td>CLha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1/59</td>\n",
       "      <td>Psamoferralicos</td>\n",
       "      <td>Psamo-ferralicos Amarelos ou Alaranjados, sedi...</td>\n",
       "      <td>FRxa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1/63</td>\n",
       "      <td>Ferraliticos</td>\n",
       "      <td>Fracamente Ferralicos Vermelhos Clino-argilico...</td>\n",
       "      <td>FRh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10/54</td>\n",
       "      <td>Ferraliticos</td>\n",
       "      <td>Fracamente Ferralicos pardo-amarelados</td>\n",
       "      <td>FRh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   soil_type_id profile           CEP_GR  \\\n",
       "0             1    1/51              NaN   \n",
       "1             2    1/57         Aridicos   \n",
       "2             3    1/59  Psamoferralicos   \n",
       "3             4    1/63     Ferraliticos   \n",
       "4             5   10/54     Ferraliticos   \n",
       "\n",
       "                                            CEP_NAME   FAO  \n",
       "0                                                NaN   NaN  \n",
       "1              Aridicos com calcario Pardo-cinzentos  CLha  \n",
       "2  Psamo-ferralicos Amarelos ou Alaranjados, sedi...  FRxa  \n",
       "3  Fracamente Ferralicos Vermelhos Clino-argilico...   FRh  \n",
       "4             Fracamente Ferralicos pardo-amarelados   FRh  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soil_type_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå These rows reference soil_type_id values that do NOT exist in soil_type.csv:\n",
      "      profile_record_id profile site_info_id  soil_type_id sample_id\n",
      "0                     1     139         <NA>           NaN       630\n",
      "1                     2     139         <NA>           NaN       631\n",
      "2                     3     139         <NA>           NaN       632\n",
      "3                     4     139         <NA>           NaN       633\n",
      "4                     5     139         <NA>           NaN      1817\n",
      "...                 ...     ...          ...           ...       ...\n",
      "86660             86661  540/67         3627           NaN     18867\n",
      "86661             86662  540/67         3627           NaN     18868\n",
      "86662             86663  540/67         3627           NaN     18869\n",
      "86663             86664  540/67         3627           NaN     18870\n",
      "86664             86665  538/67         3611           NaN     18871\n",
      "\n",
      "[51532 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get list of valid soil_type_ids\n",
    "valid_soil_type_ids = set(soil_type_clean['soil_type_id'])\n",
    "\n",
    "# Find invalid soil_type_ids in profile_df\n",
    "invalid_rows = profile_record_clean[~profile_record_clean['soil_type_id'].isin(valid_soil_type_ids)]\n",
    "\n",
    "# Show the result\n",
    "if not invalid_rows.empty:\n",
    "    print(\"‚ùå These rows reference soil_type_id values that do NOT exist in soil_type.csv:\")\n",
    "    print(invalid_rows)\n",
    "else:\n",
    "    print(\"‚úÖ All soil_type_id values in profile_record.csv are valid.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_record_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>soil_type_id</th>\n",
       "      <th>sample_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  profile_record_id profile site_info_id  soil_type_id sample_id\n",
       "0                 1     139         <NA>           NaN       630\n",
       "1                 2     139         <NA>           NaN       631\n",
       "2                 3     139         <NA>           NaN       632\n",
       "3                 4     139         <NA>           NaN       633\n",
       "4                 5     139         <NA>           NaN      1817"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_record_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå These rows have NULL values in the site_info_id column of profile_record.csv:\n",
      "      profile_record_id profile site_info_id  soil_type_id sample_id\n",
      "0                     1     139         <NA>           NaN       630\n",
      "1                     2     139         <NA>           NaN       631\n",
      "2                     3     139         <NA>           NaN       632\n",
      "3                     4     139         <NA>           NaN       633\n",
      "4                     5     139         <NA>           NaN      1817\n",
      "...                 ...     ...          ...           ...       ...\n",
      "86240             86241  508/71         <NA>           NaN     18765\n",
      "86267             86268  516/68         <NA>           NaN     18772\n",
      "86268             86269  516/69         <NA>           NaN     18773\n",
      "86270             86271  493/68         <NA>           NaN     18781\n",
      "86271             86272  493/69         <NA>           NaN     18782\n",
      "\n",
      "[29884 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find rows where soil_type_id is null in profile_record_clean\n",
    "null_rows = profile_record_clean[profile_record_clean['site_info_id'].isnull()]\n",
    "\n",
    "# Show the result\n",
    "if not null_rows.empty:\n",
    "    print(\"‚ùå These rows have NULL values in the site_info_id column of profile_record.csv:\")\n",
    "    print(null_rows)\n",
    "else:\n",
    "    print(\"‚úÖ No NULL values found in the site_info_id column of profile_record.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ No NULL values found in the ID column of perfis_local.csv.\n"
     ]
    }
   ],
   "source": [
    "# Find rows where soil_type_id is null in profile_record_clean\n",
    "null_rows = profile_loc[profile_loc['ID'].isnull()]\n",
    "\n",
    "# Show the result\n",
    "if not null_rows.empty:\n",
    "    print(\"‚ùå These rows have NULL values in the ID column of perfis_local.csv:\")\n",
    "    print(null_rows)\n",
    "else:\n",
    "    print(\"‚úÖ No NULL values found in the ID column of perfis_local.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soil_type_ids in profile_record missing from soil_type.csv: {nan}\n"
     ]
    }
   ],
   "source": [
    "soil_type_ids = set(soil_type_clean['soil_type_id'].unique())\n",
    "profile_soil_type_ids = set(profile_record_clean['soil_type_id'].unique())\n",
    "\n",
    "missing_ids = profile_soil_type_ids - soil_type_ids\n",
    "\n",
    "print(\"soil_type_ids in profile_record missing from soil_type.csv:\", missing_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4835 in soil_type.csv? False\n",
      "4835 in profile_record.csv? False\n"
     ]
    }
   ],
   "source": [
    "# Check if 4835 is in soil_type_id column of soil_type.csv\n",
    "soil_type_has_4835 = 4835 in soil_type_clean['soil_type_id'].values\n",
    "\n",
    "# Check if 4835 is in soil_type_id column of profile_record.csv\n",
    "profile_record_has_4835 = 4835 in profile_record_clean['soil_type_id'].values\n",
    "\n",
    "print(f\"4835 in soil_type.csv? {soil_type_has_4835}\")\n",
    "print(f\"4835 in profile_record.csv? {profile_record_has_4835}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(4835 in profile_record_clean['soil_type_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Empty DataFrame\n",
      "Columns: [profile_record_id, profile, site_info_id, soil_type_id, sample_id]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(4835 in profile_record_clean['soil_type_id'].values)\n",
    "print(profile_record_clean[profile_record_clean['soil_type_id'] == 4835])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing soil_type_ids in soil_type.csv: {nan}\n"
     ]
    }
   ],
   "source": [
    "soil_type_ids_in_soil_type = set(soil_type_clean['soil_type_id'].unique())\n",
    "soil_type_ids_in_profile_record = set(profile_record_clean['soil_type_id'].unique())\n",
    "\n",
    "missing_soil_type_ids = soil_type_ids_in_profile_record - soil_type_ids_in_soil_type\n",
    "\n",
    "print(f\"Missing soil_type_ids in soil_type.csv: {missing_soil_type_ids}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
