{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in original csvs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.parser import parse\n",
    "\n",
    "#import and read data\n",
    "samples = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/AmostrasAngolaTerrario.xlsx\")\n",
    "analyses = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Horizontes Analises.xlsx\")\n",
    "morphology = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Horizontes_Morfologia.xlsx\")\n",
    "profile_loc = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Perfis_local.xlsx\")\n",
    "soil_profile = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Perfis_solo.xlsx\")\n",
    "elemental_analyses = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Data XRF Angola_inicial.xlsx\")\n",
    "#soil_type = pd.read_excel(\"/Users/inesschwartz/Desktop/Thesis/tables_soil_database/Perfis_solo.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning samples data (AmostrasAngolaTerrario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [sample_id, site_info_id, year, profile, campaign, country, district, sample_sifted, Prov√≠ncia, sample_not_sifted, shelf, room, Obs]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Rename columns for clarity\n",
    "samples.rename(columns={\n",
    "    'Registo': 'sample_id',\n",
    "    'N¬∫ Campo': 'site_info_id',\n",
    "    'Ano': 'year',\n",
    "    'Perfil': 'profile',\n",
    "    'Campanha': 'campaign',\n",
    "    'Col√≥nia_Pais': 'country',\n",
    "    'Distrito': 'district',\n",
    "    'AmostraCrivada': 'sample_sifted',\n",
    "    'AmostraNaoCrivada': 'sample_not_sifted',\n",
    "    'Prateleira': 'shelf',\n",
    "    'Sala': 'room'\n",
    "}, inplace=True)\n",
    "\n",
    "# Drop unused columns\n",
    "samples_cleaning = samples.drop(columns=[\n",
    "    'campaign', 'country', 'Prov√≠ncia', 'sample_not_sifted', 'sample_sifted', 'Obs'\n",
    "], errors='ignore')\n",
    "\n",
    "# # Add a new Primary Key ID column starting from 1\n",
    "# samples_cleaning.insert(0, 'sample_id', range(1, len(samples_cleaning) + 1))\n",
    "\n",
    "# This will show all rows that are duplicates (keep=False shows all duplicates, not just subsequent ones)\n",
    "duplicates1 = samples[samples.duplicated('sample_id',keep=False)]\n",
    "print(duplicates1)\n",
    "\n",
    "# Add empty FK columns\n",
    "samples_cleaning['lab_info_id'] = pd.NA\n",
    "samples_cleaning['morpho_id'] = pd.NA\n",
    "samples_cleaning['site_info_id'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>year</th>\n",
       "      <th>profile</th>\n",
       "      <th>district</th>\n",
       "      <th>shelf</th>\n",
       "      <th>room</th>\n",
       "      <th>lab_info_id</th>\n",
       "      <th>morpho_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>630</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>139</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>631</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>139</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>139</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>633</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>139</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>687</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>208</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id site_info_id    year profile district shelf room lab_info_id  \\\n",
       "0        630         <NA>  1946.0     139   Huambo     1   22        <NA>   \n",
       "1        631         <NA>  1946.0     139   Huambo     1   22        <NA>   \n",
       "2        632         <NA>  1946.0     139   Huambo     1   22        <NA>   \n",
       "3        633         <NA>  1946.0     139   Huambo     1   22        <NA>   \n",
       "4        687         <NA>  1946.0     208   Huambo     1   22        <NA>   \n",
       "\n",
       "  morpho_id  \n",
       "0      <NA>  \n",
       "1      <NA>  \n",
       "2      <NA>  \n",
       "3      <NA>  \n",
       "4      <NA>  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_cleaning.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Duplicate sample_id values:\n",
      "Empty DataFrame\n",
      "Columns: [sample_id]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Ensure consistent types and formatting\n",
    "samples_cleaning['profile'] = samples_cleaning['profile'].astype(str).str.replace('/', '_').str.strip().str[:20]\n",
    "samples_cleaning['shelf'] = samples_cleaning['shelf'].astype(\"string\")\n",
    "\n",
    "#formatting site_info_id\n",
    "samples_cleaning['site_info_id'] = samples_cleaning['site_info_id'].astype(str).str.replace('/', '_').str.strip().str[:20]\n",
    "#formatting morpho_id\n",
    "samples_cleaning['morpho_id'] = samples_cleaning['morpho_id'].astype(str).str.strip().str[:20]\n",
    "#formatting lab_info_id\n",
    "samples_cleaning['lab_info_id'] = samples_cleaning['lab_info_id'].astype(str).str.strip().str[:20]\n",
    "# formatting year\n",
    "samples_cleaning['year'] = samples_cleaning['year'].apply(\n",
    "    lambda x: str(int(x)) if pd.notnull(x) else ''\n",
    ")\n",
    "\n",
    "# Convert sample_id to string (no truncation unless necessary)\n",
    "samples_cleaning['sample_id'] = samples_cleaning['sample_id'].astype(str).str.strip()\n",
    "\n",
    "# Check for duplicates AFTER conversion\n",
    "duplicate_ids = samples_cleaning[samples_cleaning.duplicated('sample_id', keep=False)]\n",
    "print(\"üîç Duplicate sample_id values:\")\n",
    "print(duplicate_ids[['sample_id']])\n",
    "\n",
    "samples_cleaning1 = samples_cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_id               object\n",
       "site_info_id            object\n",
       "year                    object\n",
       "profile                 object\n",
       "district                object\n",
       "shelf           string[python]\n",
       "room                    object\n",
       "lab_info_id             object\n",
       "morpho_id               object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_cleaning1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>morpho_id</th>\n",
       "      <th>lab_info_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>year</th>\n",
       "      <th>shelf</th>\n",
       "      <th>room</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>630</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>139</td>\n",
       "      <td>1946</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>631</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>139</td>\n",
       "      <td>1946</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>139</td>\n",
       "      <td>1946</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>633</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>139</td>\n",
       "      <td>1946</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>687</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>208</td>\n",
       "      <td>1946</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id site_info_id morpho_id lab_info_id profile  year shelf room\n",
       "0       630         <NA>      <NA>        <NA>     139  1946     1   22\n",
       "1       631         <NA>      <NA>        <NA>     139  1946     1   22\n",
       "2       632         <NA>      <NA>        <NA>     139  1946     1   22\n",
       "3       633         <NA>      <NA>        <NA>     139  1946     1   22\n",
       "4       687         <NA>      <NA>        <NA>     208  1946     1   22"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reorder to match DB schema\n",
    "samples_check = samples_cleaning1[[\n",
    "    'sample_id',\n",
    "    'site_info_id',\n",
    "    'morpho_id',\n",
    "    'lab_info_id',\n",
    "    'profile',\n",
    "    'year',\n",
    "    'shelf',\n",
    "    'room'\n",
    "]]\n",
    "\n",
    "samples_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_check.to_csv(\"/Users/inesschwartz/Desktop/samples_check.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Soil Profile Table (dropping this table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##rename columns for clarity add FK/PK\n",
    "#create copy of samples\n",
    "soil_profile_record = samples.copy()\n",
    "# Rename columns in the DataFrame\n",
    "samples.rename(columns={\n",
    "    'Registo': 'sample_id',\n",
    "    'N¬∫ Campo': 'site_info_id',\n",
    "    'Ano': 'year',\n",
    "    'Perfil': 'profile',\n",
    "    'Campanha': 'campaign',\n",
    "    'Col√≥nia_Pais': 'country',\n",
    "    'Distrito': 'district',\n",
    "    'AmostraCrivada': 'sample_sifted',\n",
    "    'AmostraNaoCrivada': 'sample_not_sifted',\n",
    "    'Prateleira': 'shelf',\n",
    "    'Sala': 'room'\n",
    "}, inplace=True)\n",
    "\n",
    "soil_profile_cleaning2 = soil_profile_record\n",
    "# Add missing columns\n",
    "soil_profile_cleaning2['site_info_id'] = pd.NA\n",
    "soil_profile_cleaning2['sample_id'] = pd.NA \n",
    "soil_profile_cleaning2['soil_type_id'] = pd.NA \n",
    "soil_profile_cleaning2['site_info_id'] = soil_profile_cleaning2['site_info_id'].astype(str)\n",
    "\n",
    "# Add a new Primary Key ID column starting from 1\n",
    "# Check if 'lab_sample_id' exists and drop it\n",
    "if 'profile_record_id' in soil_profile_cleaning2.columns:\n",
    "    soil_profile_cleaning2 = soil_profile_cleaning2.drop(columns=['profile_record_id'])\n",
    "\n",
    "# Insert a new column, e.g., 'lab_sample_id' as a primary key starting from 1\n",
    "soil_profile_cleaning2.insert(0, 'profile_record_id', range(1, len(soil_profile_cleaning2) + 1))\n",
    "\n",
    "# Reorder columns to match SAMPLES schema\n",
    "profile_record_clean = soil_profile_cleaning2[[\n",
    "    'profile_record_id',\n",
    "    'profile',\n",
    "    'site_info_id',\n",
    "    'sample_id',\n",
    "    'soil_type_id'\n",
    "]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure consistent data types and formatting\n",
    "\n",
    "# correct datatypes\n",
    "#profile to string\n",
    "profile_record_clean['profile'] = profile_record_clean['profile'].astype(str).str.strip().str[:20]\n",
    "\n",
    "#sample_id to string\n",
    "# Convert cleaned values to numeric safely\n",
    "# Convert sample_id to numeric first (coerce errors to NaN)\n",
    "profile_record_clean['sample_id'] = pd.to_numeric(profile_record_clean['sample_id'], errors='coerce')\n",
    "# Then convert to string (will keep NaNs as <NA>)\n",
    "profile_record_clean['sample_id'] = profile_record_clean['sample_id'].astype('Int64').astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<NA>']\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates and nulls\n",
    "# Filter out and print duplicated samples (including the first occurrence)\n",
    "duplicated_values = profile_record_clean['sample_id'][profile_record_clean['sample_id'].duplicated()].unique()\n",
    "print(duplicated_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_record_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>soil_type_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>208</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   profile_record_id profile site_info_id sample_id soil_type_id\n",
       "0                  1     139         <NA>      <NA>         <NA>\n",
       "1                  2     139         <NA>      <NA>         <NA>\n",
       "2                  3     139         <NA>      <NA>         <NA>\n",
       "3                  4     139         <NA>      <NA>         <NA>\n",
       "4                  5     208         <NA>      <NA>         <NA>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the result\n",
    "profile_record_clean.head()\n",
    "#profile_record_clean.to_csv(\"/Users/inesschwartz/Desktop/profile_record_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Site info table (this gives info on location of soil profile/pit and its characteristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>ID</th>\n",
       "      <th>profile</th>\n",
       "      <th>X_coord</th>\n",
       "      <th>Y_coord</th>\n",
       "      <th>district</th>\n",
       "      <th>climate_id</th>\n",
       "      <th>geology_id</th>\n",
       "      <th>topo_feature_id</th>\n",
       "      <th>soil_type_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2770</td>\n",
       "      <td>1_57</td>\n",
       "      <td>12.161278</td>\n",
       "      <td>-15.222598</td>\n",
       "      <td>Namibe</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>1_59</td>\n",
       "      <td>12.575775</td>\n",
       "      <td>-4.866986</td>\n",
       "      <td>Cabinda</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1618</td>\n",
       "      <td>1_61</td>\n",
       "      <td>15.098840</td>\n",
       "      <td>-11.225411</td>\n",
       "      <td>Cuanza Sul</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>881</td>\n",
       "      <td>1_63</td>\n",
       "      <td>17.081955</td>\n",
       "      <td>-9.274587</td>\n",
       "      <td>Malanje</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1750</td>\n",
       "      <td>1_64</td>\n",
       "      <td>20.788116</td>\n",
       "      <td>-11.568683</td>\n",
       "      <td>Moxico</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   site_info_id    ID profile    X_coord    Y_coord    district climate_id  \\\n",
       "0             1  2770    1_57  12.161278 -15.222598      Namibe       <NA>   \n",
       "1             2    48    1_59  12.575775  -4.866986     Cabinda       <NA>   \n",
       "2             3  1618    1_61  15.098840 -11.225411  Cuanza Sul       <NA>   \n",
       "3             4   881    1_63  17.081955  -9.274587     Malanje       <NA>   \n",
       "4             5  1750    1_64  20.788116 -11.568683      Moxico       <NA>   \n",
       "\n",
       "  geology_id topo_feature_id soil_type_id  \n",
       "0       <NA>            <NA>         <NA>  \n",
       "1       <NA>            <NA>         <NA>  \n",
       "2       <NA>            <NA>         <NA>  \n",
       "3       <NA>            <NA>         <NA>  \n",
       "4       <NA>            <NA>         <NA>  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "site_info_cleaning = profile_loc\n",
    "\n",
    "site_info_cleaning.rename(columns={\n",
    "    'PERFIL': 'profile',\n",
    "    'X_COORD': 'X_coord',\n",
    "    'Y_COORD': 'Y_coord', \n",
    "    'PRO': 'district'\n",
    "}, inplace=True)\n",
    "\n",
    "# Add a new Primary Key ID column starting from 1\n",
    "site_info_cleaning.insert(0, 'site_info_id', range(1, len(site_info_cleaning) + 1))\n",
    "\n",
    "\n",
    "# Add missing columns\n",
    "site_info_cleaning['land_cover_id'] = pd.NA\n",
    "site_info_cleaning['climate_id'] = pd.NA  \n",
    "site_info_cleaning['geology_id'] = pd.NA\n",
    "site_info_cleaning['topo_feature_id'] = pd.NA  \n",
    "#site_info_cleaning['sampling_date'] = pd.NA # might leave out...\n",
    "site_info_cleaning['soil_type_id'] = pd.NA  \n",
    "#site_info_cleaning['sample_id'] = pd.NA  # don't want this, creates a many to one relationship\n",
    "\n",
    "\n",
    "# Function to remove accents\n",
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        return ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    return text\n",
    "\n",
    "# Apply to all object (string) columns\n",
    "for col in site_info_cleaning.select_dtypes(include='object').columns:\n",
    "    site_info_cleaning[col] = site_info_cleaning[col].apply(remove_accents)\n",
    "\n",
    "# Replace / with _ and strip/shorten 'profile' BEFORE slicing into site_info_clean\n",
    "site_info_cleaning['profile'] = (\n",
    "    site_info_cleaning['profile']\n",
    "    .astype(str)\n",
    "    .str.replace('/', '_')\n",
    "    .str.strip()\n",
    "    .str[:20]\n",
    ")\n",
    "\n",
    "# Then select columns\n",
    "site_info_clean = site_info_cleaning[[\n",
    "    'site_info_id',\n",
    "    'ID',\n",
    "    'profile',\n",
    "    # 'sampling_date', # maybe?\n",
    "    'X_coord',\n",
    "    'Y_coord',\n",
    "    'district',\n",
    "    #'land_cover_id',\n",
    "    'climate_id',\n",
    "    'geology_id',\n",
    "    'topo_feature_id',\n",
    "    'soil_type_id'\n",
    "    #'sample_id'\n",
    "\n",
    "]]\n",
    "\n",
    "\n",
    "site_info_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_info_clean.to_csv(\"/Users/inesschwartz/Desktop/site_info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphology Horizon Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "morphology.rename(columns={\n",
    "    'Morfo_id':'morpho_id',\n",
    "    'Amostra': 'sample_id',\n",
    "    'Perfil': 'profile',\n",
    "    'CM':'horizon_layer',\n",
    "    'Limite Superior': 'upper_depth',\n",
    "    'Limite inferior': 'lower_depth',\n",
    "    'Grau de humidade': 'moisture_degree',\n",
    "    'Quantidade de ra√≠zes': 'root_quantity',\n",
    "    'Di√¢metro de ra√≠zes': 'root_diameter',\n",
    "    'Manchas': 'stains',\n",
    "    'Textura': 'texture',\n",
    "    'Abund√¢ncia de elementos grosseiros': 'thick_contents_count',\n",
    "    'Forma de elementos grosseiros': 'thick_contents_shape',\n",
    "    'Natureza de elementos grosseiros': 'thick_contents_nature',\n",
    "    'Tipo de estrutura': 'structure_type',\n",
    "    'Classes de estrutura': 'structure_class',\n",
    "    'Grau de estrutura': 'structure_degree',\n",
    "    'Di√¢metro de poros': 'pore_diameter',\n",
    "    'Quantidade de poros': 'pore_quantity',\n",
    "    'Forma de poros': 'pore_shape',\n",
    "    'Cor (s)': 'dry_color_name',\n",
    "    'Matiz (s)': 'dry_hue',\n",
    "    'Valor (s)':'dry_value',\n",
    "    'Croma (s)': 'dry_chroma',\n",
    "    'Cor (h)': 'moist_color_name',\n",
    "    'Matiz (h)': 'moist_hue',\n",
    "    'Valor (h)': 'moist_value',\n",
    "    'Croma (h)': 'moist_chroma',\n",
    "    'Compacidade':'compaction',\n",
    "    'Dureza': 'durability', \n",
    "    'Friabilidade': 'friability'\n",
    "}, inplace=True)\n",
    "\n",
    "# # Drop unnecessary columns\n",
    "# morphology_cleaning = morphology.drop(columns=[\n",
    "#     'ID1', 'Agrupamento', 'REF', 'Pro', 'Observa√ßoes', 'Horizonte de diagn√≥stico', 'Propriedade de diagn√≥stico', 'Nitidez do limite', 'Designa√ß√£o do horizonte', 'Observa√ßoes', 'Confirmar', 'Adesividade', 'Plasticidade', 'Efervesc√™ncia com HCl', 'Friabilidade', 'Orienta√ß√£o das Fendas', 'Largura das fendas', 'Quantidade de fendas'\n",
    "# ])\n",
    "\n",
    "morphology_cleaning = morphology\n",
    "#add profile_record_id to populate later from soil profile table\n",
    "#morphology_cleaning['profile_record_id'] = pd.NA\n",
    "\n",
    "#drop accents\n",
    "import unicodedata\n",
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        # Normalize and remove diacritics\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "# Apply to all cells in the DataFrame\n",
    "morphology_cleaning = morphology_cleaning.applymap(remove_accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>morpho_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>horizon_layer</th>\n",
       "      <th>upper_depth</th>\n",
       "      <th>lower_depth</th>\n",
       "      <th>moisture_degree</th>\n",
       "      <th>root_quantity</th>\n",
       "      <th>root_diameter</th>\n",
       "      <th>texture</th>\n",
       "      <th>...</th>\n",
       "      <th>dry_chroma</th>\n",
       "      <th>moist_color_name</th>\n",
       "      <th>moist_hue</th>\n",
       "      <th>moist_value</th>\n",
       "      <th>moist_chroma</th>\n",
       "      <th>compaction</th>\n",
       "      <th>durability</th>\n",
       "      <th>friability</th>\n",
       "      <th>thick_contents_count</th>\n",
       "      <th>thick_contents_nature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>10999.0</td>\n",
       "      <td>101/62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Seco</td>\n",
       "      <td>Muitas finas e bastantes medias</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arenoso</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Pardo-acinzentado-escuro</td>\n",
       "      <td>10YR</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Pequena a minima</td>\n",
       "      <td>Brando</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>101/62</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Seco</td>\n",
       "      <td>Bastantes finas e medias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arenoso-franco</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Pardo-amarelado-escuro</td>\n",
       "      <td>10YR</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Brando</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>11001.0</td>\n",
       "      <td>101/62</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>Seco</td>\n",
       "      <td>Algumas finas e medias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arenoso-franco</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Pardo-amarelado-escuro</td>\n",
       "      <td>10YR</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Pequena a minima</td>\n",
       "      <td>Brando</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>11002.0</td>\n",
       "      <td>101/62</td>\n",
       "      <td>4.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Seco</td>\n",
       "      <td>Poucas finas, algumas medias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Franco-arenoso a arenoso-franco</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Pardo-forte</td>\n",
       "      <td>7,5YR</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Pequena a minima</td>\n",
       "      <td>Brando</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B_101/62_5_2</td>\n",
       "      <td>11003.2</td>\n",
       "      <td>101/62</td>\n",
       "      <td>5.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>Seco a humido</td>\n",
       "      <td>Raras</td>\n",
       "      <td>Medias e grossas</td>\n",
       "      <td>Arenoso-franco</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Pardo-forte</td>\n",
       "      <td>7,5YR</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Brando</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      morpho_id  sample_id profile  horizon_layer  upper_depth  lower_depth  \\\n",
       "0  B_101/62_1_1    10999.0  101/62            1.0          0.0         11.0   \n",
       "1  B_101/62_2_1    11000.0  101/62            2.0         11.0         28.0   \n",
       "2  B_101/62_3_1    11001.0  101/62            3.0         28.0         54.0   \n",
       "3  B_101/62_4_1    11002.0  101/62            4.0         54.0         90.0   \n",
       "4  B_101/62_5_2    11003.2  101/62            5.0         90.0        160.0   \n",
       "\n",
       "  moisture_degree                                 root_quantity  \\\n",
       "0            Seco               Muitas finas e bastantes medias   \n",
       "1            Seco      Bastantes finas e medias e raras grossas   \n",
       "2            Seco        Algumas finas e medias e raras grossas   \n",
       "3            Seco  Poucas finas, algumas medias e raras grossas   \n",
       "4   Seco a humido                                         Raras   \n",
       "\n",
       "      root_diameter                          texture  ... dry_chroma  \\\n",
       "0               NaN                          Arenoso  ...        2.5   \n",
       "1               NaN                   Arenoso-franco  ...        3.0   \n",
       "2               NaN                   Arenoso-franco  ...        4.0   \n",
       "3               NaN  Franco-arenoso a arenoso-franco  ...        6.0   \n",
       "4  Medias e grossas                   Arenoso-franco  ...        6.0   \n",
       "\n",
       "           moist_color_name moist_hue moist_value moist_chroma  \\\n",
       "0  Pardo-acinzentado-escuro      10YR         4.0          2.0   \n",
       "1    Pardo-amarelado-escuro      10YR         3.0          4.0   \n",
       "2    Pardo-amarelado-escuro      10YR         4.0          4.0   \n",
       "3               Pardo-forte     7,5YR         5.0          6.0   \n",
       "4               Pardo-forte     7,5YR         5.0          6.0   \n",
       "\n",
       "         compaction durability friability  thick_contents_count  \\\n",
       "0  Pequena a minima     Brando        NaN                   NaN   \n",
       "1           Pequena     Brando        NaN                   NaN   \n",
       "2  Pequena a minima     Brando        NaN                   NaN   \n",
       "3  Pequena a minima     Brando        NaN                   NaN   \n",
       "4           Pequena     Brando        NaN                   NaN   \n",
       "\n",
       "   thick_contents_nature  \n",
       "0                    NaN  \n",
       "1                    NaN  \n",
       "2                    NaN  \n",
       "3                    NaN  \n",
       "4                    NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reorder columns to match SAMPLES schema\n",
    "morphology_cleaning1 = morphology_cleaning [[\n",
    "    'morpho_id',\n",
    "    'sample_id',\n",
    "    #'profile_record_id',\n",
    "    'profile',\n",
    "    'horizon_layer',\n",
    "    'upper_depth',\n",
    "    'lower_depth',\n",
    "    'moisture_degree',\n",
    "    'root_quantity',\n",
    "    'root_diameter',\n",
    "    'texture',\n",
    "    'structure_type',\n",
    "    'structure_class',\n",
    "    'structure_degree',\n",
    "    'pore_diameter',\n",
    "    'pore_quantity',\n",
    "    'pore_shape',\n",
    "    'dry_color_name',\n",
    "    'dry_hue',\n",
    "    'dry_value',\n",
    "    'dry_chroma',\n",
    "    'moist_color_name',\n",
    "    'moist_hue',\n",
    "    'moist_value',\n",
    "    'moist_chroma',\n",
    "    'compaction',\n",
    "    'durability',\n",
    "    'friability',\n",
    "    'thick_contents_count',\n",
    "    'thick_contents_nature'\n",
    "]]\n",
    "\n",
    "# Show first few rows\n",
    "morphology_cleaning1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/79mdnyy56_xc3g1jvp9wf4_80000gn/T/ipykernel_11715/4253896047.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morphology_cleaning1['profile'] = morphology_cleaning1['profile'].astype(str).str.replace('/', '_').str.strip().str[:20]\n",
      "/var/folders/tp/79mdnyy56_xc3g1jvp9wf4_80000gn/T/ipykernel_11715/4253896047.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morphology_cleaning1['sample_id'] = morphology_cleaning1['sample_id'].apply(clean_sample_id)\n"
     ]
    }
   ],
   "source": [
    "# Ensure consistent data types and formatting\n",
    "morphology_cleaning1['profile'] = morphology_cleaning1['profile'].astype(str).str.replace('/', '_').str.strip().str[:20]\n",
    "\n",
    "#convert sample_to to string of the integer if it's a float like 11003.0\n",
    "#The original string if it's not an integer or has decimal places\n",
    "#\"NULL\" if the value is missing\n",
    "def clean_sample_id(x):\n",
    "    if pd.isnull(x):\n",
    "        return \"NULL\"\n",
    "    try:\n",
    "        float_val = float(x)\n",
    "        if float_val.is_integer():\n",
    "            return str(int(float_val))\n",
    "        else:\n",
    "            return str(float_val)\n",
    "    except:\n",
    "        return str(x)\n",
    "\n",
    "morphology_cleaning1['sample_id'] = morphology_cleaning1['sample_id'].apply(clean_sample_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NULL']\n",
      "üîç Number of sample_id values equal to 'NULL': 6638\n",
      "[]\n",
      "üîç Number of samorpho_idmple_id values equal to 'NULL': 0\n"
     ]
    }
   ],
   "source": [
    "# Check for nulls in 'sample_id' column\n",
    "null_string_values = morphology_cleaning1[morphology_cleaning1['sample_id'] == \"NULL\"]['sample_id'].unique()\n",
    "print(null_string_values)\n",
    "\n",
    "null_string_count = (morphology_cleaning1['sample_id'] == \"NULL\").sum()\n",
    "print(f\"üîç Number of sample_id values equal to 'NULL': {null_string_count}\")\n",
    "\n",
    "#check for nulls in morpho_id column\n",
    "null_string_values = morphology_cleaning1[morphology_cleaning1['morpho_id'] == \"NULL\"]['sample_id'].unique()\n",
    "print(null_string_values)\n",
    "\n",
    "null_string_count = (morphology_cleaning1['morpho_id'] == \"NULL\").sum()\n",
    "print(f\"üîç Number of samorpho_idmple_id values equal to 'NULL': {null_string_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal** = drop any morpho_id from morphology_cleaning that does not have both:\n",
    "\n",
    "1. Key horizon characteristics\n",
    "\n",
    "2. A valid profile (w location and site characteristics) that appears in site_info_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Found 2597 rows with at least one of the following issues:\n",
      "   ‚Ä¢ missing key horizon data\n",
      "   ‚Ä¢ invalid profile (not in site_info_clean)\n",
      "   ‚Ä¢ missing both upper_depth and lower_depth\n",
      "‚úÖ Remaining rows in morphology_cleaning: 10438\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------\n",
    "# 1. Standardize 'profile' column formatting\n",
    "# -----------------------------------------\n",
    "site_info_clean.loc[:, 'profile'] = (\n",
    "    site_info_clean['profile']\n",
    "    .astype(str)\n",
    "    .str.replace('/', '_')\n",
    "    .str.strip()\n",
    "    .str.upper()\n",
    "    .str[:20]\n",
    ")\n",
    "\n",
    "morphology_cleaning1.loc[:, 'profile'] = (\n",
    "    morphology_cleaning1['profile']\n",
    "    .astype(str)\n",
    "    .str.replace('/', '_')\n",
    "    .str.strip()\n",
    "    .str.upper()\n",
    "    .str[:20]\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Define valid profiles\n",
    "# ---------------------------\n",
    "valid_profiles = set(site_info_clean['profile'])\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Define helper to check for missing/zero\n",
    "# ---------------------------\n",
    "def is_missing(val):\n",
    "    return pd.isna(val) or val == 0 or val == '0'\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Build masks for filtering\n",
    "# ---------------------------\n",
    "\n",
    "# A. Missing key horizon data\n",
    "missing_key_data_mask = (\n",
    "    morphology_cleaning1['morpho_id'].notna() &\n",
    "    morphology_cleaning1['moist_color_name'].apply(is_missing) &\n",
    "    morphology_cleaning1['texture'].apply(is_missing) &\n",
    "    morphology_cleaning1['structure_type'].apply(is_missing)\n",
    ")\n",
    "\n",
    "# B. Invalid profile\n",
    "invalid_profile_mask = ~morphology_cleaning1['profile'].isin(valid_profiles)\n",
    "\n",
    "# C. Missing both upper and lower depth\n",
    "missing_depth_mask = (\n",
    "    morphology_cleaning1['upper_depth'].apply(is_missing) &\n",
    "    morphology_cleaning1['lower_depth'].apply(is_missing)\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Combine all masks\n",
    "# ---------------------------\n",
    "rows_to_flag_mask = missing_key_data_mask | invalid_profile_mask | missing_depth_mask\n",
    "rows_to_flag = morphology_cleaning[rows_to_flag_mask]\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Report and drop flagged rows\n",
    "# ---------------------------\n",
    "print(f\"‚ö†Ô∏è Found {len(rows_to_flag)} rows with at least one of the following issues:\")\n",
    "print(\"   ‚Ä¢ missing key horizon data\")\n",
    "print(\"   ‚Ä¢ invalid profile (not in site_info_clean)\")\n",
    "print(\"   ‚Ä¢ missing both upper_depth and lower_depth\")\n",
    "\n",
    "# Optional: Save flagged rows for review\n",
    "# rows_to_flag.to_csv(\"/Users/inesschwartz/Desktop/morph_flagged_rows_combined.csv\", index=False)\n",
    "\n",
    "# Drop flagged rows from main DataFrame\n",
    "morphology_cleaning1 = morphology_cleaning1[~rows_to_flag_mask].copy()\n",
    "\n",
    "# Summary after cleanup\n",
    "print(f\"‚úÖ Remaining rows in morphology_cleaning: {len(morphology_cleaning1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11016' '11017' 'NULL' '11352' '11353' '17532' '17533' '17534' '2438'\n",
      " '7192' '13178' '20883' '9027' '9231' '8232' '9255' '8639' '8536' '4918']\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates\n",
    "# Make sure sample_id and horizon_id are strings\n",
    "morphology_cleaning1['sample_id'] = morphology_cleaning1['sample_id'].astype(str).str.strip()\n",
    "morphology_cleaning1['morpho_id'] = morphology_cleaning1['morpho_id'].astype(str).str.strip()\n",
    "# Filter out and print duplicated samples (including the first occurrence)\n",
    "duplicated_values = morphology_cleaning1['sample_id'][morphology_cleaning1['sample_id'].duplicated()].unique()\n",
    "print(duplicated_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Morphology data cleaned and sample_id updated via morpho_id.\n"
     ]
    }
   ],
   "source": [
    "#addressing duplicates\n",
    "\n",
    "# -----------------------------------\n",
    "# 1. Correct sample_id using morpho_id map\n",
    "# -----------------------------------\n",
    "# Step 1: Make sure IDs are strings and trimmed\n",
    "morphology_cleaning1['morpho_id'] = morphology_cleaning1['morpho_id'].astype(str).str.strip()\n",
    "morphology_cleaning1['sample_id'] = morphology_cleaning1['sample_id'].astype(str).str.strip()\n",
    "\n",
    "# Step 2: Your mapping (morpho_id ‚Üí new sample_id)\n",
    "horizon_to_sample_map = {\n",
    "    'UZ_24c/60_1_1': '8639',\n",
    "    'UZ_24c/60_2_1': '8640',\n",
    "    'UZ_24c/60_3_1': '8641',\n",
    "    'UZ_24c/60_4_1': '8642',\n",
    "    'UZ_24c/60_5_1': '8643',\n",
    "    'UZ_66c/60_1_1': '8777',\n",
    "    'UZ_66c/60_2_1': '8778',\n",
    "    'UZ_66c/60_3_1': '8779',\n",
    "    'UZ_66c/60_4_1': '8780',\n",
    "    'CC_473/66_1_1': '17596',\n",
    "    'CC_473/66_2_1': '17597',\n",
    "    'CC_473/66_3_1': '17598',\n",
    "    'N_85/57_1_0': '4947',\n",
    "    'N_85/57_2_0': '4948',\n",
    "    'N_85/57_3_0': '4949',\n",
    "    'UZ_231/60_4_1': '8255',\n",
    "    'B_139/61_5_1': '10353',\n",
    "    'H_480/55_1_1': '3919',\n",
    "    'H_91/54_1_1': '1255',\n",
    "    'H_90/54_3_3': '1252',\n",
    "    'Hb_136/56_1_1':'2961',\n",
    "    'Hb_136/56_2_1': '2962',\n",
    "    'Hb_136/56_3_1': '2963',\n",
    "    'Hb_136/56_4_1': '2964',\n",
    "    'UZ_216/60_5_1': '8231',\n",
    "    'Bg_217/46_1_1': '694',\n",
    "    'Bg_217/46_2_1': '695',\n",
    "    'Bg_217/46_3_1': '696',\n",
    "    'UZ_215c/60_1_1':'9255',\n",
    "    'UZ_215c/60_2_1': '9256',\n",
    "    'UZ_215c/60_3_1': '9257',\n",
    "    'UZ_215c/60_4_1': '9258',\n",
    "    'UZ_215c/60_5_1': '9259',\n",
    "    'UZ_215c/60_6_1': '9260',\n",
    "    'N_30/57_1_1':'4848',\n",
    "    'N_30/57_2_2':'4849',\n",
    "    'N_30/57_3_1':'4850',\n",
    "    'UZ_66c/60_1_1': '8777',\n",
    "    'UZ_66c/60_2_1': '8778',\n",
    "    'UZ_66c/60_3_1': '8779',\n",
    "    'UZ_66c/60_4_1': '8780',\n",
    "    'UZ_66c/60_5_1': '8781',\n",
    "    'UZ_66c/60_6_1': '8783',\n",
    "    'UZ_66c/60_7_1': '8782',\n",
    "    'H_91/54_1_1': '1255',\n",
    "    'H_91/54_2_1': '1256',\n",
    "    'H_91/54_3_1': '1257'\n",
    "}\n",
    "\n",
    "# Step 3: Apply the mapping to replace sample_id based on horizon_id\n",
    "morphology_cleaning1['sample_id'] = morphology_cleaning1.apply(\n",
    "    lambda row: horizon_to_sample_map.get(row['morpho_id'], row['sample_id']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# -----------------------------------\n",
    "# 2. Drop specific horizon_ids (might have already been dropped)\n",
    "# -----------------------------------\n",
    "horizons_to_drop = [\n",
    "    'B_139/61_4_1', 'H_113/55_1_1', 'H_1306/52_1_1', 'H_1611/52_1_1',\n",
    "    'H_227/46_1_1', 'H_227/46_2_1', 'H_227/46_3_1', 'H_227/46_4_1',\n",
    "    'Mj_178c/63_4_1', 'Mj_321c/63_4_1', 'B_110/62_5_1', 'B_110/62_6_1'\n",
    "]\n",
    "morphology_cleaning1 = morphology_cleaning1[~morphology_cleaning1['morpho_id'].isin(horizons_to_drop)]\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# 3. Done ‚Äî Optional: Save to file\n",
    "# -----------------------------------\n",
    "#df_morpho_cleaned.to_csv(\"/Users/inesschwartz/Desktop/df_morpho_cleaned_corrected.csv\", index=False)\n",
    "print(\"‚úÖ Morphology data cleaned and sample_id updated via morpho_id.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NULL']\n",
      "üîç Number of sample_id values equal to 'NULL': 4713\n"
     ]
    }
   ],
   "source": [
    "#re-count null sample_id's\n",
    "morphology_cleaning1\n",
    "# Check for nulls in 'sample_id' column\n",
    "null_string_values2 = morphology_cleaning1[morphology_cleaning1['sample_id'] == \"NULL\"]['sample_id'].unique()\n",
    "print(null_string_values2)\n",
    "\n",
    "null_string_count2 = (morphology_cleaning1['sample_id'] == \"NULL\").sum()\n",
    "print(f\"üîç Number of sample_id values equal to 'NULL': {null_string_count2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Found 0 duplicated morpho_id(s).\n"
     ]
    }
   ],
   "source": [
    "num_duplicates = morphology_cleaning1['morpho_id'].duplicated().sum()\n",
    "print(f\"üîÅ Found {num_duplicates} duplicated morpho_id(s).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_morpho_cleaned = morphology_cleaning1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dealing w decimals in sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Found 439 sample_id(s) with decimal values.\n",
      "\n",
      "üßæ Affected sample_ids:\n",
      "['11003.2' '11285.2' '10843.2' '10428.2' '10470.2' '10480.2' '10485.2'\n",
      " '10446.2' '10849.2' '10509.2' '10562.2' '10536.2' '11327.2' '11356.2'\n",
      " '11393.2' '11176.2' '11454.2' '10885.2' '10904.2' '10922.2' '11485.2'\n",
      " '10933.2' '4972.3' '4981.2' '5810.2' '5045.2' '5822.2' '4989.2' '5006.2'\n",
      " '5010.2' '5839.2' '4996.2' '5082.2' '5026.2' '6314.2' '6317.2' '6323.2'\n",
      " '6332.2' '6336.3' '5883.2' '6342.3' '6348.2' '6350.2' '6358.2' '5902.2'\n",
      " '5897.3' '6376.3' '6381.2' '6383.3' '6388.2' '6391.2' '6402.2' '6405.2'\n",
      " '6416.2' '5914.2' '5916.2' '6439.3' '5088.2' '5053.2' '5101.2' '5107.2'\n",
      " '5123.2' '5945.2' '5174.2' '5176.2' '5963.2' '5973.2' '5202.2' '5207.2'\n",
      " '5192.2' '5214.2' '5218.2' '5225.3' '5983.2' '5990.2' '5270.2' '5995.2'\n",
      " '5997.2' '5447.2' '6014.2' '6022.2' '5417.2' '5332.2' '6032.2' '5476.2'\n",
      " '5478.2' '5434.2' '5359.2' '5442.2' '5444.2' '5327.2' '5342.2' '5315.2'\n",
      " '5348.2' '7390.2' '7397.2' '5717.2' '5745.2' '5751.2' '5737.2' '5777.2'\n",
      " '7408.2' '5770.2' '5780.2' '20912.2' '20921.2' '19554.2' '19255.2'\n",
      " '18814.2' '18740.2' '6787.2' '6903.2' '6924.2' '6684.2' '6686.2' '6921.2'\n",
      " '7047.2' '6726.2' '6815.2' '6754.2' '6633.2' '6635.2' '6890.2' '6908.2'\n",
      " '6895.2' '6897.2' '6652.2' '6964.2' '7108.2' '6872.2' '6669.2' '6768.2'\n",
      " '6931.2' '6853.2' '7268.2' '7141.2' '6988.2' '7011.2' '7184.2' '6974.2'\n",
      " '7289.2' '7291.2' '7255.2' '16853.2' '16870.2' '18498.2' '18513.2'\n",
      " '17536.2' '18575.2' '18588.2' '18671.2' '17607.2' '18703.2' '10064.2'\n",
      " '11560.2' '11794.2' '1100.2' '1557.2' '1560.2' '1752.2' '1754.2' '1747.3'\n",
      " '1275.3' '1785.3' '1792.2' '1343.2' '1352.3' '1356.3' '1802.2' '1366.3'\n",
      " '1325.3' '836.2' '1376.4' '1383.2' '1335.3' '1388.2' '1108.2' '1562.2'\n",
      " '1413.2' '1408.3' '1111.2' '1568.2' '2248.2' '1843.2' '1845.2' '2253.2'\n",
      " '1426.3' '1850.2' '1580.2' '1442.2' '1602.2' '1604.2' '2263.2' '1463.2'\n",
      " '1119.2' '1475.2' '1490.3' '1485.2' '1874.2' '2268.2' '1589.3' '2282.2'\n",
      " '1140.2' '1585.2' '1598.2' '1144.3' '1938.2' '1149.3' '1956.3' '1968.2'\n",
      " '1154.2' '1158.3' '1991.2' '1993.2' '1548.3' '2002.2' '2013.4' '2007.3'\n",
      " '2371.2' '2373.3' '2378.2' '2380.2' '2382.2' '2019.2' '2023.2' '2055.2'\n",
      " '2057.2' '2051.2' '2393.2' '2398.2' '2400.2' '2075.2' '2069.3' '2408.2'\n",
      " '2410.3' '2415.3' '2093.2' '2089.2' '2107.4' '2115.2' '2136.2' '1627.2'\n",
      " '1629.2' '1546.2' '2463.2' '1184.2' '2169.2' '2475.2' '1191.2' '1200.3'\n",
      " '1206.2' '1209.2' '1215.2' '1220.2' '1225.2' '1229.2' '1236.2' '2921.2'\n",
      " '2888.2' '2890.3' '2906.2' '2908.2' '2910.2' '2927.2' '609.2' '612.2'\n",
      " '3506.2' '2945.2' '2716.3' '2994.2' '2987.2' '2978.2' '2981.2' '3021.3'\n",
      " '3006.2' '3008.2' '3014.2' '3016.3' '3535.2' '299.2' '3031.2' '3526.2'\n",
      " '3528.3' '3045.2' '3050.2' '3052.2' '3103.2' '3109.2' '3119.2' '2730.2'\n",
      " '3145.2' '3179.2' '3570.2' '3572.2' '3574.2' '3156.2' '3151.2' '3161.2'\n",
      " '3163.2' '3165.2' '3194.2' '3213.2' '3216.2' '3606.2' '3259.2' '3636.2'\n",
      " '2750.3' '3290.2' '3309.2' '2743.3' '2738.2' '2763.2' '2765.2' '2757.3'\n",
      " '2776.2' '2785.2' '2711.2' '3492.2' '2847.2' '2842.3' '2877.2' '2869.2'\n",
      " '10576.2' '19244.2' '15778.2' '15305.2' '16266.2' '15005.2' '15286.2'\n",
      " '15049.2' '15337.2' '15031.2' '15032.2' '15957.2' '15951.2' '15896.2'\n",
      " '15352.2' '15370.2' '16494.2' '15641.2' '15132.2' '14857.2' '14455.2'\n",
      " '14920.2' '16031.2' '13521.2' '13779.2' '13765.2' '13493.2' '13817.2'\n",
      " '13793.2' '13810.2' '13804.2' '13475.2' '13883.2' '13901.2' '13481.2'\n",
      " '13924.2' '14018.2' '13999.2' '13988.2' '13931.2' '14112.2' '14119.2'\n",
      " '13394.2' '14071.2' '13549.2' '14135.2' '14064.2' '13387.2' '14156.2'\n",
      " '13556.2' '13274.2' '12692.2' '13532.2' '14249.2' '13299.2' '14278.2'\n",
      " '14213.2' '14266.2' '14296.2' '13404.2' '12715.2' '51.2' '12636.2'\n",
      " '578.2' '12630.2' '13599.2' '75.1' '13581.2' '12653.2' '13621.2'\n",
      " '13606.2' '13638.2' '13661.2' '13695.2' '4840.3' '3348.2' '3417.2'\n",
      " '3412.2' '4867.3' '4899.2' '4908.2' '4916.2' '4920.2' '5404.2' '4958.2'\n",
      " '9039.2' '8959.2' '9125.2' '8119.2' '9183.2' '9208.2' '9143.2' '9155.2'\n",
      " '7838.2' '9281.2' '7849.2' '9297.2' '9302.2' '9449.2' '8274.2' '8271.2'\n",
      " '9442.2' '6549.2' '9564.2' '9455.2' '9604.2' '8360.2' '7859.2' '7817.2'\n",
      " '8681.2' '7932.2' '7935.2' '7944.2' '7958.2' '8012.2' '8865.2' '8830.2']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Ensure sample_id is a string\n",
    "df_morpho_cleaned['sample_id'] = df_morpho_cleaned['sample_id'].astype(str).str.strip()\n",
    "\n",
    "# Step 2: Identify sample_ids that contain a decimal point\n",
    "decimal_sample_ids = df_morpho_cleaned[df_morpho_cleaned['sample_id'].str.contains(r'\\.\\d+$', regex=True)]\n",
    "\n",
    "# Step 3: Output results\n",
    "print(f\"üîç Found {len(decimal_sample_ids)} sample_id(s) with decimal values.\\n\")\n",
    "\n",
    "# Optional: Print the unique sample_ids with decimals\n",
    "print(\"üßæ Affected sample_ids:\")\n",
    "print(decimal_sample_ids['sample_id'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## take out all decimals in sample_id (keep as just whole number)\n",
    "# Function to truncate decimal part and return string\n",
    "def truncate_to_int_string(x):\n",
    "    try:\n",
    "        return str(int(float(x)))\n",
    "    except:\n",
    "        return str(x).strip()  # fallback to string version if it can't convert\n",
    "\n",
    "# Apply to sample_id column\n",
    "df_morpho_cleaned['sample_id'] = df_morpho_cleaned['sample_id'].apply(truncate_to_int_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "df_morpho_cleaned.to_csv(\"/Users/inesschwartz/Desktop/morpho_cleaned_updated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab_Info  table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Amostra', 'ID', 'Morfo_id', 'Analise-id', 'PERFIL', 'GR', 'COD_PROV',\n",
       "       'NA', 'LS', 'LI', 'EG', 'AG', 'AF', 'L', 'Argila', 'Eq_Hum', 'atm_1/3',\n",
       "       'atm_15', 'CACO3', 'Gesso', 'Fe livre', 'CO', 'N total', 'C/N', 'MO',\n",
       "       'P205 total', 'pH (H2O)', 'pH(KCL)', 'Ca++', 'Mg++', 'K+', 'Na+',\n",
       "       'Soma de bases', 'CTC', 'V', 'Cloretos', 'Sulfatos', 'Condutividade',\n",
       "       'S√≥dio sol√∫vel', 'EqMol (SiO2)', 'EqMol(Al2O3)', 'EqMol(Fe2O3)',\n",
       "       'SiO2/Al2O3', 'SiO2/Fe2O3', 'SiO2/R2O3', 'Fe2O3/Al2O3', 'FE2O3_TARG',\n",
       "       'FE2O3_LARG', 'CEC_ARG', 'Min_<0,002', 'Min_0,05-0,02', 'Min_0,2-0,05',\n",
       "       'Min_2-0,2', 'Confirmar'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyses = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Horizontes Analises.xlsx\")\n",
    "\n",
    "analyses.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change column names\n",
    "analyses.rename(columns={\n",
    "    'Amostra': 'sample_id',\n",
    "    'Morfo_id': 'morpho_id',\n",
    "    'Analise-id': 'analysis_id',\n",
    "    'PERFIL': 'profile', #change to reference PK profile_record_id\n",
    "    'LS': 'upper_limit',\n",
    "    'LI': 'lower_limit',\n",
    "    'EG': 'EG',\n",
    "    'AG': 'thick_sand',\n",
    "    'AF': 'fine_sand',\n",
    "    'Argila': 'clay',\n",
    "    'L': 'silt',\n",
    "    'Gesso': 'gypsum',\n",
    "    'Fe livre': 'free_iron',\n",
    "    'CO':'organic_carbon',\n",
    "    'N total': 'total_N',\n",
    "    'MO': 'OM',\n",
    "    'Soma de bases': 'exchangable_bases_sum',\n",
    "    'CTC': 'CEC',\n",
    "    'Cloretos':'chlorides',\n",
    "    'Sulfatos':'sulfates',\n",
    "    'Condutividade':'conductivity',\n",
    "    'S√≥dio sol√∫vel':'soluble_sodium',\n",
    "    'P205 total': 'P205',\n",
    "    'pH (H2O)':'pH_H2O',\n",
    "    'pH(KCL)':'pH_KCL',\n",
    "    'MO': 'organic_material',\n",
    "    'atm_1/3': 'atm_1_3',\n",
    "    'Ca++': 'Ca',\n",
    "    'Mg++': 'Mg',\n",
    "    'Na+': 'Na',\n",
    "    'K+': 'K',\n",
    "    'Min_<0,002': 'Min_lt_0002',\n",
    "    'Min_0,05-0,02': 'Min_005_002',\n",
    "    'Min_0,2-0,05': 'Min_02_005',\n",
    "    'Min_2-0,2': 'Min_2_02',\n",
    "}, inplace=True)\n",
    "\n",
    "# Drop columns not needed\n",
    "analyses_drop = analyses.drop([\n",
    "    'EqMol (SiO2)', 'EqMol(Al2O3)', 'EqMol(Fe2O3)', 'SiO2/Al2O3', \n",
    "    'SiO2/Fe2O3', 'SiO2/R2O3', 'Fe2O3/Al2O3', 'FE2O3_TARG', \n",
    "    'FE2O3_LARG', 'CEC_ARG', 'GR', 'COD_PROV'\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new Primary Key ID column starting from 1\n",
    "#drop lab_sample_id if already exist error comes up\n",
    "# Check if 'lab_sample_id' exists and drop it\n",
    "if 'lab_sample_id' in analyses_drop.columns:\n",
    "    analyses_drop = analyses_drop.drop(columns=['lab_sample_id'])\n",
    "\n",
    "# Insert a new column, e.g., 'lab_sample_id' as a primary key starting from 1\n",
    "analyses_drop.insert(0, 'lab_sample_id', range(1, len(analyses_drop) + 1))\n",
    "\n",
    "#add minerology_id column\n",
    "analyses_drop['minerology_id'] = pd.NA\n",
    "#add soil_biology_id column (just in case)\n",
    "analyses_drop['soil_biology_id'] = pd.NA\n",
    "\n",
    "\n",
    "#Define the desired final column order\n",
    "final_columns = [\n",
    "    'lab_sample_id',\n",
    "    'analysis_id',\n",
    "    'morpho_id',\n",
    "    'sample_id',\n",
    "    'profile',\n",
    "    'minerology_id',\n",
    "    'soil_biology_id',\n",
    "    'upper_limit',\n",
    "    'lower_limit',\n",
    "    'EG',\n",
    "    'thick_sand',\n",
    "    'fine_sand',\n",
    "    'silt',\n",
    "    'clay',\n",
    "    'Eq_Hum',\n",
    "    'atm_1_3',\n",
    "    'atm_15',\n",
    "    'CACO3',\n",
    "    'gypsum',\n",
    "    'free_iron',\n",
    "    'organic_carbon',\n",
    "    'total_N',\n",
    "    'P205',\n",
    "    'organic_material',\n",
    "    'pH_H2O',\n",
    "    'pH_KCL',\n",
    "    'Ca',\n",
    "    'Mg',\n",
    "    'Na',\n",
    "    'K',\n",
    "    'exchangable_bases_sum',\n",
    "    'CEC',\n",
    "    'V',\n",
    "    'conductivity',\n",
    "    'soluble_sodium',\n",
    "    'Min_lt_0002',\n",
    "    'Min_005_002',\n",
    "    'Min_02_005',\n",
    "    'Min_2_02',\n",
    "]\n",
    "\n",
    "analyses1 = analyses_drop[final_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Columns with NaN values:\n",
      "sample_id                  19\n",
      "minerology_id            7847\n",
      "soil_biology_id          7847\n",
      "upper_limit                 7\n",
      "lower_limit                 7\n",
      "EG                       4910\n",
      "thick_sand                448\n",
      "fine_sand                 390\n",
      "silt                      430\n",
      "clay                      420\n",
      "Eq_Hum                   2297\n",
      "atm_1_3                  7469\n",
      "atm_15                   5689\n",
      "CACO3                    7334\n",
      "gypsum                   7815\n",
      "free_iron                2792\n",
      "organic_carbon           2395\n",
      "total_N                  6259\n",
      "P205                     5464\n",
      "organic_material         2413\n",
      "pH_H2O                    684\n",
      "pH_KCL                   1749\n",
      "Ca                       3018\n",
      "Mg                       3025\n",
      "Na                       3024\n",
      "K                        2995\n",
      "exchangable_bases_sum    6740\n",
      "CEC                      2423\n",
      "V                        2476\n",
      "conductivity             7201\n",
      "soluble_sodium           1270\n",
      "Min_lt_0002              7517\n",
      "Min_005_002              7719\n",
      "Min_02_005               7723\n",
      "Min_2_02                 7810\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#address nulls \n",
    "# Show count of NaN values in each column\n",
    "nan_counts = analyses1.isna().sum()\n",
    "\n",
    "# Filter to show only columns with at least one NaN\n",
    "nan_columns = nan_counts[nan_counts > 0]\n",
    "\n",
    "# Print them\n",
    "print(\"üîç Columns with NaN values:\")\n",
    "print(nan_columns)\n",
    "\n",
    "#formatting and checking for duplicates\n",
    "# Make a safe copy\n",
    "analyses1 = analyses_drop[final_columns].copy()\n",
    "\n",
    "# Convert sample_id to numeric, coercing errors to NaN\n",
    "analyses1['sample_id'] = pd.to_numeric(analyses1['sample_id'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN in sample_id to avoid int conversion error\n",
    "analyses1 = analyses1.dropna(subset=['sample_id'])\n",
    "\n",
    "# Convert to int, then to str\n",
    "analyses1['sample_id'] = analyses1['sample_id'].astype(int).astype(str)\n",
    "\n",
    "# Clean the profile column\n",
    "analyses1['profile'] = (\n",
    "    analyses1['profile']\n",
    "    .astype(str)\n",
    "    .str.replace('/', '_')\n",
    "    .str.strip()\n",
    "    .str[:20]\n",
    ")\n",
    "\n",
    "# Alias to another variable\n",
    "analyses2 = analyses1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10337' '3498' '2923' '2924' '2925' '2926' '2927' '2928' '2929' '2930'\n",
      " '16355' '16356' '419' '420' '15682' '13817' '75' '13965' '3418' '9230'\n",
      " '8639' '3529']\n",
      "‚úÖ Sample ID corrections applied.\n"
     ]
    }
   ],
   "source": [
    "# Filter out and print duplicated samples (including the first occurrence)\n",
    "duplicated_values = analyses2['sample_id'][analyses2['sample_id'].duplicated()].unique()\n",
    "print(duplicated_values)\n",
    "\n",
    "# 1. Ensure sample_id and analysis_id are strings\n",
    "analyses2['sample_id'] = analyses2['sample_id'].astype(str).str.strip()\n",
    "analyses2['analysis_id'] = analyses2['analysis_id'].astype(str).str.strip()\n",
    "\n",
    "# 2. Correct duplicates for sample_id '11608'\n",
    "mask_11608 = analyses2['sample_id'] == '11608'\n",
    "analyses2.loc[mask_11608, 'sample_id'] = [\n",
    "    f\"11608_{i+1}\" for i in range(mask_11608.sum())\n",
    "]\n",
    "\n",
    "# 3. Remove duplicate for sample_id '3497' (keep first occurrence only)\n",
    "duplicates_3497 = analyses2[analyses2['sample_id'] == '3497']\n",
    "if len(duplicates_3497) > 1:\n",
    "    indices_to_drop = duplicates_3497.index[1:]\n",
    "    analyses2 = analyses2.drop(indices_to_drop)\n",
    "\n",
    "# 4. Correct sample_id ‚Üí analysis_id assignments for specific sample_ids\n",
    "id_map = {\n",
    "    'UZ_24c/60_1_1': '8639',\n",
    "    'UZ_24c/60_2_1': '8640',\n",
    "    'UZ_24c/60_3_1': '8641',\n",
    "    'UZ_24c/60_4_1': '8642',\n",
    "    'UZ_24c/60_5_1': '8643'\n",
    "}\n",
    "\n",
    "# Apply mapping to update analysis_id based on sample_id\n",
    "analyses2['analysis_id'] = analyses2.apply(\n",
    "    lambda row: id_map.get(row['sample_id'], row['analysis_id']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Sample ID corrections applied.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóë Dropped second occurrence of sample_id 2923\n",
      "üóë Dropped second occurrence of sample_id 2924\n",
      "üóë Dropped second occurrence of sample_id 2925\n",
      "üóë Dropped second occurrence of sample_id 2926\n",
      "üóë Dropped second occurrence of sample_id 2927\n",
      "üóë Dropped second occurrence of sample_id 2928\n",
      "üóë Dropped second occurrence of sample_id 2929\n",
      "üóë Dropped second occurrence of sample_id 2930\n"
     ]
    }
   ],
   "source": [
    "#drop unwanted/un-needed duplicates or erroneous reccords\n",
    "\n",
    "# Drop second row where sample_id == '3497'\n",
    "indices = analyses2.index[analyses2['sample_id'] == '3497'].tolist()\n",
    "if len(indices) > 1:\n",
    "    analyses2 = analyses2.drop(indices[1])\n",
    "\n",
    "# Drop second row where sample_id == '3497'\n",
    "indices = analyses2.index[analyses2['sample_id'] == '13817'].tolist()\n",
    "if len(indices) > 1:\n",
    "    analyses2 = analyses2.drop(indices[1])\n",
    "\n",
    "# drop second row where sample_id == '8639'\n",
    "indices = analyses2.index[analyses2['sample_id'] == '8639'].tolist()\n",
    "if len(indices) > 1:\n",
    "    analyses2 = analyses2.drop(indices[1])\n",
    "\n",
    "#Drop second row where sample_id =='2923-2930'\n",
    "# Ensure sample_id is string for consistent comparison\n",
    "analyses2['sample_id'] = analyses2['sample_id'].astype(str)\n",
    "\n",
    "# Define the sample_ids to clean duplicates for\n",
    "target_ids = [str(i) for i in range(2923, 2931)]\n",
    "\n",
    "# Identify and drop the second occurrence of each\n",
    "for sid in target_ids:\n",
    "    matches = analyses2.index[analyses2['sample_id'] == sid].tolist()\n",
    "    if len(matches) > 1:\n",
    "        # Drop the second occurrence (index 1 in the list)\n",
    "        analyses2 = analyses2.drop(matches[1])\n",
    "        print(f\"üóë Dropped second occurrence of sample_id {sid}\")\n",
    "\n",
    "\n",
    "# Ensure sample_id is string for correct matching\n",
    "analyses2['sample_id'] = analyses2['sample_id'].astype(str)\n",
    "\n",
    "# List of sample_ids to drop (as strings)\n",
    "sample_ids_to_drop = [\n",
    "    '4247', '4248', '4249', '4250', '4251', '4252', '4253', '4254',  # invalid samples\n",
    "    '5533', '6636', '6637', '6638', '6639',\n",
    "    '6875', '7361',\n",
    "    '3012', '3013', '3014', '3015', '3016',\n",
    "    '3049', '3050', '3051', '3052', '3053', '3054', '419', '420', '421', '422'\n",
    "]\n",
    "\n",
    "# Drop rows where sample_id is in the list\n",
    "analyses2 = analyses2[~analyses2['sample_id'].isin(sample_ids_to_drop)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Manual sample_id corrections applied.\n",
      "Remaining rows: 7798\n",
      "Missing sample_id entries: 0\n"
     ]
    }
   ],
   "source": [
    "# Ensure both 'lab_sample_id' and 'sample_id' columns exist\n",
    "assert 'lab_sample_id' in analyses2.columns, \"Missing column: lab_sample_id\"\n",
    "assert 'sample_id' in analyses2.columns, \"Missing column: sample_id\"\n",
    "\n",
    "# Step 1: Ensure lab_sample_id is string so comparison works\n",
    "analyses2['lab_sample_id'] = analyses2['lab_sample_id'].astype(str)\n",
    "\n",
    "# Step 2: Manual corrections mapping\n",
    "manual_corrections = {\n",
    "    '66': 10377,\n",
    "    '4219': 3497,\n",
    "    '4221': 3498,   # same sample, different depths\n",
    "    '4222': 3499,\n",
    "    '4747': 16255,\n",
    "    '4748': 16256,\n",
    "    '5010': 15862,\n",
    "    '6802': 13695,\n",
    "    '7255': 8230,\n",
    "    '7686': 8529,\n",
    "    '6875': 3419,\n",
    "    '6638': 75_1,\n",
    "}\n",
    "\n",
    "# Step 3: Apply manual corrections\n",
    "for lab_id, sample_id in manual_corrections.items():\n",
    "    matches = analyses2['lab_sample_id'] == lab_id\n",
    "    if matches.sum() == 0:\n",
    "        print(f\"‚ö†Ô∏è Warning: lab_sample_id {lab_id} not found in data.\")\n",
    "    else:\n",
    "        analyses2.loc[matches, 'sample_id'] = sample_id\n",
    "\n",
    "# Step 4: Summary after correction\n",
    "print(\"‚úÖ Manual sample_id corrections applied.\")\n",
    "print(f\"Remaining rows: {len(analyses2)}\")\n",
    "print(f\"Missing sample_id entries: {analyses2['sample_id'].isna().sum()}\")\n",
    "\n",
    "# Optional: Show which rows are still missing sample_id\n",
    "missing_sample_ids = analyses2[analyses2['sample_id'].isna()]\n",
    "if not missing_sample_ids.empty:\n",
    "    print(\"üîç Rows with missing sample_id after correction:\")\n",
    "    print(missing_sample_ids[['lab_sample_id', 'sample_id']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Missing sample_id in 0 rows.\n",
      "üîç lab_sample_id values with missing sample_id:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Find rows with missing sample_id\n",
    "missing_sample_id_rows = analyses2[analyses2['sample_id'].isna()]\n",
    "\n",
    "# Print how many are missing\n",
    "print(f\"‚ùå Missing sample_id in {len(missing_sample_id_rows)} rows.\")\n",
    "\n",
    "# Show the unique lab_sample_id values that are missing a sample_id\n",
    "print(\"üîç lab_sample_id values with missing sample_id:\")\n",
    "print(missing_sample_id_rows['lab_sample_id'].unique())\n",
    "\n",
    "# Optional: Save to CSV for review\n",
    "#missing_sample_id_rows.to_csv(\"missing_sample_ids.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing sample_id (NaN only)\n",
    "analyses2 = analyses2[analyses2['sample_id'].notna()].copy()\n",
    "\n",
    "analyses3 = analyses2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Filter out and print duplicated samples (including the first occurrence)\n",
    "duplicated_values = analyses3['sample_id'][analyses3['sample_id'].duplicated()].unique()\n",
    "print(duplicated_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## joining elemental analyses to analyses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Found 0 rows with NaN sample_id:\n",
      "Empty DataFrame\n",
      "Columns: [sample_id, Soil Profile, Code, Code.1, Depht, Sampling Date, Mg, Al, Si, P, S, Cl, K, Ca, Ti, V, Cr, Mn, Fe, Co, Ni, Cu, Zn, As, Se, Rb, Sr, Zr, Nb, Mo, Cd, Sn, Sb, Ba, Ta, W, Pt, Au, Hg, Tl, Pb, Bi, Th, U]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "# Rename and clean sample_id as before\n",
    "elemental_analyses.rename(columns={'Lab. Code': 'sample_id'}, inplace=True)\n",
    "\n",
    "# Convert to string to clean text\n",
    "sample_id_cleaned = elemental_analyses['sample_id'].astype(str)\n",
    "sample_id_cleaned = sample_id_cleaned.str.replace('C-', '', regex=False)\n",
    "sample_id_cleaned = sample_id_cleaned.str.replace(' MNL', '', regex=False)\n",
    "\n",
    "# Convert to numeric with coercion (NaNs will appear here)\n",
    "elemental_analyses['sample_id'] = pd.to_numeric(sample_id_cleaned, errors='coerce')\n",
    "\n",
    "# üîç Show rows where sample_id is NaN\n",
    "nan_sample_ids = elemental_analyses[elemental_analyses['sample_id'].isna()]\n",
    "print(f\"‚ùå Found {len(nan_sample_ids)} rows with NaN sample_id:\")\n",
    "print(nan_sample_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'Lab. Code' to 'sample_id'\n",
    "elemental_analyses.rename(columns={'Lab. Code': 'sample_id'}, inplace=True)\n",
    "\n",
    "# Temporarily convert to string to do the replacements\n",
    "sample_id_cleaned = elemental_analyses['sample_id'].astype(str)\n",
    "sample_id_cleaned = sample_id_cleaned.str.replace('C-', '', regex=False)\n",
    "sample_id_cleaned = sample_id_cleaned.str.replace(' MNL', '', regex=False)\n",
    "\n",
    "# Convert cleaned values to numeric safely\n",
    "elemental_analyses['sample_id'] = pd.to_numeric(sample_id_cleaned, errors='coerce')\n",
    "\n",
    "\n",
    "# Convert to integer and then string for uniformity\n",
    "elemental_analyses['sample_id'] = elemental_analyses['sample_id'].astype(int).astype(str)\n",
    "\n",
    "# Rename 'Code' to 'field_sample_code'\n",
    "elemental_analyses.rename(columns={'Code': 'field_sample_code'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert sample_id to string (no truncation unless necessary)\n",
    "elemental_analyses['sample_id'] = elemental_analyses['sample_id'].astype(str).str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "print (analyses3['sample_id'].dtype)\n",
    "print (elemental_analyses['sample_id'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample IDs in elemental_analyses not found in analyses2:\n",
      "['16105' '16217' '16033' '16231' '16208' '16274' '16225' '15731' '16078'\n",
      " '15786' '15693' '15678' '15959' '16372' '16418' '16459' '16477' '15498'\n",
      " '15437' '12749' '13109' '14337' '14319' '15484' '15508' '15614' '15580'\n",
      " '15582' '15606' '15463' '14496' '11343' '17892' '16976' '17269' '18248'\n",
      " '16946' '17686' '17728' '17415' '17642' '18212' '17016' '17282' '18429'\n",
      " '17340' '18824' '18448' '8721' '7313' '5502' '5392' '7317' '8626' '7193'\n",
      " '7254' '7293' '8240' '15397' '14403' '14983' '16126' '15059' '15322'\n",
      " '15268' '14808' '15106']\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "# Check which sample_ids in elemental_analyses are NOT in analyses_ready\n",
    "missing_ids = elemental_analyses[~elemental_analyses['sample_id'].isin(analyses3['sample_id'])]\n",
    "\n",
    "# Display them\n",
    "print(\"Sample IDs in elemental_analyses not found in analyses2:\")\n",
    "print(missing_ids['sample_id'].unique())\n",
    "\n",
    "print(len(missing_ids['sample_id'].unique()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab_sample_id</th>\n",
       "      <th>analysis_id</th>\n",
       "      <th>morpho_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>minerology_id</th>\n",
       "      <th>soil_biology_id</th>\n",
       "      <th>upper_limit</th>\n",
       "      <th>lower_limit</th>\n",
       "      <th>EG</th>\n",
       "      <th>...</th>\n",
       "      <th>Ta</th>\n",
       "      <th>W</th>\n",
       "      <th>Pt</th>\n",
       "      <th>Au</th>\n",
       "      <th>Hg</th>\n",
       "      <th>Tl</th>\n",
       "      <th>Pb</th>\n",
       "      <th>Bi</th>\n",
       "      <th>Th</th>\n",
       "      <th>U</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>10999</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>11000</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>11.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>11001</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>28.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>11002</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>55.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B_101/62_5_21</td>\n",
       "      <td>B_101/62_5_2</td>\n",
       "      <td>11003</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>90.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  lab_sample_id    analysis_id     morpho_id sample_id profile minerology_id  \\\n",
       "0             1   B_101/62_1_1  B_101/62_1_1     10999  101_62          <NA>   \n",
       "1             2   B_101/62_2_1  B_101/62_2_1     11000  101_62          <NA>   \n",
       "2             3   B_101/62_3_1  B_101/62_3_1     11001  101_62          <NA>   \n",
       "3             4   B_101/62_4_1  B_101/62_4_1     11002  101_62          <NA>   \n",
       "4             5  B_101/62_5_21  B_101/62_5_2     11003  101_62          <NA>   \n",
       "\n",
       "  soil_biology_id  upper_limit  lower_limit  EG  ...  Ta   W  Pt  Au  Hg  Tl  \\\n",
       "0            <NA>          0.0         11.0 NaN  ... NaN NaN NaN NaN NaN NaN   \n",
       "1            <NA>         11.0         28.0 NaN  ... NaN NaN NaN NaN NaN NaN   \n",
       "2            <NA>         28.0         54.0 NaN  ... NaN NaN NaN NaN NaN NaN   \n",
       "3            <NA>         55.0         85.0 NaN  ... NaN NaN NaN NaN NaN NaN   \n",
       "4            <NA>         90.0        120.0 NaN  ... NaN NaN NaN NaN NaN NaN   \n",
       "\n",
       "   Pb  Bi  Th   U  \n",
       "0 NaN NaN NaN NaN  \n",
       "1 NaN NaN NaN NaN  \n",
       "2 NaN NaN NaN NaN  \n",
       "3 NaN NaN NaN NaN  \n",
       "4 NaN NaN NaN NaN  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LEFT JOIN ELEMENTAL ANALYSE TO ANALYSE TABLE\n",
    "\n",
    "merged1 = analyses3.merge(elemental_analyses, on='sample_id', how='left')\n",
    "merged1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     lab_sample_id    analysis_id      morpho_id sample_id profile  \\\n",
      "1361          1362  Cb_134/59_6_1  Cb_134/59_6_1      6940  134_59   \n",
      "\n",
      "     minerology_id soil_biology_id  upper_limit  lower_limit  EG  ...   K  \\\n",
      "1361          <NA>            <NA>        130.0        170.0 NaN  ... NaN   \n",
      "\n",
      "      exchangable_bases_sum  CEC   V  conductivity  soluble_sodium  \\\n",
      "1361                    NaN  NaN NaN           NaN             0.0   \n",
      "\n",
      "      Min_lt_0002  Min_005_002  Min_02_005  Min_2_02  \n",
      "1361          NaN          NaN         NaN       NaN  \n",
      "\n",
      "[1 rows x 39 columns]\n",
      "   sample_id Soil Profile field_sample_code Code.1      Depht Sampling Date  \\\n",
      "80      6940      P134/59          A-320/59  S-492  1.30-1.70    1954-08-06   \n",
      "\n",
      "             Mg             Al             Si      P  ...    Ta   W  Pt  Au  \\\n",
      "80  1457.666667  106266.333333  308859.666667  468.0  ...  26.0 NaN NaN NaN   \n",
      "\n",
      "           Hg   Tl         Pb  Bi         Th   U  \n",
      "80  10.666667  4.0  25.333333 NaN  12.333333 NaN  \n",
      "\n",
      "[1 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "print(analyses3[analyses3['sample_id'] == '6940'])\n",
    "print(elemental_analyses[elemental_analyses['sample_id'] == '6940'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of each column in merged1:\n",
      "\n",
      "lab_sample_id     object\n",
      "analysis_id       object\n",
      "morpho_id         object\n",
      "sample_id         object\n",
      "profile           object\n",
      "                  ...   \n",
      "Tl               float64\n",
      "Pb               float64\n",
      "Bi               float64\n",
      "Th               float64\n",
      "U                float64\n",
      "Length: 74, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Define final column list\n",
    "final_columns = [\n",
    "    'lab_sample_id', 'analysis_id', 'morpho_id','sample_id', 'profile', 'soil_biology_id', 'upper_limit', 'lower_limit', 'EG', 'thick_sand', 'fine_sand', 'silt', 'clay', 'Eq_Hum', 'atm_1/3', 'atm_15',\n",
    "    'CACO3', 'gypsum', 'free_iron', 'organic_carbon', 'total_N', 'P205', 'organic_material', 'pH_H2O', 'pH_KCL',\n",
    "    'Ca++', 'Mg++', 'Na+', 'K+', 'exchangable_bases_sum', 'CEC', 'V', 'conductivity', 'soluble_sodium', 'Min_<0,002',\n",
    "    'Min_0,05-0,02', 'Min_0,2-0,05', 'Min_2-0,2', \n",
    "    'field_sample_code', 'Depth', 'Al', 'Si', 'P', 'S', 'Cl', 'Ti', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn',\n",
    "    'As', 'Se', 'Rb', 'Sr', 'Zr', 'Nb', 'Mo', 'Cd', 'Sn', 'Sb', 'Ba', 'Ta', 'W', 'Pt', 'Au', 'Hg', 'Tl', 'Pb', 'Bi',\n",
    "    'Th', 'U',\n",
    "]\n",
    "\n",
    "# Ensure all columns exist in the DataFrame\n",
    "for col in final_columns:\n",
    "    if col not in merged1.columns:\n",
    "        merged1[col] = pd.NA  # or np.nan if preferred\n",
    "\n",
    "# Reorder DataFrame columns\n",
    "merged1 = merged1[final_columns]\n",
    "\n",
    "# Check and print the datatypes of each column\n",
    "print(\"Data types of each column in merged1:\\n\")\n",
    "print(merged1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab_sample_id     object\n",
      "analysis_id       object\n",
      "morpho_id         object\n",
      "sample_id         object\n",
      "profile           object\n",
      "                  ...   \n",
      "Tl               float64\n",
      "Pb               float64\n",
      "Bi               float64\n",
      "Th               float64\n",
      "U                float64\n",
      "Length: 74, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# CHECK/CHANGE DATATYPES\n",
    "\n",
    "# Change PK (sample_id) to a consistent datatype: Converted to string and truncate to 20 characters (similar to VARCHAR(20))\n",
    "merged1['sample_id'] = merged1['sample_id'].astype(str).str.strip().str[:20]\n",
    "merged1['lab_sample_id'] = merged1['lab_sample_id'].astype(str).str.strip().str[:20]\n",
    "\n",
    "print(merged1.dtypes)\n",
    "\n",
    "# CHECK AND/OR ADD FK\n",
    "# Check if 'lab_sample_id' exists and drop it\n",
    "if 'lab_sample_id' in merged1.columns:\n",
    "    merged1 = merged1.drop(columns=['lab_sample_id'])\n",
    "\n",
    "# Insert a new column, e.g., 'lab_sample_id' as a primary key starting from 1\n",
    "merged1.insert(0, 'lab_sample_id', range(1, len(merged1) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE TO CSV\n",
    "merged1.to_csv(\"/Users/inesschwartz/Desktop/analyses_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soil Type Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soil_type_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>CEP_GR</th>\n",
       "      <th>CEP_NAME</th>\n",
       "      <th>FAO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1_51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1_57</td>\n",
       "      <td>Aridicos</td>\n",
       "      <td>Aridicos com calcario Pardo-cinzentos</td>\n",
       "      <td>CLha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   soil_type_id profile    CEP_GR                               CEP_NAME   FAO\n",
       "0             1    1_51       NaN                                    NaN   NaN\n",
       "1             2    1_57  Aridicos  Aridicos com calcario Pardo-cinzentos  CLha"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns\n",
    "soil_type = soil_profile.copy()\n",
    "\n",
    "soil_type.rename(columns={\n",
    "    'Perfil': 'profile',\n",
    "    'Agrupamento': 'grouping',\n",
    "    'Pro': 'province',\n",
    "    'Pa√≠s': 'country',\n",
    "    'Local': 'location',\n",
    "    'DATA': 'date',\n",
    "    'CEP_NOME': 'CEP_NAME',\n",
    "}, inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "soil_type_cleaning = soil_type.drop(columns=[\n",
    "    'REF', 'province', 'country', 'location', 'DESCRITOR1', 'DESCRITOR2', 'DESCRITOR3',\n",
    "    'date', 'Fase', 'D_INSER√áAO', 'Publica√ß√£o', 'WRB_old', 'Miss√£o'\n",
    "], errors='ignore')  # use errors='ignore' in case some columns were already missing\n",
    "\n",
    "# Add a new Primary Key ID column starting from 1\n",
    "soil_type_cleaning.insert(0, 'soil_type_id', range(1, len(soil_type_cleaning) + 1))\n",
    "\n",
    "# Drop accents\n",
    "import unicodedata\n",
    "\n",
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        return ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    return text\n",
    "\n",
    "# Apply to all cells in the DataFrame\n",
    "soil_type_cleaning = soil_type_cleaning.applymap(remove_accents)\n",
    "\n",
    "# Replace / with _ and strip/shorten 'profile' BEFORE slicing into site_info_clean\n",
    "soil_type_cleaning['profile'] = (\n",
    "    soil_type_cleaning['profile']\n",
    "    .astype(str)\n",
    "    .str.replace('/', '_')\n",
    "    .str.strip()\n",
    "    .str[:20]\n",
    ")\n",
    "\n",
    "# Keep only relevant columns\n",
    "soil_type_clean = soil_type_cleaning[[\n",
    "    'soil_type_id',\n",
    "    'profile',\n",
    "    'CEP_GR',\n",
    "    'CEP_NAME',\n",
    "    'FAO'\n",
    "]]\n",
    "\n",
    "# Preview\n",
    "soil_type_clean.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "üîç Unique 'profile' values equal to string 'NULL': []\n",
      "üî¢ Number of 'profile' values equal to string 'NULL': 0\n"
     ]
    }
   ],
   "source": [
    "# check duplicates\n",
    "# Filter out and print duplicated samples (including the first occurrence)\n",
    "duplicated_values_st = soil_type_clean['profile'][soil_type_clean['profile'].duplicated()].unique()\n",
    "print(duplicated_values_st)\n",
    "\n",
    "# Check for profile values that are the string \"NULL\"\n",
    "null_string_values_st = soil_type_clean[soil_type_clean['profile'] == \"NULL\"]['profile'].unique()\n",
    "print(\"üîç Unique 'profile' values equal to string 'NULL':\", null_string_values_st)\n",
    "\n",
    "null_string_count_st = (soil_type_clean['profile'] == \"NULL\").sum()\n",
    "print(f\"üî¢ Number of 'profile' values equal to string 'NULL': {null_string_count_st}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clean table\n",
    "soil_type_clean.to_csv(\"/Users/inesschwartz/Desktop/soil_type_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>climate_id</th>\n",
       "      <th>ID</th>\n",
       "      <th>mean_annual_temp</th>\n",
       "      <th>mean_annual_precip</th>\n",
       "      <th>koppen_climate</th>\n",
       "      <th>thornthwaite_climate</th>\n",
       "      <th>hydric_regime</th>\n",
       "      <th>thermal_regime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2770</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arido</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Humido</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1618</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>881</td>\n",
       "      <td>21.5</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>Tropical chuvoso com estacao seca no Inverno, ...</td>\n",
       "      <td>Humido</td>\n",
       "      <td>Tropustico udico</td>\n",
       "      <td>Iso-Hipertermico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1750</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   climate_id    ID mean_annual_temp mean_annual_precip  \\\n",
       "0           1  2770                                       \n",
       "1           2    48                                       \n",
       "2           3  1618                                       \n",
       "3           4   881             21.5             1500.0   \n",
       "4           5  1750                                       \n",
       "\n",
       "                                      koppen_climate thornthwaite_climate  \\\n",
       "0                                                NaN                Arido   \n",
       "1                                                NaN               Humido   \n",
       "2                                                NaN                  NaN   \n",
       "3  Tropical chuvoso com estacao seca no Inverno, ...               Humido   \n",
       "4                                                NaN                  NaN   \n",
       "\n",
       "      hydric_regime    thermal_regime  \n",
       "0               NaN               NaN  \n",
       "1               NaN               NaN  \n",
       "2               NaN               NaN  \n",
       "3  Tropustico udico  Iso-Hipertermico  \n",
       "4               NaN               NaN  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load original Excel data\n",
    "profile_loc = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Perfis_local.xlsx\")\n",
    "\n",
    "# Create a copy for cleaning\n",
    "climate_features_cleaning = profile_loc.copy()\n",
    "\n",
    "# Mapping from climate codes to descriptions\n",
    "code_to_description = {\n",
    "    \"B1\": \"H√∫mido\",\n",
    "    \"B2\": \"H√∫mido\",\n",
    "    \"B3\": \"H√∫mido\",\n",
    "    \"B4\": \"H√∫mido\",\n",
    "    \"C1\": \"Sub-h√∫mido seco\",\n",
    "    \"C2\": \"Sub-h√∫mido chuvoso\",\n",
    "    \"D\": \"Semi-√°rido\",\n",
    "    \"E\": \"√Årido\",\n",
    "    \"Aw\": \"Tropical chuvoso com esta√ß√£o seca no Inverno, de savana\",\n",
    "    \"BSw\": \"Seco de estepe, com chuva predominante no Ver√£o\",\n",
    "    \"BWw\": \"Seco de deserto, com chuva predominante no Ver√£o\",\n",
    "    \"Cw\": \"Mesot√©rmico h√∫mido com esta√ß√£o seca no Inverno\",\n",
    "    \"ARe\": \"Ar√≠dico extremo\",\n",
    "    \"ARf\": \"Ar√≠dico fraco\",\n",
    "    \"ARt\": \"Ar√≠dico t√≠pico\",\n",
    "    \"tUDs\": \"Temp√∫dico seco\",\n",
    "    \"tUSh\": \"Temp√∫stico h√∫mido\",\n",
    "    \"tUSt\": \"Temp√∫stico t√≠pico\",\n",
    "    \"TUDs\": \"Trop√∫dico seco\",\n",
    "    \"TUSa\": \"Trop√∫stico ar√≠dico\",\n",
    "    \"TUSu\": \"Trop√∫stico √∫dico\",\n",
    "    \"TUSt\": \"Tropustico t√≠pico\",\n",
    "    \"H\": \"Hipert√©rmico\",\n",
    "    \"iH\": \"Iso-Hipert√©rmico\",\n",
    "    \"iT\": \"Iso-T√©rmico\",\n",
    "    \"T\": \"T√©rmico\"\n",
    "}\n",
    "\n",
    "# Replace climate codes with descriptions\n",
    "for col in [\"CL_THORNTH\", \"CL_KOPPEN\", \"REG_H√çDRIC\", \"REG_T√âRMIC\"]:\n",
    "    climate_features_cleaning[col] = climate_features_cleaning[col].replace(code_to_description)\n",
    "\n",
    "# Function to average values like \"21-22\" -> 21.5\n",
    "def average_range(value):\n",
    "    if isinstance(value, str) and '-' in value:\n",
    "        try:\n",
    "            nums = [float(x.strip()) for x in value.split('-')]\n",
    "            return sum(nums) / len(nums)\n",
    "        except:\n",
    "            return None\n",
    "    try:\n",
    "        return float(value)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply to temperature and precipitation columns\n",
    "climate_features_cleaning[\"TMA\"] = climate_features_cleaning[\"TMA\"].apply(average_range)\n",
    "climate_features_cleaning[\"PMA\"] = climate_features_cleaning[\"PMA\"].apply(average_range)\n",
    "\n",
    "# Rename columns for clarity\n",
    "climate_features_cleaning.rename(columns={\n",
    "    'PERFIL': 'profile',\n",
    "    'CL_THORNTH': 'thornthwaite_climate',\n",
    "    'CL_KOPPEN': 'koppen_climate',\n",
    "    'TMA': 'mean_annual_temp',\n",
    "    'PMA': 'mean_annual_precip',\n",
    "    'REG_H√çDRIC': 'hydric_regime',\n",
    "    'REG_T√âRMIC': 'thermal_regime'\n",
    "}, inplace=True)\n",
    "\n",
    "# ensure consistent profile formatting\n",
    "climate_features_cleaning['profile'] = climate_features_cleaning['profile'].astype(str).str.replace('/', '_').str.strip().str[:20]\n",
    "\n",
    "# Drop accents from text values\n",
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        return ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    return text\n",
    "\n",
    "climate_features_cleaning = climate_features_cleaning.applymap(remove_accents)\n",
    "\n",
    "# Add primary key column\n",
    "climate_features_cleaning.insert(0, 'climate_id', range(1, len(climate_features_cleaning) + 1))\n",
    "\n",
    "# Replace empty strings or nulls in numeric columns with \\N (Postgres null)\n",
    "for col in ['mean_annual_temp', 'mean_annual_precip']:\n",
    "    climate_features_cleaning[col] = climate_features_cleaning[col].replace(\n",
    "        ['', ' ', 'NULL', None, pd.NA, pd.NaT, 'nan', float('nan')], ''\n",
    "    )\n",
    "\n",
    "# Select and reorder relevant columns for export\n",
    "climate_features_clean = climate_features_cleaning[[\n",
    "    'climate_id',\n",
    "    #'profile',\n",
    "    'ID',\n",
    "    'mean_annual_temp',\n",
    "    'mean_annual_precip',\n",
    "    'koppen_climate',\n",
    "    'thornthwaite_climate',\n",
    "    'hydric_regime',\n",
    "    'thermal_regime'\n",
    "]]\n",
    "\n",
    "# Preview\n",
    "climate_features_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check FK relationships and datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of each column in climate_clean:\n",
      "\n",
      "climate_id               int64\n",
      "ID                       int64\n",
      "mean_annual_temp        object\n",
      "mean_annual_precip      object\n",
      "koppen_climate          object\n",
      "thornthwaite_climate    object\n",
      "hydric_regime           object\n",
      "thermal_regime          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check and print the datatypes of each column\n",
    "print(\"Data types of each column in climate_clean:\\n\")\n",
    "print(climate_features_clean.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clean csv table\n",
    "climate_features_clean.to_csv(\"/Users/inesschwartz/Desktop/climate_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topo features table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topo_id</th>\n",
       "      <th>ID</th>\n",
       "      <th>slope_code</th>\n",
       "      <th>altitude</th>\n",
       "      <th>aspect</th>\n",
       "      <th>land_surface_temp</th>\n",
       "      <th>dem_elevation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>D5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>881</td>\n",
       "      <td>D1</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topo_id    ID slope_code  altitude aspect land_surface_temp dem_elevation\n",
       "0        1  2770        NaN      32.0   <NA>              <NA>          <NA>\n",
       "1        2    48         D5       NaN   <NA>              <NA>          <NA>\n",
       "2        3  1618        NaN       NaN   <NA>              <NA>          <NA>\n",
       "3        4   881         D1    1210.0   <NA>              <NA>          <NA>\n",
       "4        5  1750        NaN       NaN   <NA>              <NA>          <NA>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a copy for cleaning\n",
    "topo_features_cleaning = profile_loc.copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "topo_features_cleaning.rename(columns={\n",
    "    'PERFIL': 'profile',\n",
    "    'TOPOGRAFIA': 'slope_code',\n",
    "    'ALTITUDE': 'altitude',\n",
    "}, inplace=True)\n",
    "\n",
    "# ensure consistent profile formatting\n",
    "topo_features_cleaning['profile'] = topo_features_cleaning['profile'].astype(str).str.replace('/', '_').str.strip().str[:20]\n",
    "\n",
    "# Add missing columns\n",
    "topo_features_cleaning['aspect'] = pd.NA  \n",
    "topo_features_cleaning['land_surface_temp'] = pd.NA\n",
    "topo_features_cleaning['dem_elevation'] = pd.NA  \n",
    "\n",
    "# Add primary key column\n",
    "topo_features_cleaning.insert(0, 'topo_id', range(1, len(topo_features_cleaning) + 1))\n",
    "\n",
    "# Create slope class mapping dictionary\n",
    "slope_code_to_description = {\n",
    "    \"D1\": \"Plano (Declives < 2%)\",\n",
    "    \"D2\": \"Ondulado muito suave (Declives > 2% e < 3%)\",\n",
    "    \"D3\": \"Ondulado suave (Declives > 3% e < 5%)\",\n",
    "    \"D4\": \"Ondulado (Declives > 5% e < 8%)\",\n",
    "    \"D5\": \"Acidentado (Declives > 8% e < 15%)\",\n",
    "    \"D6\": \"Escarpado (Declives >15% e < 30%)\",\n",
    "    \"D7\": \"Montanhoso (Declives > 30%)\"\n",
    "}\n",
    "\n",
    "# Create a mapping DataFrame for slope classes\n",
    "slope_classes_df = pd.DataFrame([\n",
    "    {\"slope_code\": code, \"slope_description\": desc}\n",
    "    for code, desc in slope_code_to_description.items()\n",
    "])\n",
    "\n",
    "#drop accents\n",
    "import unicodedata\n",
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        # Normalize and remove diacritics\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "# Apply to all cells in the DataFrame\n",
    "topo_features_cleaning = topo_features_cleaning.applymap(remove_accents)\n",
    "\n",
    "# Final cleaned topo features table (referencing slope_code, not description)\n",
    "topo_features_clean = topo_features_cleaning[[\n",
    "    'topo_id',\n",
    "    'ID',\n",
    "    #'profile', \n",
    "    'slope_code',\n",
    "    'altitude',\n",
    "    'aspect',\n",
    "    'land_surface_temp',\n",
    "    'dem_elevation'\n",
    "]]\n",
    "\n",
    "# Preview\n",
    "topo_features_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clean csv table\n",
    "topo_features_clean.to_csv(\"/Users/inesschwartz/Desktop/topo_features_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geological features table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_features_id</th>\n",
       "      <th>ID</th>\n",
       "      <th>geology_id</th>\n",
       "      <th>lithology_id</th>\n",
       "      <th>lithology_1954_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>Oendolongo</td>\n",
       "      <td>pp</td>\n",
       "      <td>Sistema do Maiombe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>881</td>\n",
       "      <td>Karroo</td>\n",
       "      <td>Cs/Cal</td>\n",
       "      <td>Serie de Cassanje - T2'T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   geo_features_id    ID  geology_id lithology_id          lithology_1954_id\n",
       "0                1  2770         NaN            d                        NaN\n",
       "1                2    48  Oendolongo           pp         Sistema do Maiombe\n",
       "2                3  1618         NaN          NaN                        NaN\n",
       "3                4   881      Karroo       Cs/Cal  Serie de Cassanje - T2'T1\n",
       "4                5  1750         NaN          NaN                        NaN"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load cleaned geo features data\n",
    "geo_features_cleaning = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Perfis_local.xlsx\")\n",
    "\n",
    "# Rename columns\n",
    "geo_features_cleaning.rename(columns={\n",
    "    'PERFIL': 'profile',\n",
    "    'GEOLOGIA': 'geology_id',\n",
    "    'LITOLOGIA': 'lithology_id',\n",
    "    'LITOLOGIA_1954': 'lithology_1954_id',\n",
    "}, inplace=True)\n",
    "\n",
    "# Add primary key column\n",
    "geo_features_cleaning.insert(0, 'geo_features_id', range(1, len(geo_features_cleaning) + 1))\n",
    "\n",
    "# Final normalized geo_features table (with codes as foreign keys)\n",
    "geo_features_clean = geo_features_cleaning[[\n",
    "    'geo_features_id',\n",
    "    #'profile',\n",
    "    'ID',\n",
    "    'geology_id',\n",
    "    'lithology_id',\n",
    "    'lithology_1954_id'\n",
    "]]\n",
    "\n",
    "# ensure consistent profile formatting\n",
    "#geo_features_clean['profile'] = geo_features_clean['profile'].astype(str).str.replace('/', '_').str.strip().str[:20]\n",
    "\n",
    "#drop accents\n",
    "import unicodedata\n",
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        # Normalize and remove diacritics\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "# Apply to all cells in the DataFrame\n",
    "geo_features_clean = geo_features_clean.applymap(remove_accents)\n",
    "\n",
    "#preview\n",
    "geo_features_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clean csv table\n",
    "geo_features_clean.to_csv(\"/Users/inesschwartz/Desktop/geo_features_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mapping tables\n",
    "# Mappings for geology\n",
    "geology_mapping = {\n",
    "    \"Kalahari\": \"Sistema do Kalahari\",\n",
    "    \"Superficiais\": \"Forma√ß√µes Superficiais\",\n",
    "    \"Karroo\": \"Sistema do Karroo\",\n",
    "    \"Bembe\": \"Sistema do Bembe\",\n",
    "    \"Oendolongo\": \"Sistema do Oendolongo\",\n",
    "    \"Base\": \"Complexo de base\",\n",
    "    \"Proteroz√≥ico\": \"Proteroz√≥ico\",\n",
    "    \"Pleistoc√©nico\": \"Pleistoc√©nico\",\n",
    "    \"Terci√°rio\": \"Terci√°rio (m√©dio e inferior)\",\n",
    "    \"TQ\": \"Quatern√°rio e Terci√°rio superior\",\n",
    "    \"Cret√°cio\": \"\",  # no description provided\n",
    "    \"RPKS\": \"Recente Plistoc√©nico e Kalahari Superior\"\n",
    "}\n",
    "\n",
    "# Mappings for lithology_1954\n",
    "lithology_1954_mapping = {\n",
    "    \"Œ≥\": \"Granitos, Granodioritos e Quartzodioritos\",\n",
    "    \"PL\": \"Xistos, metaquartzitos, conglomerados, arcoses, ect.\",\n",
    "    \"Œª\": \"Rochas eruptivas indeterminadas\",\n",
    "    \"Œ¥p\": \"Doleritos, doleritos pigeon√≠ticos\",\n",
    "    \"Œ¥ab\": \"Diabases, diabases albito-cloriticas\",\n",
    "    \"Œµ\": \"Noritos, gabros e peridotitos\",\n",
    "    \"JK\": \"Composto de conglomerados, areias, cascalhos do Kalahari\",\n",
    "    \"C\": \"S√©rie Xisto - calc√°ria\",\n",
    "    \"Cal\": \"Sedimentos arenosos n√£o consolidados\",\n",
    "    \"K\": \"S√©rie xisto - gresosa\",\n",
    "    \"RT\": \"N√£o diferenciado\",\n",
    "    \"œÉ\": \"Sienitos, sienitos nefel√≠nicos\",\n",
    "    \"Q\": \"Dep√≥sitos fossil√≠feros\",\n",
    "    \"CS\": \"Grande conglomerado e s√©rie de Mwashya\"\n",
    "}\n",
    "\n",
    "# Mappings for lithology\n",
    "lithology_mapping = {\n",
    "    \"a\": \"Rochas aren√°ceas consolidadas\",\n",
    "    \"aq\": \"Gr√©s quartz√≠ticos do Oendolongo\",\n",
    "    \"b\": \"Rochas eruptivas b√°sicas\",\n",
    "    \"c\": \"Rochas sedimentares consolidadas calc√°rias\",\n",
    "    \"c'\": \"Rochas sedimentares n√£o consolidadas calc√°rias\",\n",
    "    \"cg\": \"Rochas cristalof√≠licas argil√°ceas\",\n",
    "    \"d\": \"Sedimentos n√£o consolidados de origem marinha\",\n",
    "    \"dc\": \"Dep√≥sitos coluvionares\",\n",
    "    \"dr\": \"Dior√≠tos\",\n",
    "    \"e\": \"Rochas sedimentares consolidadas n√£o calc√°rias\",\n",
    "    \"g\": \"Rochas argil√°ceas consolidadas n√£o calc√°rias\",\n",
    "    \"g'\": \"Rochas argil√°ceas n√£o consolidadas n√£o calc√°rias\",\n",
    "    \"g''\": \"Rochas cristalinas pouco micas em quartzo\",\n",
    "    \"gp\": \"Rochas do  complexo gabro-plagioclast√≠co\",\n",
    "    \"k\": \"Sedimentos n√£o consolidados grosseiros do Kalahari\",\n",
    "    \"m\": \"Rochas sedimentares n√£o consolidadas calco-gips√≠feras\",\n",
    "    \"m'\": \"Dep√≥sitos coluvionares margosos\",\n",
    "    \"mm\": \"Materiais mistos\",\n",
    "    \"n\": \"Sedimentos n√£o consolidados de origem continental\",\n",
    "    \"nd\": \"n√£o descrito\",\n",
    "    \"pp\": \"Sedimentos n√£o consolidados grosseiros plio-plistoc√©nicos\",\n",
    "    \"q\": \"Rochas cristalinas quartz√≠feras\",\n",
    "    \"q'\": \"Materiais redistribu√≠dos provenientes de desagrega√ß√£o rochas crist. quartz√≠feras\",\n",
    "    \"qf\": \"Quartzitos ferruginosos do Oendolongo\",\n",
    "    \"r\": \"Sedimentos grosseiros n√£o especificados\",\n",
    "    \"s\": \"Sienitos\",\n",
    "    \"sx\": \"Forma√ß√µes (ou rochas) sedimentares n√£o especificadas\",\n",
    "    \"sx1\": \"Rochas sedimentares consolidadas com e sem calc√°rio\",\n",
    "    \"sx2\": \"Rochas sedimentares consolidadas\",\n",
    "    \"v\": \"Materiais vulc√¢nicos\",\n",
    "    \"v'\": \"Rochas do complexo alcalino e/ou carboat√≠tico\",\n",
    "    \"x\": \"Rochas consolidadas n√£o especificadas\",\n",
    "    \"xm\": \"Xistos metam√≥rficos\",\n",
    "    \"xq\": \"Rochas cristalinas n√£o especificadas\",\n",
    "    \"z\": \"Rochas metassedimentares\"\n",
    "}\n",
    "\n",
    "# Save each mapping as a DataFrame\n",
    "pd.DataFrame([\n",
    "    {\"geology_code\": k, \"geology_description\": v}\n",
    "    for k, v in geology_mapping.items()\n",
    "]).to_csv(\"/Users/inesschwartz/Desktop/geology_mapping.csv\", index=False)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"lithology_code\": k, \"lithology_description\": v}\n",
    "    for k, v in lithology_mapping.items()\n",
    "]).to_csv(\"/Users/inesschwartz/Desktop/lithology_mapping.csv\", index=False)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"lithology_1954_code\": k, \"lithology_1954_description\": v}\n",
    "    for k, v in lithology_1954_mapping.items()\n",
    "]).to_csv(\"/Users/inesschwartz/Desktop/lithology1954_mapping.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minerology info table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biology Info table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
