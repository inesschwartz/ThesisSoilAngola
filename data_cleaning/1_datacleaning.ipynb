{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.parser import parse\n",
    "\n",
    "#import and read data\n",
    "samples = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/AmostrasAngolaTerrario.xlsx\")\n",
    "analyses = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Horizontes Analises.xlsx\")\n",
    "morphology = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Horizontes_Morfologia.xlsx\")\n",
    "profile_loc = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Perfis_local.xlsx\")\n",
    "soil_profile = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Perfis_solo.xlsx\")\n",
    "elemental_analyses = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Data XRF Angola_inicial.xlsx\")\n",
    "#soil_type = pd.read_excel(\"/Users/inesschwartz/Desktop/Thesis/tables_soil_database/Perfis_solo.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def analyze_profile_matching():\n",
    "#     \"\"\"\n",
    "#     Analyze profile matching across all soil database tables\n",
    "#     \"\"\"\n",
    "    \n",
    "#     print(\"üîç PROFILE MATCHING ANALYSIS\")\n",
    "#     print(\"=\" * 50)\n",
    "    \n",
    "#     # Load all data files\n",
    "#     print(\"üìÇ Loading data files...\")\n",
    "    \n",
    "#     samples = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/AmostrasAngolaTerrario.xlsx\")\n",
    "#     analyses = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Horizontes Analises.xlsx\")\n",
    "#     morphology = pd.read_csv(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_clean/morpho_cleaned.csv\")\n",
    "#     profile_loc = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Perfis_local.xlsx\")\n",
    "#     soil_profile = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Perfis_solo.xlsx\")\n",
    "    \n",
    "#     print(\"‚úÖ Data loaded successfully\")\n",
    "    \n",
    "#     # Function to standardize profile names consistently\n",
    "#     def standardize_profile(profile_series, table_name):\n",
    "#         \"\"\"Standardize profile names using your cleaning approach\"\"\"\n",
    "#         print(f\"  üßπ Cleaning profiles in {table_name}...\")\n",
    "        \n",
    "#         cleaned = (profile_series\n",
    "#                   .astype(str)\n",
    "#                   .str.replace('/', '_')\n",
    "#                   .str.strip()\n",
    "#                   .str[:20]\n",
    "#                   .str.upper()  # Convert to uppercase for consistent comparison\n",
    "#                   )\n",
    "        \n",
    "#         return cleaned\n",
    "    \n",
    "#     # Clean profile names in each table\n",
    "#     print(\"\\nüßπ Standardizing profile names...\")\n",
    "    \n",
    "#     # Samples table\n",
    "#     samples_profiles = standardize_profile(samples['Perfil'], 'samples')\n",
    "#     samples_unique = set(samples_profiles.dropna())\n",
    "    \n",
    "#     # Analyses table  \n",
    "#     analyses_profiles = standardize_profile(analyses['PERFIL'], 'analyses')\n",
    "#     analyses_unique = set(analyses_profiles.dropna())\n",
    "    \n",
    "#     # Morphology table\n",
    "#     morphology_profiles = standardize_profile(morphology['Perfil'], 'morphology')\n",
    "#     morphology_unique = set(morphology_profiles.dropna())\n",
    "    \n",
    "#     # Profile location table (perfis_local)\n",
    "#     profile_loc_profiles = standardize_profile(profile_loc['PERFIL'], 'profile_loc')\n",
    "#     profile_loc_unique = set(profile_loc_profiles.dropna())\n",
    "    \n",
    "#     # Soil profile table\n",
    "#     soil_profile_profiles = standardize_profile(soil_profile['Perfil'], 'soil_profile')\n",
    "#     soil_profile_unique = set(soil_profile_profiles.dropna())\n",
    "    \n",
    "#     # Print summary statistics\n",
    "#     print(\"\\nüìä PROFILE SUMMARY BY TABLE:\")\n",
    "#     print(f\"  üìç Profile_loc (perfis_local):  {len(profile_loc_unique):,} unique profiles\")\n",
    "#     print(f\"  üß™ Samples:                     {len(samples_unique):,} unique profiles\")\n",
    "#     print(f\"  üî¨ Analyses:                    {len(analyses_unique):,} unique profiles\") \n",
    "#     print(f\"  ü™® Morphology:                  {len(morphology_unique):,} unique profiles\")\n",
    "#     print(f\"  üå± Soil_profile:                {len(soil_profile_unique):,} unique profiles\")\n",
    "    \n",
    "#     # Find intersections\n",
    "#     print(\"\\nüéØ FINDING MATCHING PROFILES...\")\n",
    "    \n",
    "#     # Profiles that appear in ALL tables\n",
    "#     all_tables_match = (profile_loc_unique & \n",
    "#                        samples_unique & \n",
    "#                        analyses_unique & \n",
    "#                        morphology_unique & \n",
    "#                        soil_profile_unique)\n",
    "    \n",
    "#     # Profiles that appear in perfis_local AND each other table\n",
    "#     loc_samples_match = profile_loc_unique & samples_unique\n",
    "#     loc_analyses_match = profile_loc_unique & analyses_unique\n",
    "#     loc_morphology_match = profile_loc_unique & morphology_unique\n",
    "#     loc_soil_match = profile_loc_unique & soil_profile_unique\n",
    "    \n",
    "#     # Profiles that appear in perfis_local AND at least 3 other tables\n",
    "#     at_least_4_tables = set()\n",
    "#     for profile in profile_loc_unique:\n",
    "#         count = sum([\n",
    "#             profile in samples_unique,\n",
    "#             profile in analyses_unique, \n",
    "#             profile in morphology_unique,\n",
    "#             profile in soil_profile_unique\n",
    "#         ])\n",
    "#         if count >= 3:  # perfis_local + at least 3 others = 4 total\n",
    "#             at_least_4_tables.add(profile)\n",
    "    \n",
    "#     # Print matching results\n",
    "#     print(f\"\\n‚úÖ MATCHING RESULTS:\")\n",
    "#     print(f\"  üéØ Profiles in ALL 5 tables:           {len(all_tables_match):,}\")\n",
    "#     print(f\"  üìç Perfis_local ‚à© Samples:             {len(loc_samples_match):,}\")\n",
    "#     print(f\"  üìç Perfis_local ‚à© Analyses:            {len(loc_analyses_match):,}\")\n",
    "#     print(f\"  üìç Perfis_local ‚à© Morphology:          {len(loc_morphology_match):,}\")\n",
    "#     print(f\"  üìç Perfis_local ‚à© Soil_profile:        {len(loc_soil_match):,}\")\n",
    "#     print(f\"  üåü Perfis_local + at least 3 others:   {len(at_least_4_tables):,}\")\n",
    "    \n",
    "#     # Create detailed results dataframe\n",
    "#     print(\"\\nüìã Creating detailed matching report...\")\n",
    "    \n",
    "#     # Create a comprehensive report\n",
    "#     all_profiles = profile_loc_unique | samples_unique | analyses_unique | morphology_unique | soil_profile_unique\n",
    "    \n",
    "#     matching_report = []\n",
    "    \n",
    "#     for profile in sorted(all_profiles):\n",
    "#         row = {\n",
    "#             'profile': profile,\n",
    "#             'in_profile_loc': profile in profile_loc_unique,\n",
    "#             'in_samples': profile in samples_unique,\n",
    "#             'in_analyses': profile in analyses_unique,\n",
    "#             'in_morphology': profile in morphology_unique,\n",
    "#             'in_soil_profile': profile in soil_profile_unique,\n",
    "#             'total_tables': sum([\n",
    "#                 profile in profile_loc_unique,\n",
    "#                 profile in samples_unique,\n",
    "#                 profile in analyses_unique,\n",
    "#                 profile in morphology_unique,\n",
    "#                 profile in soil_profile_unique\n",
    "#             ]),\n",
    "#             'in_all_tables': profile in all_tables_match,\n",
    "#             'in_perfis_local_plus_3': profile in at_least_4_tables\n",
    "#         }\n",
    "#         matching_report.append(row)\n",
    "    \n",
    "#     matching_df = pd.DataFrame(matching_report)\n",
    "    \n",
    "#     # Create specific lists for your analysis\n",
    "#     print(\"\\nüìù Creating specific profile lists...\")\n",
    "    \n",
    "#     # List 1: Profiles in ALL tables (highest confidence)\n",
    "#     all_tables_list = sorted(list(all_tables_match))\n",
    "    \n",
    "#     # List 2: Profiles in perfis_local + at least 3 other tables (good confidence)\n",
    "#     good_match_list = sorted(list(at_least_4_tables))\n",
    "    \n",
    "#     # List 3: Profiles only in perfis_local (potential orphans)\n",
    "#     only_in_perfis_local = profile_loc_unique - (samples_unique | analyses_unique | morphology_unique | soil_profile_unique)\n",
    "#     orphan_list = sorted(list(only_in_perfis_local))\n",
    "    \n",
    "#     # List 4: Profiles missing from perfis_local but in other tables\n",
    "#     missing_from_perfis_local = (samples_unique | analyses_unique | morphology_unique | soil_profile_unique) - profile_loc_unique\n",
    "#     missing_list = sorted(list(missing_from_perfis_local))\n",
    "    \n",
    "#     print(f\"  ‚úÖ All tables match list:       {len(all_tables_list):,} profiles\")\n",
    "#     print(f\"  ‚úÖ Good match list:             {len(good_match_list):,} profiles\") \n",
    "#     print(f\"  ‚ö†Ô∏è Orphans (only perfis_local): {len(orphan_list):,} profiles\")\n",
    "#     print(f\"  ‚ö†Ô∏è Missing from perfis_local:   {len(missing_list):,} profiles\")\n",
    "    \n",
    "#     # Save results to CSV files\n",
    "#     print(\"\\nüíæ Saving results to CSV files...\")\n",
    "    \n",
    "#     output_dir = \"/Users/inesschwartz/GreenDataScience/Thesis/profile_matching_results\"\n",
    "#     import os\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "#     # Save comprehensive report\n",
    "#     matching_df.to_csv(f\"{output_dir}/comprehensive_profile_matching_report.csv\", index=False)\n",
    "#     print(f\"  üíæ Saved: comprehensive_profile_matching_report.csv\")\n",
    "    \n",
    "#     # Save specific lists\n",
    "#     pd.DataFrame({'profile': all_tables_list}).to_csv(f\"{output_dir}/profiles_in_all_tables.csv\", index=False)\n",
    "#     print(f\"  üíæ Saved: profiles_in_all_tables.csv ({len(all_tables_list)} profiles)\")\n",
    "    \n",
    "#     pd.DataFrame({'profile': good_match_list}).to_csv(f\"{output_dir}/profiles_good_match.csv\", index=False)\n",
    "#     print(f\"  üíæ Saved: profiles_good_match.csv ({len(good_match_list)} profiles)\")\n",
    "    \n",
    "#     pd.DataFrame({'profile': orphan_list}).to_csv(f\"{output_dir}/profiles_orphaned.csv\", index=False)\n",
    "#     print(f\"  üíæ Saved: profiles_orphaned.csv ({len(orphan_list)} profiles)\")\n",
    "    \n",
    "#     pd.DataFrame({'profile': missing_list}).to_csv(f\"{output_dir}/profiles_missing_from_perfis_local.csv\", index=False)\n",
    "#     print(f\"  üíæ Saved: profiles_missing_from_perfis_local.csv ({len(missing_list)} profiles)\")\n",
    "    \n",
    "#     # Create a summary for each specific intersection\n",
    "#     intersections = {\n",
    "#         'perfis_local_AND_samples': sorted(list(loc_samples_match)),\n",
    "#         'perfis_local_AND_analyses': sorted(list(loc_analyses_match)),\n",
    "#         'perfis_local_AND_morphology': sorted(list(loc_morphology_match)),\n",
    "#         'perfis_local_AND_soil_profile': sorted(list(loc_soil_match))\n",
    "#     }\n",
    "    \n",
    "#     for name, profile_list in intersections.items():\n",
    "#         pd.DataFrame({'profile': profile_list}).to_csv(f\"{output_dir}/{name}.csv\", index=False)\n",
    "#         print(f\"  üíæ Saved: {name}.csv ({len(profile_list)} profiles)\")\n",
    "    \n",
    "#     # Print sample of results for quick review\n",
    "#     print(f\"\\nüëÄ SAMPLE RESULTS:\")\n",
    "#     print(f\"\\nFirst 10 profiles that appear in ALL tables:\")\n",
    "#     for i, profile in enumerate(all_tables_list[:10]):\n",
    "#         print(f\"  {i+1:2d}. {profile}\")\n",
    "    \n",
    "#     if orphan_list:\n",
    "#         print(f\"\\nFirst 10 profiles ONLY in perfis_local (potential issues):\")\n",
    "#         for i, profile in enumerate(orphan_list[:10]):\n",
    "#             print(f\"  {i+1:2d}. {profile}\")\n",
    "    \n",
    "#     if missing_list:\n",
    "#         print(f\"\\nFirst 10 profiles MISSING from perfis_local:\")\n",
    "#         for i, profile in enumerate(missing_list[:10]):\n",
    "#             print(f\"  {i+1:2d}. {profile}\")\n",
    "    \n",
    "#     # Summary recommendations\n",
    "#     print(f\"\\nüéØ RECOMMENDATIONS:\")\n",
    "#     print(f\"  1. Start with the {len(all_tables_list)} profiles that appear in ALL tables\")\n",
    "#     print(f\"  2. Focus fixing on the {len(good_match_list)} profiles with good matches\") \n",
    "#     print(f\"  3. Investigate the {len(orphan_list)} orphaned profiles in perfis_local\")\n",
    "#     print(f\"  4. Consider adding {len(missing_list)} missing profiles to perfis_local\")\n",
    "    \n",
    "#     return {\n",
    "#         'matching_df': matching_df,\n",
    "#         'all_tables_match': all_tables_list,\n",
    "#         'good_match': good_match_list,\n",
    "#         'orphans': orphan_list,\n",
    "#         'missing': missing_list,\n",
    "#         'intersections': intersections\n",
    "#     }\n",
    "\n",
    "# # Run the analysis\n",
    "# if __name__ == \"__main__\":\n",
    "#     results = analyze_profile_matching()\n",
    "#     print(f\"\\n‚úÖ Analysis complete! Check the CSV files for detailed results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåü GOLDEN PROFILES ANALYSIS\n",
      "==================================================\n",
      "Working with 1,109 profiles that appear in ALL tables\n",
      "Goal: Create clean, linked database with composite keys\n",
      "==================================================\n",
      "\n",
      "üìÇ SECTION 1: LOADING DATA AND FILTERING TO GOLDEN PROFILES\n",
      "------------------------------------------------------------\n",
      "Loading original data files...\n",
      "‚úÖ Original data loaded successfully\n",
      "‚úÖ Loaded 1100 golden profiles\n",
      "Sample golden profiles: ['100_58', '100_59', '100_61', '100_63', '101A_58']\n",
      "\n",
      "üßπ Standardizing profile names...\n",
      "  üìä Samples: 14,715 ‚Üí 5,649 records\n",
      "  üìä Analyses: 7,847 ‚Üí 5,474 records\n",
      "  üìä Morphology: 10,434 ‚Üí 5,328 records\n",
      "  üìä Profile_loc: 4,321 ‚Üí 1,100 records\n",
      "  üìä Soil_profile: 2,518 ‚Üí 1,100 records\n",
      "‚úÖ Golden profile filtering complete!\n",
      "\n",
      "üßπ SECTION 2: CLEANING AND PREPARING DATA\n",
      "------------------------------------------------------------\n",
      "Cleaning samples data...\n",
      "  ‚úÖ Samples cleaned: 5,649 records\n",
      "Cleaning morphology data...\n",
      "  ‚úÖ Morphology cleaned: 5,328 records\n",
      "      - Non-null sample_ids: 4,256\n",
      "      - NULL sample_ids: 1,072\n",
      "Cleaning profile location data...\n",
      "  ‚úÖ Profile location cleaned: 1,100 records\n",
      "Cleaning analyses data...\n",
      "  ‚úÖ Analyses cleaned: 5,474 records\n",
      "‚úÖ Data cleaning complete!\n",
      "\n",
      "üîó SECTION 3: FIXING SAMPLE-HORIZON LINKS\n",
      "------------------------------------------------------------\n",
      "Creating sample-horizon mapping...\n",
      "  üìã Created mapping for 8,508 sample-horizon relationships\n",
      "  ‚úÖ Sample-horizon linking results:\n",
      "      - Total samples: 5,649\n",
      "      - Successfully linked: 4,144\n",
      "      - Link rate: 73.4%\n",
      "      - Unlinked samples: 1,505\n",
      "\n",
      "üåç SECTION 4: FIXING PROFILE-SITE LINKS\n",
      "------------------------------------------------------------\n",
      "Creating profile-site mapping...\n",
      "  üìã Created mapping for 1,100 profile-site relationships\n",
      "  üìä Existing vs mapped site_info_id match rate: 2/5649\n",
      "‚úÖ Profile-site linking complete!\n",
      "\n",
      "üîë SECTION 5: CREATING COMPOSITE KEYS\n",
      "------------------------------------------------------------\n",
      "Creating site composite keys...\n",
      "  ‚úÖ Created 1100 site composite keys\n",
      "Creating profile composite keys...\n",
      "  ‚úÖ Created profile composite keys for all tables\n",
      "Creating horizon composite keys...\n",
      "  ‚úÖ Created 5328 horizon composite keys\n",
      "Creating sample composite keys...\n",
      "  ‚úÖ Created 5649 sample composite keys\n",
      "Creating analysis composite keys...\n",
      "  ‚úÖ Created 5474 analysis composite keys\n",
      "‚úÖ All composite keys created!\n",
      "\n",
      "üîç SECTION 6: VALIDATING COMPOSITE KEYS\n",
      "------------------------------------------------------------\n",
      "  ‚úÖ No duplicates in samples\n",
      "  ‚úÖ No duplicates in morphology\n",
      "  ‚úÖ No duplicates in analyses\n",
      "  ‚ö†Ô∏è Found 4 duplicate keys in profile_loc\n",
      "    ‚úÖ Fixed duplicates with sequence numbers\n",
      "\n",
      "üìä Key uniqueness validation:\n",
      "  Site keys: 1100/1100\n",
      "  Profile keys: 1100/5649\n",
      "  Horizon keys: 5328/5328\n",
      "  Sample keys: 5649/5649\n",
      "  Analysis keys: 5474/5474\n",
      "\n",
      "üß™ SECTION 7: TESTING QUERIES AND ANALYSIS\n",
      "------------------------------------------------------------\n",
      "Test 1: Joining samples with morphology...\n",
      "  ‚úÖ Traditional join successful: 4,146 records\n",
      "  üìä Sample analysis - Records by depth:\n",
      "             sample_id                         texture\n",
      "upper_depth                                           \n",
      "0.0                825                         Arenoso\n",
      "0.5                  1  Franco-argiloso pouco calcario\n",
      "2.0                  2           Franco-argilo-arenoso\n",
      "3.0                  6                        Argiloso\n",
      "4.0                  9                  Arenoso-franco\n",
      "5.0                 20                  Arenoso-franco\n",
      "6.0                 35                         Arenoso\n",
      "7.0                 39                         Arenoso\n",
      "8.0                 83                         Arenoso\n",
      "9.0                 55                         Arenoso\n",
      "\n",
      "Test 2: Geographic distribution analysis...\n",
      "  üìä Profiles by district:\n",
      "                composite_site_key  X_coord  Y_coord\n",
      "district                                            \n",
      "Bengo                            5  13.5269  -9.7337\n",
      "Benguela                       129  13.8761 -12.5125\n",
      "Bi√©                             73  16.5177 -11.9381\n",
      "CUANDO-CUBANGO                   1  16.8183 -14.9847\n",
      "Cabinda                         92  12.4292  -5.0235\n",
      "Cuando-Cubango                  61  18.7304 -15.6194\n",
      "Cuanza Sul                     155  14.4982 -10.8277\n",
      "Huila                            2  15.9377 -17.2183\n",
      "Luanda                           2  13.3759  -8.9127\n",
      "Lunda Norte                      2  17.3398  -9.1452\n",
      "\n",
      "Test 3: Data completeness assessment...\n",
      "  üìä Data completeness summary:\n",
      "         Table  Total_Records  Composite_Keys  Linked_Records  Completeness_%\n",
      "0      Samples           5649            5649            4144            73.4\n",
      "1   Morphology           5328            5328            4256            79.9\n",
      "2     Analyses           5474            5474            5469            99.9\n",
      "3  Profile_loc           1100            1100            1099            99.9\n",
      "\n",
      "üíæ SECTION 8: SAVING RESULTS\n",
      "------------------------------------------------------------\n",
      "  üíæ Saved: samples_golden_with_keys.csv (5,649 records)\n",
      "  üíæ Saved: morphology_golden_with_keys.csv (5,328 records)\n",
      "  üíæ Saved: analyses_golden_with_keys.csv (5,474 records)\n",
      "  üíæ Saved: profile_loc_golden_with_keys.csv (1,100 records)\n",
      "  üíæ Saved: data_completeness_report.csv\n",
      "  üíæ Saved: analysis_summary.txt\n",
      "\n",
      "üéâ FINAL SUMMARY\n",
      "============================================================\n",
      "‚úÖ Successfully processed 1100 golden profiles\n",
      "‚úÖ Created composite keys for 5,649 samples\n",
      "‚úÖ Linked 4,144 samples to horizons (73.4% success rate)\n",
      "‚úÖ All tables now have robust composite keys\n",
      "‚úÖ Data ready for database import and analysis\n",
      "\n",
      "üìÅ Output files saved to: /Users/inesschwartz/GreenDataScience/Thesis/golden_profiles_output\n",
      "\n",
      "üéØ NEXT STEPS:\n",
      "  1. Review the data completeness report\n",
      "  2. Test additional queries on your clean data\n",
      "  3. Import to database using composite keys as primary keys\n",
      "  4. Expand to the additional 646 'good match' profiles when ready\n",
      "\n",
      "üåü You now have a clean, linked subset representing your highest quality soil data!\n"
     ]
    }
   ],
   "source": [
    "# Golden Profiles Analysis: 1,109 High-Quality Soil Profiles\n",
    "# Focus on profiles that appear in ALL 5 tables for maximum data integrity\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üåü GOLDEN PROFILES ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Working with 1,109 profiles that appear in ALL tables\")\n",
    "print(\"Goal: Create clean, linked database with composite keys\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: LOAD DATA AND FILTER TO GOLDEN PROFILES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìÇ SECTION 1: LOADING DATA AND FILTERING TO GOLDEN PROFILES\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Load original data files\n",
    "print(\"Loading original data files...\")\n",
    "samples = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/AmostrasAngolaTerrario.xlsx\")\n",
    "analyses = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Horizontes Analises.xlsx\")\n",
    "#morphology = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Horizontes_Morfologia.xlsx\")\n",
    "morphology = pd.read_csv(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_clean/morpho_cleaned.csv\")\n",
    "profile_loc = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Perfis_local.xlsx\")\n",
    "soil_profile = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Perfis_solo.xlsx\")\n",
    "elemental_analyses = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Data XRF Angola_inicial.xlsx\")\n",
    "\n",
    "print(\"‚úÖ Original data loaded successfully\")\n",
    "\n",
    "# Load golden profiles list\n",
    "golden_profiles_df = pd.read_csv(\"/Users/inesschwartz/GreenDataScience/Thesis/profile_matching_results/profiles_in_all_tables.csv\")\n",
    "golden_profiles = golden_profiles_df['profile'].tolist()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(golden_profiles)} golden profiles\")\n",
    "print(f\"Sample golden profiles: {golden_profiles[:5]}\")\n",
    "\n",
    "# Function to standardize profile names (consistent with your analysis)\n",
    "def standardize_profile(profile_series):\n",
    "    \"\"\"Standardize profile names consistently\"\"\"\n",
    "    return (profile_series\n",
    "            .astype(str)\n",
    "            .str.replace('/', '_')\n",
    "            .str.strip()\n",
    "            .str[:20]\n",
    "            .str.upper())\n",
    "\n",
    "# Apply standardization to all tables\n",
    "print(\"\\nüßπ Standardizing profile names...\")\n",
    "\n",
    "# Samples\n",
    "samples['profile_clean'] = standardize_profile(samples['Perfil'])\n",
    "samples_original_count = len(samples)\n",
    "samples_golden = samples[samples['profile_clean'].isin(golden_profiles)].copy()\n",
    "print(f\"  üìä Samples: {samples_original_count:,} ‚Üí {len(samples_golden):,} records\")\n",
    "\n",
    "# Analyses\n",
    "analyses['profile_clean'] = standardize_profile(analyses['PERFIL'])\n",
    "analyses_original_count = len(analyses)\n",
    "analyses_golden = analyses[analyses['profile_clean'].isin(golden_profiles)].copy()\n",
    "print(f\"  üìä Analyses: {analyses_original_count:,} ‚Üí {len(analyses_golden):,} records\")\n",
    "\n",
    "# Morphology\n",
    "morphology['profile_clean'] = standardize_profile(morphology['Perfil'])\n",
    "morphology_original_count = len(morphology)\n",
    "morphology_golden = morphology[morphology['profile_clean'].isin(golden_profiles)].copy()\n",
    "print(f\"  üìä Morphology: {morphology_original_count:,} ‚Üí {len(morphology_golden):,} records\")\n",
    "\n",
    "# Profile location\n",
    "profile_loc['profile_clean'] = standardize_profile(profile_loc['PERFIL'])\n",
    "profile_loc_original_count = len(profile_loc)\n",
    "profile_loc_golden = profile_loc[profile_loc['profile_clean'].isin(golden_profiles)].copy()\n",
    "print(f\"  üìä Profile_loc: {profile_loc_original_count:,} ‚Üí {len(profile_loc_golden):,} records\")\n",
    "\n",
    "# Soil profile\n",
    "soil_profile['profile_clean'] = standardize_profile(soil_profile['Perfil'])\n",
    "soil_profile_original_count = len(soil_profile)\n",
    "soil_profile_golden = soil_profile[soil_profile['profile_clean'].isin(golden_profiles)].copy()\n",
    "print(f\"  üìä Soil_profile: {soil_profile_original_count:,} ‚Üí {len(soil_profile_golden):,} records\")\n",
    "\n",
    "print(\"‚úÖ Golden profile filtering complete!\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: CLEAN AND PREPARE DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüßπ SECTION 2: CLEANING AND PREPARING DATA\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Clean samples data\n",
    "print(\"Cleaning samples data...\")\n",
    "samples_golden.rename(columns={\n",
    "    'Registo': 'sample_id',\n",
    "    'N¬∫ Campo': 'site_info_id', \n",
    "    'Ano': 'year',\n",
    "    'Perfil': 'profile_original',\n",
    "    'Distrito': 'district',\n",
    "    'Prateleira': 'shelf',\n",
    "    'Sala': 'room'\n",
    "}, inplace=True)\n",
    "\n",
    "# Keep essential columns\n",
    "samples_golden = samples_golden[[\n",
    "    'sample_id', 'site_info_id', 'profile_original', 'profile_clean', \n",
    "    'year', 'district', 'shelf', 'room'\n",
    "]].copy()\n",
    "\n",
    "# Clean data types\n",
    "samples_golden['sample_id'] = samples_golden['sample_id'].astype(str).str.strip()\n",
    "samples_golden['site_info_id'] = samples_golden['site_info_id'].astype(str).str.strip()\n",
    "\n",
    "print(f\"  ‚úÖ Samples cleaned: {len(samples_golden):,} records\")\n",
    "\n",
    "# Clean morphology data\n",
    "print(\"Cleaning morphology data...\")\n",
    "morphology_golden.rename(columns={\n",
    "    'Morfo_id': 'horizon_id',\n",
    "    'Amostra': 'sample_id',\n",
    "    'Perfil': 'profile_original',\n",
    "    'CM': 'horizon_layer',\n",
    "    'Limite Superior': 'upper_depth',\n",
    "    'Limite inferior': 'lower_depth',\n",
    "    'Textura': 'texture',\n",
    "    'Tipo de estrutura': 'structure_type'\n",
    "}, inplace=True)\n",
    "\n",
    "# Clean sample_id in morphology (remove decimals, handle NULLs)\n",
    "def clean_sample_id(x):\n",
    "    if pd.isna(x) or str(x).upper() == 'NULL':\n",
    "        return None\n",
    "    try:\n",
    "        # Remove decimal parts like your original cleaning\n",
    "        return str(int(float(str(x).replace('.0', ''))))\n",
    "    except:\n",
    "        return str(x).strip()\n",
    "\n",
    "morphology_golden['sample_id_clean'] = morphology_golden['sample_id'].apply(clean_sample_id)\n",
    "\n",
    "# Keep essential columns\n",
    "morphology_golden = morphology_golden[[\n",
    "    'horizon_id', 'sample_id', 'sample_id_clean', 'profile_original', 'profile_clean',\n",
    "    'horizon_layer', 'upper_depth', 'lower_depth', 'texture', 'structure_type'\n",
    "]].copy()\n",
    "\n",
    "print(f\"  ‚úÖ Morphology cleaned: {len(morphology_golden):,} records\")\n",
    "print(f\"      - Non-null sample_ids: {morphology_golden['sample_id_clean'].notna().sum():,}\")\n",
    "print(f\"      - NULL sample_ids: {morphology_golden['sample_id_clean'].isna().sum():,}\")\n",
    "\n",
    "# Clean profile location data\n",
    "print(\"Cleaning profile location data...\")\n",
    "profile_loc_golden.rename(columns={\n",
    "    'PERFIL': 'profile_original',\n",
    "    'X_COORD': 'X_coord',\n",
    "    'Y_COORD': 'Y_coord',\n",
    "    'PRO': 'district'\n",
    "}, inplace=True)\n",
    "\n",
    "# Add primary key\n",
    "profile_loc_golden.insert(0, 'site_info_id', range(1, len(profile_loc_golden) + 1))\n",
    "\n",
    "# Keep essential columns\n",
    "profile_loc_golden = profile_loc_golden[[\n",
    "    'site_info_id', 'ID', 'profile_original', 'profile_clean',\n",
    "    'X_coord', 'Y_coord', 'district'\n",
    "]].copy()\n",
    "\n",
    "print(f\"  ‚úÖ Profile location cleaned: {len(profile_loc_golden):,} records\")\n",
    "\n",
    "# Clean analyses data\n",
    "print(\"Cleaning analyses data...\")\n",
    "analyses_golden.rename(columns={\n",
    "    'Amostra': 'sample_id',\n",
    "    'Morfo_id': 'horizon_id',\n",
    "    'Analise-id': 'analysis_id',\n",
    "    'PERFIL': 'profile_original'\n",
    "}, inplace=True)\n",
    "\n",
    "# Clean sample_id in analyses\n",
    "analyses_golden['sample_id_clean'] = analyses_golden['sample_id'].apply(clean_sample_id)\n",
    "\n",
    "# Add lab_sample_id\n",
    "analyses_golden.insert(0, 'lab_sample_id', range(1, len(analyses_golden) + 1))\n",
    "\n",
    "print(f\"  ‚úÖ Analyses cleaned: {len(analyses_golden):,} records\")\n",
    "\n",
    "print(\"‚úÖ Data cleaning complete!\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: FIX SAMPLE-HORIZON LINKS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüîó SECTION 3: FIXING SAMPLE-HORIZON LINKS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create sample_id to horizon_id mapping from morphology\n",
    "print(\"Creating sample-horizon mapping...\")\n",
    "\n",
    "# Remove null sample_ids from morphology for mapping\n",
    "morpho_for_mapping = morphology_golden[morphology_golden['sample_id_clean'].notna()].copy()\n",
    "\n",
    "# Create mapping dictionary: sample_id ‚Üí horizon_id\n",
    "sample_to_horizon = {}\n",
    "for _, row in morpho_for_mapping.iterrows():\n",
    "    sample_id = row['sample_id_clean']\n",
    "    horizon_id = row['horizon_id']\n",
    "    profile = row['profile_clean']\n",
    "    \n",
    "    # Use profile + sample_id as key for more specificity\n",
    "    key = f\"{profile}_{sample_id}\"\n",
    "    sample_to_horizon[key] = horizon_id\n",
    "    \n",
    "    # Also add just sample_id as fallback\n",
    "    if sample_id not in sample_to_horizon:\n",
    "        sample_to_horizon[sample_id] = horizon_id\n",
    "\n",
    "print(f\"  üìã Created mapping for {len(sample_to_horizon):,} sample-horizon relationships\")\n",
    "\n",
    "# Apply mapping to samples\n",
    "def get_horizon_id(row):\n",
    "    sample_id = str(row['sample_id']).strip()\n",
    "    profile = row['profile_clean']\n",
    "    \n",
    "    # Try profile + sample_id first\n",
    "    key = f\"{profile}_{sample_id}\"\n",
    "    if key in sample_to_horizon:\n",
    "        return sample_to_horizon[key]\n",
    "    \n",
    "    # Try just sample_id\n",
    "    if sample_id in sample_to_horizon:\n",
    "        return sample_to_horizon[sample_id]\n",
    "    \n",
    "    return None\n",
    "\n",
    "samples_golden['horizon_id'] = samples_golden.apply(get_horizon_id, axis=1)\n",
    "\n",
    "# Check results\n",
    "linked_samples = samples_golden['horizon_id'].notna().sum()\n",
    "total_samples = len(samples_golden)\n",
    "link_rate = (linked_samples / total_samples) * 100\n",
    "\n",
    "print(f\"  ‚úÖ Sample-horizon linking results:\")\n",
    "print(f\"      - Total samples: {total_samples:,}\")\n",
    "print(f\"      - Successfully linked: {linked_samples:,}\")\n",
    "print(f\"      - Link rate: {link_rate:.1f}%\")\n",
    "print(f\"      - Unlinked samples: {total_samples - linked_samples:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: FIX PROFILE-SITE LINKS  \n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüåç SECTION 4: FIXING PROFILE-SITE LINKS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create profile to site_info_id mapping\n",
    "print(\"Creating profile-site mapping...\")\n",
    "\n",
    "# Create mapping from profile_loc: profile ‚Üí site_info_id\n",
    "profile_to_site = dict(zip(profile_loc_golden['profile_clean'], profile_loc_golden['site_info_id']))\n",
    "\n",
    "print(f\"  üìã Created mapping for {len(profile_to_site):,} profile-site relationships\")\n",
    "\n",
    "# Apply to samples (add site_info_id)\n",
    "samples_golden['site_info_id_mapped'] = samples_golden['profile_clean'].map(profile_to_site)\n",
    "\n",
    "# Check if existing site_info_id matches mapped one\n",
    "site_match_rate = (samples_golden['site_info_id'] == samples_golden['site_info_id_mapped'].astype(str)).sum()\n",
    "print(f\"  üìä Existing vs mapped site_info_id match rate: {site_match_rate}/{len(samples_golden)}\")\n",
    "\n",
    "# Use mapped site_info_id\n",
    "samples_golden['site_info_id_final'] = samples_golden['site_info_id_mapped']\n",
    "\n",
    "# Apply to morphology (add site_info_id)\n",
    "morphology_golden['site_info_id'] = morphology_golden['profile_clean'].map(profile_to_site)\n",
    "\n",
    "# Apply to analyses (add site_info_id)  \n",
    "analyses_golden['site_info_id'] = analyses_golden['profile_clean'].map(profile_to_site)\n",
    "\n",
    "print(\"‚úÖ Profile-site linking complete!\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: CREATE COMPOSITE KEYS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüîë SECTION 5: CREATING COMPOSITE KEYS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 1. Site composite keys\n",
    "print(\"Creating site composite keys...\")\n",
    "def create_site_key(row):\n",
    "    if pd.notna(row['X_coord']) and pd.notna(row['Y_coord']):\n",
    "        x_clean = f\"{row['X_coord']:.4f}\".replace('.', 'p').replace('-', 'n')\n",
    "        y_clean = f\"{row['Y_coord']:.4f}\".replace('.', 'p').replace('-', 'n')\n",
    "        district = str(row.get('district', 'UNK')).replace(' ', '_').replace('/', '_')\n",
    "        return f\"SITE_{x_clean}_{y_clean}_{district}\"\n",
    "    else:\n",
    "        profile_clean = str(row['profile_clean']).replace('/', '_').replace(' ', '_')\n",
    "        return f\"SITE_PROF_{profile_clean}\"\n",
    "\n",
    "profile_loc_golden['composite_site_key'] = profile_loc_golden.apply(create_site_key, axis=1)\n",
    "print(f\"  ‚úÖ Created {len(profile_loc_golden)} site composite keys\")\n",
    "\n",
    "# 2. Profile composite keys\n",
    "print(\"Creating profile composite keys...\")\n",
    "# Create site mapping for profiles\n",
    "site_key_mapping = dict(zip(profile_loc_golden['profile_clean'], profile_loc_golden['composite_site_key']))\n",
    "\n",
    "def create_profile_key(row):\n",
    "    profile_clean = str(row['profile_clean']).replace('/', '_').replace(' ', '_')\n",
    "    site_key = site_key_mapping.get(row['profile_clean'], 'UNKNOWN_SITE')\n",
    "    return f\"PROF_{site_key}_{profile_clean}\"\n",
    "\n",
    "# Apply to all relevant tables\n",
    "samples_golden['composite_profile_key'] = samples_golden.apply(create_profile_key, axis=1)\n",
    "morphology_golden['composite_profile_key'] = morphology_golden.apply(create_profile_key, axis=1)\n",
    "analyses_golden['composite_profile_key'] = analyses_golden.apply(create_profile_key, axis=1)\n",
    "\n",
    "print(f\"  ‚úÖ Created profile composite keys for all tables\")\n",
    "\n",
    "# 3. Horizon composite keys\n",
    "print(\"Creating horizon composite keys...\")\n",
    "def create_horizon_key(row):\n",
    "    profile_key = row['composite_profile_key']\n",
    "    layer = int(row.get('horizon_layer', 0)) if pd.notna(row.get('horizon_layer')) else 0\n",
    "    upper = int(row.get('upper_depth', 0)) if pd.notna(row.get('upper_depth')) else 0\n",
    "    lower = int(row.get('lower_depth', 999)) if pd.notna(row.get('lower_depth')) else 999\n",
    "    \n",
    "    return f\"HOR_{profile_key}_L{layer}_D{upper}to{lower}\"\n",
    "\n",
    "morphology_golden['composite_horizon_key'] = morphology_golden.apply(create_horizon_key, axis=1)\n",
    "print(f\"  ‚úÖ Created {len(morphology_golden)} horizon composite keys\")\n",
    "\n",
    "# 4. Sample composite keys\n",
    "print(\"Creating sample composite keys...\")\n",
    "# Create horizon mapping for samples\n",
    "horizon_key_mapping = {}\n",
    "for _, row in morphology_golden.iterrows():\n",
    "    if pd.notna(row['sample_id_clean']):\n",
    "        horizon_key_mapping[row['sample_id_clean']] = row['composite_horizon_key']\n",
    "\n",
    "def create_sample_key(row):\n",
    "    sample_id = str(row['sample_id']).strip()\n",
    "    profile_key = row['composite_profile_key']\n",
    "    \n",
    "    # Try to get horizon key\n",
    "    horizon_key = horizon_key_mapping.get(sample_id, '')\n",
    "    \n",
    "    if horizon_key:\n",
    "        return f\"SAMP_{sample_id}_{horizon_key}\"\n",
    "    else:\n",
    "        # Fallback: use profile + year + storage\n",
    "        year = str(int(row['year'])) if pd.notna(row['year']) else 'UNK'\n",
    "        shelf = str(row.get('shelf', 'X'))\n",
    "        room = str(row.get('room', 'X'))\n",
    "        return f\"SAMP_{sample_id}_{profile_key}_{year}_{shelf}_{room}\"\n",
    "\n",
    "samples_golden['composite_sample_key'] = samples_golden.apply(create_sample_key, axis=1)\n",
    "print(f\"  ‚úÖ Created {len(samples_golden)} sample composite keys\")\n",
    "\n",
    "# 5. Analysis composite keys  \n",
    "print(\"Creating analysis composite keys...\")\n",
    "# Create sample mapping for analyses\n",
    "sample_key_mapping = dict(zip(samples_golden['sample_id'], samples_golden['composite_sample_key']))\n",
    "\n",
    "def create_analysis_key(row):\n",
    "    sample_id = str(row['sample_id_clean']) if pd.notna(row['sample_id_clean']) else str(row['sample_id'])\n",
    "    sample_key = sample_key_mapping.get(sample_id, f\"SAMP_UNKNOWN_{sample_id}\")\n",
    "    lab_id = str(row['lab_sample_id'])\n",
    "    analysis_id = str(row.get('analysis_id', 'STD'))\n",
    "    \n",
    "    return f\"ANAL_{sample_key}_{lab_id}_{analysis_id}\"\n",
    "\n",
    "analyses_golden['composite_analysis_key'] = analyses_golden.apply(create_analysis_key, axis=1)\n",
    "print(f\"  ‚úÖ Created {len(analyses_golden)} analysis composite keys\")\n",
    "\n",
    "print(\"‚úÖ All composite keys created!\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: HANDLE DUPLICATES AND VALIDATE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüîç SECTION 6: VALIDATING COMPOSITE KEYS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def check_and_fix_duplicates(df, key_column, table_name):\n",
    "    \"\"\"Check for duplicate keys and add sequence numbers if needed\"\"\"\n",
    "    duplicates = df[key_column].duplicated()\n",
    "    \n",
    "    if duplicates.any():\n",
    "        duplicate_count = duplicates.sum()\n",
    "        print(f\"  ‚ö†Ô∏è Found {duplicate_count} duplicate keys in {table_name}\")\n",
    "        \n",
    "        # Add sequence numbers to duplicates\n",
    "        df['_seq'] = df.groupby(key_column).cumcount() + 1\n",
    "        mask = df['_seq'] > 1\n",
    "        df.loc[mask, key_column] = df.loc[mask, key_column] + '_SEQ' + df.loc[mask, '_seq'].astype(str)\n",
    "        df = df.drop('_seq', axis=1)\n",
    "        \n",
    "        print(f\"    ‚úÖ Fixed duplicates with sequence numbers\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ No duplicates in {table_name}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Check all tables for duplicates\n",
    "samples_golden = check_and_fix_duplicates(samples_golden, 'composite_sample_key', 'samples')\n",
    "morphology_golden = check_and_fix_duplicates(morphology_golden, 'composite_horizon_key', 'morphology')\n",
    "analyses_golden = check_and_fix_duplicates(analyses_golden, 'composite_analysis_key', 'analyses')\n",
    "profile_loc_golden = check_and_fix_duplicates(profile_loc_golden, 'composite_site_key', 'profile_loc')\n",
    "\n",
    "# Validate key uniqueness\n",
    "print(\"\\nüìä Key uniqueness validation:\")\n",
    "print(f\"  Site keys: {profile_loc_golden['composite_site_key'].nunique()}/{len(profile_loc_golden)}\")\n",
    "print(f\"  Profile keys: {samples_golden['composite_profile_key'].nunique()}/{len(samples_golden)}\")\n",
    "print(f\"  Horizon keys: {morphology_golden['composite_horizon_key'].nunique()}/{len(morphology_golden)}\")\n",
    "print(f\"  Sample keys: {samples_golden['composite_sample_key'].nunique()}/{len(samples_golden)}\")\n",
    "print(f\"  Analysis keys: {analyses_golden['composite_analysis_key'].nunique()}/{len(analyses_golden)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: TEST QUERIES AND ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüß™ SECTION 7: TESTING QUERIES AND ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Test 1: Join samples with morphology using composite keys\n",
    "print(\"Test 1: Joining samples with morphology...\")\n",
    "try:\n",
    "    # Create a simplified join\n",
    "    samples_for_join = samples_golden[['composite_sample_key', 'sample_id', 'profile_clean', 'year']].copy()\n",
    "    morphology_for_join = morphology_golden[['composite_horizon_key', 'sample_id_clean', 'profile_clean', 'texture', 'upper_depth', 'lower_depth']].copy()\n",
    "    \n",
    "    # Join on sample_id (traditional way)\n",
    "    traditional_join = samples_for_join.merge(\n",
    "        morphology_for_join, \n",
    "        left_on='sample_id', \n",
    "        right_on='sample_id_clean',\n",
    "        how='inner',\n",
    "        suffixes=('_samp', '_morph')\n",
    "    )\n",
    "    \n",
    "    print(f\"  ‚úÖ Traditional join successful: {len(traditional_join):,} records\")\n",
    "    \n",
    "    # Test sample query: pH by depth\n",
    "    if len(traditional_join) > 0:\n",
    "        depth_analysis = traditional_join.groupby('upper_depth').agg({\n",
    "            'sample_id': 'count',\n",
    "            'texture': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'Unknown'\n",
    "        }).head(10)\n",
    "        \n",
    "        print(\"  üìä Sample analysis - Records by depth:\")\n",
    "        print(depth_analysis)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Join failed: {e}\")\n",
    "\n",
    "# Test 2: Geographic analysis\n",
    "print(\"\\nTest 2: Geographic distribution analysis...\")\n",
    "try:\n",
    "    geo_summary = profile_loc_golden.groupby('district').agg({\n",
    "        'composite_site_key': 'count',\n",
    "        'X_coord': 'mean',\n",
    "        'Y_coord': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"  üìä Profiles by district:\")\n",
    "    print(geo_summary.head(10))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Geographic analysis failed: {e}\")\n",
    "\n",
    "# Test 3: Data completeness assessment\n",
    "print(\"\\nTest 3: Data completeness assessment...\")\n",
    "\n",
    "completeness_report = {\n",
    "    'Table': ['Samples', 'Morphology', 'Analyses', 'Profile_loc'],\n",
    "    'Total_Records': [len(samples_golden), len(morphology_golden), len(analyses_golden), len(profile_loc_golden)],\n",
    "    'Composite_Keys': [\n",
    "        samples_golden['composite_sample_key'].notna().sum(),\n",
    "        morphology_golden['composite_horizon_key'].notna().sum(), \n",
    "        analyses_golden['composite_analysis_key'].notna().sum(),\n",
    "        profile_loc_golden['composite_site_key'].notna().sum()\n",
    "    ],\n",
    "    'Linked_Records': [\n",
    "        samples_golden['horizon_id'].notna().sum(),\n",
    "        morphology_golden['sample_id_clean'].notna().sum(),\n",
    "        analyses_golden['sample_id_clean'].notna().sum(),\n",
    "        profile_loc_golden['X_coord'].notna().sum()\n",
    "    ]\n",
    "}\n",
    "\n",
    "completeness_df = pd.DataFrame(completeness_report)\n",
    "completeness_df['Completeness_%'] = (completeness_df['Linked_Records'] / completeness_df['Total_Records'] * 100).round(1)\n",
    "\n",
    "print(\"  üìä Data completeness summary:\")\n",
    "print(completeness_df)\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüíæ SECTION 8: SAVING RESULTS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"/Users/inesschwartz/GreenDataScience/Thesis/golden_profiles_output\"\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save cleaned tables with composite keys\n",
    "tables_to_save = {\n",
    "    'samples_golden_with_keys': samples_golden,\n",
    "    'morphology_golden_with_keys': morphology_golden,\n",
    "    'analyses_golden_with_keys': analyses_golden,\n",
    "    'profile_loc_golden_with_keys': profile_loc_golden\n",
    "}\n",
    "\n",
    "for name, df in tables_to_save.items():\n",
    "    filename = f\"{output_dir}/{name}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"  üíæ Saved: {name}.csv ({len(df):,} records)\")\n",
    "\n",
    "# Save completeness report\n",
    "completeness_df.to_csv(f\"{output_dir}/data_completeness_report.csv\", index=False)\n",
    "print(f\"  üíæ Saved: data_completeness_report.csv\")\n",
    "\n",
    "# Create summary report\n",
    "summary_report = {\n",
    "    'Analysis_Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'Golden_Profiles_Count': len(golden_profiles),\n",
    "    'Total_Samples': len(samples_golden),\n",
    "    'Total_Morphology_Records': len(morphology_golden),\n",
    "    'Total_Analyses': len(analyses_golden),\n",
    "    'Total_Sites': len(profile_loc_golden),\n",
    "    'Sample_Horizon_Link_Rate': f\"{link_rate:.1f}%\",\n",
    "    'Composite_Keys_Created': True,\n",
    "    'Data_Quality': 'HIGH - All profiles appear in all 5 tables'\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/analysis_summary.txt\", 'w') as f:\n",
    "    for key, value in summary_report.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "print(f\"  üíæ Saved: analysis_summary.txt\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 9: FINAL SUMMARY AND RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüéâ FINAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úÖ Successfully processed {len(golden_profiles)} golden profiles\")\n",
    "print(f\"‚úÖ Created composite keys for {len(samples_golden):,} samples\")\n",
    "print(f\"‚úÖ Linked {linked_samples:,} samples to horizons ({link_rate:.1f}% success rate)\")\n",
    "print(f\"‚úÖ All tables now have robust composite keys\")\n",
    "print(f\"‚úÖ Data ready for database import and analysis\")\n",
    "\n",
    "print(f\"\\nüìÅ Output files saved to: {output_dir}\")\n",
    "print(f\"\\nüéØ NEXT STEPS:\")\n",
    "print(f\"  1. Review the data completeness report\")\n",
    "print(f\"  2. Test additional queries on your clean data\")\n",
    "print(f\"  3. Import to database using composite keys as primary keys\")\n",
    "print(f\"  4. Expand to the additional 646 'good match' profiles when ready\")\n",
    "\n",
    "print(f\"\\nüåü You now have a clean, linked subset representing your highest quality soil data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samples table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Registo, N¬∫ Campo, Ano, Perfil, Campanha, Col√≥nia_Pais, Distrito, AmostraCrivada, Prov√≠ncia, AmostraNaoCrivada, Prateleira, Sala, Obs]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load original dataset\n",
    "samples = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/AmostrasAngolaTerrario.xlsx\")\n",
    "\n",
    "# This will show all rows that are duplicates (keep=False shows all duplicates, not just subsequent ones)\n",
    "duplicates1 = samples[samples.duplicated('Registo',keep=False)]\n",
    "print(duplicates1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for clarity\n",
    "samples.rename(columns={\n",
    "    'Registo': 'sample_id',\n",
    "    'N¬∫ Campo': 'site_info_id',\n",
    "    'Ano': 'year',\n",
    "    'Perfil': 'profile',\n",
    "    'Campanha': 'campaign',\n",
    "    'Col√≥nia_Pais': 'country',\n",
    "    'Distrito': 'district',\n",
    "    'AmostraCrivada': 'sample_sifted',\n",
    "    'AmostraNaoCrivada': 'sample_not_sifted',\n",
    "    'Prateleira': 'shelf',\n",
    "    'Sala': 'room'\n",
    "}, inplace=True)\n",
    "\n",
    "# Drop unused columns\n",
    "samples_cleaning = samples.drop(columns=[\n",
    "    'campaign', 'country', 'Prov√≠ncia', 'sample_not_sifted', 'sample_sifted', 'Obs'\n",
    "], errors='ignore')\n",
    "\n",
    "# # Add a new Primary Key ID column starting from 1\n",
    "# samples_cleaning.insert(0, 'sample_id', range(1, len(samples_cleaning) + 1))\n",
    "\n",
    "# Add empty FK columns\n",
    "samples_cleaning['lab_info_id'] = pd.NA\n",
    "samples_cleaning['horizon_id'] = pd.NA\n",
    "samples_cleaning['profile_record_id'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>year</th>\n",
       "      <th>profile</th>\n",
       "      <th>district</th>\n",
       "      <th>shelf</th>\n",
       "      <th>room</th>\n",
       "      <th>lab_info_id</th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>profile_record_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>630</td>\n",
       "      <td>172</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>139</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>631</td>\n",
       "      <td>173</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>139</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>174</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>139</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>633</td>\n",
       "      <td>175</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>139</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>687</td>\n",
       "      <td>1034</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>208</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id site_info_id    year profile district shelf room lab_info_id  \\\n",
       "0        630          172  1946.0     139   Huambo     1   22        <NA>   \n",
       "1        631          173  1946.0     139   Huambo     1   22        <NA>   \n",
       "2        632          174  1946.0     139   Huambo     1   22        <NA>   \n",
       "3        633          175  1946.0     139   Huambo     1   22        <NA>   \n",
       "4        687         1034  1946.0     208   Huambo     1   22        <NA>   \n",
       "\n",
       "  horizon_id profile_record_id  \n",
       "0       <NA>              <NA>  \n",
       "1       <NA>              <NA>  \n",
       "2       <NA>              <NA>  \n",
       "3       <NA>              <NA>  \n",
       "4       <NA>              <NA>  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_cleaning.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_id              int64\n",
       "site_info_id          object\n",
       "year                 float64\n",
       "profile               object\n",
       "district              object\n",
       "shelf                 object\n",
       "room                  object\n",
       "lab_info_id           object\n",
       "horizon_id            object\n",
       "profile_record_id     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_cleaning.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Duplicate sample_id values:\n",
      "Empty DataFrame\n",
      "Columns: [sample_id]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Ensure consistent types and formatting\n",
    "samples_cleaning['profile'] = samples_cleaning['profile'].astype(str).str.replace('/', '_').str.strip().str[:20]\n",
    "samples_cleaning['shelf'] = samples_cleaning['shelf'].astype(\"string\")\n",
    "\n",
    "#formatting site_info_id\n",
    "samples_cleaning['site_info_id'] = samples_cleaning['site_info_id'].astype(str).str.replace('/', '_').str.strip().str[:20]\n",
    "\n",
    "# Convert sample_id to string (no truncation unless necessary)\n",
    "samples_cleaning['sample_id'] = samples_cleaning['sample_id'].astype(str).str.strip()\n",
    "\n",
    "# Check for duplicates AFTER conversion\n",
    "duplicate_ids = samples_cleaning[samples_cleaning.duplicated('sample_id', keep=False)]\n",
    "print(\"üîç Duplicate sample_id values:\")\n",
    "print(duplicate_ids[['sample_id']])\n",
    "\n",
    "samples_cleaning1 = samples_cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking which profiles match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_id                    object\n",
       "site_info_id                 object\n",
       "year                        float64\n",
       "profile                      object\n",
       "district                     object\n",
       "shelf                string[python]\n",
       "room                         object\n",
       "lab_info_id                  object\n",
       "horizon_id                   object\n",
       "profile_record_id            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_cleaning1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>profile_record_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>shelf</th>\n",
       "      <th>room</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>630</td>\n",
       "      <td>172</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1946.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>631</td>\n",
       "      <td>173</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1946.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>174</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1946.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>633</td>\n",
       "      <td>175</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1946.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>687</td>\n",
       "      <td>1034</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>208</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1946.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id site_info_id profile_record_id profile horizon_id shelf room  \\\n",
       "0       630          172              <NA>     139       <NA>     1   22   \n",
       "1       631          173              <NA>     139       <NA>     1   22   \n",
       "2       632          174              <NA>     139       <NA>     1   22   \n",
       "3       633          175              <NA>     139       <NA>     1   22   \n",
       "4       687         1034              <NA>     208       <NA>     1   22   \n",
       "\n",
       "     year  \n",
       "0  1946.0  \n",
       "1  1946.0  \n",
       "2  1946.0  \n",
       "3  1946.0  \n",
       "4  1946.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reorder to match DB schema\n",
    "samples_check = samples_cleaning1[[\n",
    "    'sample_id',\n",
    "    'site_info_id',\n",
    "    'profile_record_id',\n",
    "    'profile',\n",
    "    'horizon_id',\n",
    "    'shelf',\n",
    "    'room',\n",
    "    'year'\n",
    "]]\n",
    "\n",
    "samples_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_id                    object\n",
       "site_info_id                 object\n",
       "profile_record_id            object\n",
       "profile                      object\n",
       "horizon_id                   object\n",
       "shelf                string[python]\n",
       "room                         object\n",
       "year                        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_check.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_check.to_csv(\"/Users/inesschwartz/Desktop/samples_check.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab_Info  table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Amostra', 'ID', 'Morfo_id', 'Analise-id', 'PERFIL', 'GR', 'COD_PROV',\n",
       "       'NA', 'LS', 'LI', 'EG', 'AG', 'AF', 'L', 'Argila', 'Eq_Hum', 'atm_1/3',\n",
       "       'atm_15', 'CACO3', 'Gesso', 'Fe livre', 'CO', 'N total', 'C/N', 'MO',\n",
       "       'P205 total', 'pH (H2O)', 'pH(KCL)', 'Ca++', 'Mg++', 'K+', 'Na+',\n",
       "       'Soma de bases', 'CTC', 'V', 'Cloretos', 'Sulfatos', 'Condutividade',\n",
       "       'S√≥dio sol√∫vel', 'EqMol (SiO2)', 'EqMol(Al2O3)', 'EqMol(Fe2O3)',\n",
       "       'SiO2/Al2O3', 'SiO2/Fe2O3', 'SiO2/R2O3', 'Fe2O3/Al2O3', 'FE2O3_TARG',\n",
       "       'FE2O3_LARG', 'CEC_ARG', 'Min_<0,002', 'Min_0,05-0,02', 'Min_0,2-0,05',\n",
       "       'Min_2-0,2', 'Confirmar', 'profile_clean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyses.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change column names\n",
    "analyses.rename(columns={\n",
    "    'Amostra': 'sample_id',\n",
    "    'Morfo_id': 'horizon_id',\n",
    "    'Analise-id': 'analysis_id',\n",
    "    'PERFIL': 'profile', #change to reference PK profile_record_id\n",
    "    'LS': 'upper_limit',\n",
    "    'LI': 'lower_limit',\n",
    "    'EG': 'EG',\n",
    "    'AG': 'thick_clay',\n",
    "    'AF': 'fine_clay',\n",
    "    'Argila': 'clay',\n",
    "    'L': 'silt',\n",
    "    'Gesso': 'gypsum',\n",
    "    'Fe livre': 'free_iron',\n",
    "    'CO':'organic_carbon',\n",
    "    'N total': 'total_N',\n",
    "    'MO': 'OM',\n",
    "    'Soma de bases': 'exchangable_bases_sum',\n",
    "    'CTC': 'CEC',\n",
    "    'Cloretos':'chlorides',\n",
    "    'Sulfatos':'sulfates',\n",
    "    'Condutividade':'conductivity',\n",
    "    'S√≥dio sol√∫vel':'soluble_sodium',\n",
    "    'P205 total': 'P205',\n",
    "    'pH (H2O)':'pH_H2O',\n",
    "    'pH(KCL)':'pH_KCL',\n",
    "    'MO': 'organic_material',\n",
    "    'atm_1/3': 'atm_1_3',\n",
    "    'Ca++': 'Ca',\n",
    "    'Mg++': 'Mg',\n",
    "    'Na+': 'Na',\n",
    "    'K+': 'K',\n",
    "    'Min_<0,002': 'Min_lt_0002',\n",
    "    'Min_0,05-0,02': 'Min_005_002',\n",
    "    'Min_0,2-0,05': 'Min_02_005',\n",
    "    'Min_2-0,2': 'Min_2_02',\n",
    "}, inplace=True)\n",
    "\n",
    "# Drop columns not needed\n",
    "analyses_drop = analyses.drop([\n",
    "    'EqMol (SiO2)', 'EqMol(Al2O3)', 'EqMol(Fe2O3)', 'SiO2/Al2O3', \n",
    "    'SiO2/Fe2O3', 'SiO2/R2O3', 'Fe2O3/Al2O3', 'FE2O3_TARG', \n",
    "    'FE2O3_LARG', 'CEC_ARG', 'GR', 'COD_PROV'\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new Primary Key ID column starting from 1\n",
    "#drop lab_sample_id if already exist error comes up\n",
    "# Check if 'lab_sample_id' exists and drop it\n",
    "if 'lab_sample_id' in analyses_drop.columns:\n",
    "    analyses_drop = analyses_drop.drop(columns=['lab_sample_id'])\n",
    "\n",
    "# Insert a new column, e.g., 'lab_sample_id' as a primary key starting from 1\n",
    "analyses_drop.insert(0, 'lab_sample_id', range(1, len(analyses_drop) + 1))\n",
    "\n",
    "\n",
    "#add minerology_id column\n",
    "analyses_drop['minerology_id'] = pd.NA\n",
    "#add soil_biology_id column (just in case)\n",
    "analyses_drop['soil_biology_id'] = pd.NA\n",
    "\n",
    "\n",
    "#Define the desired final column order\n",
    "final_columns = [\n",
    "    'lab_sample_id',\n",
    "    'ID',\n",
    "    'analysis_id',\n",
    "    'horizon_id',\n",
    "    'sample_id',\n",
    "    'profile',\n",
    "    'minerology_id',\n",
    "    'soil_biology_id',\n",
    "    'EG',\n",
    "    'thick_clay',\n",
    "    'fine_clay',\n",
    "    'silt',\n",
    "    'clay',\n",
    "    'Eq_Hum',\n",
    "    'atm_1_3',\n",
    "    'atm_15',\n",
    "    'CACO3',\n",
    "    'gypsum',\n",
    "    'free_iron',\n",
    "    'organic_carbon',\n",
    "    'total_N',\n",
    "    'P205',\n",
    "    'organic_material',\n",
    "    'pH_H2O',\n",
    "    'pH_KCL',\n",
    "    'Ca',\n",
    "    'Mg',\n",
    "    'Na',\n",
    "    'K',\n",
    "    'exchangable_bases_sum',\n",
    "    'CEC',\n",
    "    'V',\n",
    "    'conductivity',\n",
    "    'soluble_sodium',\n",
    "    'Min_lt_0002',\n",
    "    'Min_005_002',\n",
    "    'Min_02_005',\n",
    "    'Min_2_02',\n",
    "]\n",
    "\n",
    "analyses1 = analyses_drop[final_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Columns with NaN values:\n",
      "sample_id                  19\n",
      "minerology_id            7847\n",
      "soil_biology_id          7847\n",
      "EG                       4910\n",
      "thick_clay                448\n",
      "fine_clay                 390\n",
      "silt                      430\n",
      "clay                      420\n",
      "Eq_Hum                   2297\n",
      "atm_1_3                  7469\n",
      "atm_15                   5689\n",
      "CACO3                    7334\n",
      "gypsum                   7815\n",
      "free_iron                2792\n",
      "organic_carbon           2395\n",
      "total_N                  6259\n",
      "P205                     5464\n",
      "organic_material         2413\n",
      "pH_H2O                    684\n",
      "pH_KCL                   1749\n",
      "Ca                       3018\n",
      "Mg                       3025\n",
      "Na                       3024\n",
      "K                        2995\n",
      "exchangable_bases_sum    6740\n",
      "CEC                      2423\n",
      "V                        2476\n",
      "conductivity             7201\n",
      "soluble_sodium           1270\n",
      "Min_lt_0002              7517\n",
      "Min_005_002              7719\n",
      "Min_02_005               7723\n",
      "Min_2_02                 7810\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#address nulls \n",
    "# Show count of NaN values in each column\n",
    "nan_counts = analyses1.isna().sum()\n",
    "\n",
    "# Filter to show only columns with at least one NaN\n",
    "nan_columns = nan_counts[nan_counts > 0]\n",
    "\n",
    "# Print them\n",
    "print(\"üîç Columns with NaN values:\")\n",
    "print(nan_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting and checking for duplicates\n",
    "# Make a safe copy\n",
    "analyses1 = analyses_drop[final_columns].copy()\n",
    "\n",
    "# Convert sample_id to numeric, coercing errors to NaN\n",
    "analyses1['sample_id'] = pd.to_numeric(analyses1['sample_id'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN in sample_id to avoid int conversion error\n",
    "analyses1 = analyses1.dropna(subset=['sample_id'])\n",
    "\n",
    "# Convert to int, then to str\n",
    "analyses1['sample_id'] = analyses1['sample_id'].astype(int).astype(str)\n",
    "\n",
    "# Clean the profile column\n",
    "analyses1['profile'] = (\n",
    "    analyses1['profile']\n",
    "    .astype(str)\n",
    "    .str.replace('/', '_')\n",
    "    .str.strip()\n",
    "    .str[:20]\n",
    ")\n",
    "\n",
    "# Alias to another variable\n",
    "analyses2 = analyses1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10337' '11608' '3497' '3498' '2923' '2924' '2925' '2926' '2927' '2928'\n",
      " '2929' '2930' '16355' '16356' '419' '420' '15682' '13817' '75' '13965'\n",
      " '3418' '9230' '8639' '3529']\n"
     ]
    }
   ],
   "source": [
    "# Filter out and print duplicated samples (including the first occurrence)\n",
    "duplicated_values = analyses2['sample_id'][analyses2['sample_id'].duplicated()].unique()\n",
    "print(duplicated_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sample ID corrections applied.\n"
     ]
    }
   ],
   "source": [
    "# 1. Ensure sample_id and analysis_id are strings\n",
    "analyses2['sample_id'] = analyses2['sample_id'].astype(str).str.strip()\n",
    "analyses2['analysis_id'] = analyses2['analysis_id'].astype(str).str.strip()\n",
    "\n",
    "# 2. Correct duplicates for sample_id '11608'\n",
    "mask_11608 = analyses2['sample_id'] == '11608'\n",
    "analyses2.loc[mask_11608, 'sample_id'] = [\n",
    "    f\"11608_{i+1}\" for i in range(mask_11608.sum())\n",
    "]\n",
    "\n",
    "# 3. Remove duplicate for sample_id '3497' (keep first occurrence only)\n",
    "duplicates_3497 = analyses2[analyses2['sample_id'] == '3497']\n",
    "if len(duplicates_3497) > 1:\n",
    "    indices_to_drop = duplicates_3497.index[1:]\n",
    "    analyses2 = analyses2.drop(indices_to_drop)\n",
    "\n",
    "# 4. Correct sample_id ‚Üí analysis_id assignments for specific sample_ids\n",
    "id_map = {\n",
    "    'UZ_24c/60_1_1': '8639',\n",
    "    'UZ_24c/60_2_1': '8640',\n",
    "    'UZ_24c/60_3_1': '8641',\n",
    "    'UZ_24c/60_4_1': '8642',\n",
    "    'UZ_24c/60_5_1': '8643'\n",
    "}\n",
    "\n",
    "# Apply mapping to update analysis_id based on sample_id\n",
    "analyses2['analysis_id'] = analyses2.apply(\n",
    "    lambda row: id_map.get(row['sample_id'], row['analysis_id']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Sample ID corrections applied.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóë Dropped second occurrence of sample_id 2923\n",
      "üóë Dropped second occurrence of sample_id 2924\n",
      "üóë Dropped second occurrence of sample_id 2925\n",
      "üóë Dropped second occurrence of sample_id 2926\n",
      "üóë Dropped second occurrence of sample_id 2927\n",
      "üóë Dropped second occurrence of sample_id 2928\n",
      "üóë Dropped second occurrence of sample_id 2929\n",
      "üóë Dropped second occurrence of sample_id 2930\n"
     ]
    }
   ],
   "source": [
    "#drop unwanted/un-needed duplicates or erroneous reccords\n",
    "\n",
    "# Drop second row where sample_id == '3497'\n",
    "indices = analyses2.index[analyses2['sample_id'] == '3497'].tolist()\n",
    "if len(indices) > 1:\n",
    "    analyses2 = analyses2.drop(indices[1])\n",
    "\n",
    "# Drop second row where sample_id == '3497'\n",
    "indices = analyses2.index[analyses2['sample_id'] == '13817'].tolist()\n",
    "if len(indices) > 1:\n",
    "    analyses2 = analyses2.drop(indices[1])\n",
    "\n",
    "# drop second row where sample_id == '8639'\n",
    "indices = analyses2.index[analyses2['sample_id'] == '8639'].tolist()\n",
    "if len(indices) > 1:\n",
    "    analyses2 = analyses2.drop(indices[1])\n",
    "\n",
    "#Drop second row where sample_id =='2923-2930'\n",
    "# Ensure sample_id is string for consistent comparison\n",
    "analyses2['sample_id'] = analyses2['sample_id'].astype(str)\n",
    "\n",
    "# Define the sample_ids to clean duplicates for\n",
    "target_ids = [str(i) for i in range(2923, 2931)]\n",
    "\n",
    "# Identify and drop the second occurrence of each\n",
    "for sid in target_ids:\n",
    "    matches = analyses2.index[analyses2['sample_id'] == sid].tolist()\n",
    "    if len(matches) > 1:\n",
    "        # Drop the second occurrence (index 1 in the list)\n",
    "        analyses2 = analyses2.drop(matches[1])\n",
    "        print(f\"üóë Dropped second occurrence of sample_id {sid}\")\n",
    "\n",
    "\n",
    "# Ensure sample_id is string for correct matching\n",
    "analyses2['sample_id'] = analyses2['sample_id'].astype(str)\n",
    "\n",
    "# List of sample_ids to drop (as strings)\n",
    "sample_ids_to_drop = [\n",
    "    '4247', '4248', '4249', '4250', '4251', '4252', '4253', '4254',  # invalid samples\n",
    "    '5533', '6636', '6637', '6638', '6639',\n",
    "    '6875', '7361',\n",
    "    '3012', '3013', '3014', '3015', '3016',\n",
    "    '3049', '3050', '3051', '3052', '3053', '3054', '419', '420', '421', '422'\n",
    "]\n",
    "\n",
    "# Drop rows where sample_id is in the list\n",
    "analyses2 = analyses2[~analyses2['sample_id'].isin(sample_ids_to_drop)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Manual sample_id corrections applied.\n",
      "Remaining rows: 7798\n",
      "Missing sample_id entries: 0\n"
     ]
    }
   ],
   "source": [
    "# Ensure both 'lab_sample_id' and 'sample_id' columns exist\n",
    "assert 'lab_sample_id' in analyses2.columns, \"Missing column: lab_sample_id\"\n",
    "assert 'sample_id' in analyses2.columns, \"Missing column: sample_id\"\n",
    "\n",
    "# Step 1: Ensure lab_sample_id is string so comparison works\n",
    "analyses2['lab_sample_id'] = analyses2['lab_sample_id'].astype(str)\n",
    "\n",
    "# Step 2: Manual corrections mapping\n",
    "manual_corrections = {\n",
    "    '66': 10377,\n",
    "    '4219': 3497,\n",
    "    '4221': 3498,   # same sample, different depths\n",
    "    '4222': 3499,\n",
    "    '4747': 16255,\n",
    "    '4748': 16256,\n",
    "    '5010': 15862,\n",
    "    '6802': 13695,\n",
    "    '7255': 8230,\n",
    "    '7686': 8529,\n",
    "    '6875': 3419,\n",
    "    '6638': 75_1,\n",
    "}\n",
    "\n",
    "# Step 3: Apply manual corrections\n",
    "for lab_id, sample_id in manual_corrections.items():\n",
    "    matches = analyses2['lab_sample_id'] == lab_id\n",
    "    if matches.sum() == 0:\n",
    "        print(f\"‚ö†Ô∏è Warning: lab_sample_id {lab_id} not found in data.\")\n",
    "    else:\n",
    "        analyses2.loc[matches, 'sample_id'] = sample_id\n",
    "\n",
    "# Step 4: Summary after correction\n",
    "print(\"‚úÖ Manual sample_id corrections applied.\")\n",
    "print(f\"Remaining rows: {len(analyses2)}\")\n",
    "print(f\"Missing sample_id entries: {analyses2['sample_id'].isna().sum()}\")\n",
    "\n",
    "# Optional: Show which rows are still missing sample_id\n",
    "missing_sample_ids = analyses2[analyses2['sample_id'].isna()]\n",
    "if not missing_sample_ids.empty:\n",
    "    print(\"üîç Rows with missing sample_id after correction:\")\n",
    "    print(missing_sample_ids[['lab_sample_id', 'sample_id']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Missing sample_id in 0 rows.\n",
      "üîç lab_sample_id values with missing sample_id:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Find rows with missing sample_id\n",
    "missing_sample_id_rows = analyses2[analyses2['sample_id'].isna()]\n",
    "\n",
    "# Print how many are missing\n",
    "print(f\"‚ùå Missing sample_id in {len(missing_sample_id_rows)} rows.\")\n",
    "\n",
    "# Show the unique lab_sample_id values that are missing a sample_id\n",
    "print(\"üîç lab_sample_id values with missing sample_id:\")\n",
    "print(missing_sample_id_rows['lab_sample_id'].unique())\n",
    "\n",
    "# Optional: Save to CSV for review\n",
    "#missing_sample_id_rows.to_csv(\"missing_sample_ids.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN sample_id rows: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"NaN sample_id rows:\", analyses2['sample_id'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing sample_id (NaN only)\n",
    "analyses2 = analyses2[analyses2['sample_id'].notna()].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Updated analyses2 has 7798 rows.\n"
     ]
    }
   ],
   "source": [
    "print(f\"‚úÖ Updated analyses2 has {len(analyses2)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses3 = analyses2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Filter out and print duplicated samples (including the first occurrence)\n",
    "duplicated_values = analyses3['sample_id'][analyses3['sample_id'].duplicated()].unique()\n",
    "print(duplicated_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Number of null-like sample_id values: 0\n",
      "‚ö†Ô∏è Unique null-like sample_id values found:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#double checking null sample_id's\n",
    "# Step 1: Ensure sample_id is a string for consistent comparison\n",
    "analyses3['sample_id'] = analyses3['sample_id'].astype(str).str.strip()\n",
    "\n",
    "# Step 2: Define mask for all \"null-like\" values\n",
    "null_like_mask = analyses3['sample_id'].isin(['', '0', 'NULL', 'NaN', 'nan', 'None']) | analyses3['sample_id'].isna()\n",
    "\n",
    "# Step 3: Count and list unique problematic values\n",
    "null_sample_ids = analyses3[null_like_mask]['sample_id'].unique()\n",
    "null_count = null_like_mask.sum()\n",
    "\n",
    "# Step 4: Output results\n",
    "print(f\"üîç Number of null-like sample_id values: {null_count}\")\n",
    "print(\"‚ö†Ô∏è Unique null-like sample_id values found:\")\n",
    "print(null_sample_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## joining elemental analyses to analyses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Found 0 rows with NaN sample_id:\n",
      "Empty DataFrame\n",
      "Columns: [sample_id, Soil Profile, Code, Code.1, Depht, Sampling Date, Mg, Al, Si, P, S, Cl, K, Ca, Ti, V, Cr, Mn, Fe, Co, Ni, Cu, Zn, As, Se, Rb, Sr, Zr, Nb, Mo, Cd, Sn, Sb, Ba, Ta, W, Pt, Au, Hg, Tl, Pb, Bi, Th, U]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "# Rename and clean sample_id as before\n",
    "elemental_analyses.rename(columns={'Lab. Code': 'sample_id'}, inplace=True)\n",
    "\n",
    "# Convert to string to clean text\n",
    "sample_id_cleaned = elemental_analyses['sample_id'].astype(str)\n",
    "sample_id_cleaned = sample_id_cleaned.str.replace('C-', '', regex=False)\n",
    "sample_id_cleaned = sample_id_cleaned.str.replace(' MNL', '', regex=False)\n",
    "\n",
    "# Convert to numeric with coercion (NaNs will appear here)\n",
    "elemental_analyses['sample_id'] = pd.to_numeric(sample_id_cleaned, errors='coerce')\n",
    "\n",
    "# üîç Show rows where sample_id is NaN\n",
    "nan_sample_ids = elemental_analyses[elemental_analyses['sample_id'].isna()]\n",
    "print(f\"‚ùå Found {len(nan_sample_ids)} rows with NaN sample_id:\")\n",
    "print(nan_sample_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'Lab. Code' to 'sample_id'\n",
    "elemental_analyses.rename(columns={'Lab. Code': 'sample_id'}, inplace=True)\n",
    "\n",
    "# Temporarily convert to string to do the replacements\n",
    "sample_id_cleaned = elemental_analyses['sample_id'].astype(str)\n",
    "sample_id_cleaned = sample_id_cleaned.str.replace('C-', '', regex=False)\n",
    "sample_id_cleaned = sample_id_cleaned.str.replace(' MNL', '', regex=False)\n",
    "\n",
    "# Convert cleaned values to numeric safely\n",
    "elemental_analyses['sample_id'] = pd.to_numeric(sample_id_cleaned, errors='coerce')\n",
    "\n",
    "\n",
    "# Convert to integer and then string for uniformity\n",
    "elemental_analyses['sample_id'] = elemental_analyses['sample_id'].astype(int).astype(str)\n",
    "\n",
    "# Rename 'Code' to 'field_sample_code'\n",
    "elemental_analyses.rename(columns={'Code': 'field_sample_code'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses2['sample_id'] = analyses3['sample_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "print (analyses3['sample_id'].dtype)\n",
    "print (elemental_analyses['sample_id'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert sample_id to string (no truncation unless necessary)\n",
    "elemental_analyses['sample_id'] = elemental_analyses['sample_id'].astype(str).str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample IDs in elemental_analyses not found in analyses2:\n",
      "['16105' '16217' '16033' '16231' '16208' '16274' '16225' '15731' '16078'\n",
      " '15786' '15693' '15678' '15959' '16372' '16418' '16459' '16477' '15498'\n",
      " '15437' '12749' '13109' '14337' '14319' '15484' '15508' '15614' '15580'\n",
      " '15582' '15606' '15463' '14496' '11343' '17892' '16976' '17269' '18248'\n",
      " '16946' '17686' '17728' '17415' '17642' '18212' '17016' '17282' '18429'\n",
      " '17340' '18824' '18448' '8721' '7313' '5502' '5392' '7317' '8626' '7193'\n",
      " '7254' '7293' '8240' '15397' '14403' '14983' '16126' '15059' '15322'\n",
      " '15268' '14808' '15106']\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "# Check which sample_ids in elemental_analyses are NOT in analyses_ready\n",
    "missing_ids = elemental_analyses[~elemental_analyses['sample_id'].isin(analyses3['sample_id'])]\n",
    "\n",
    "# Display them\n",
    "print(\"Sample IDs in elemental_analyses not found in analyses2:\")\n",
    "print(missing_ids['sample_id'].unique())\n",
    "\n",
    "print(len(missing_ids['sample_id'].unique()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEFT JOIN ELEMENTAL ANALYSE TO ANALYSE TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab_sample_id</th>\n",
       "      <th>ID</th>\n",
       "      <th>analysis_id</th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>minerology_id</th>\n",
       "      <th>soil_biology_id</th>\n",
       "      <th>EG</th>\n",
       "      <th>thick_clay</th>\n",
       "      <th>...</th>\n",
       "      <th>Ta</th>\n",
       "      <th>W</th>\n",
       "      <th>Pt</th>\n",
       "      <th>Au</th>\n",
       "      <th>Hg</th>\n",
       "      <th>Tl</th>\n",
       "      <th>Pb</th>\n",
       "      <th>Bi</th>\n",
       "      <th>Th</th>\n",
       "      <th>U</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>10999</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.700001</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>11000</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.799999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>11001</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>11002</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.599998</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>B_101/62_5_21</td>\n",
       "      <td>B_101/62_5_2</td>\n",
       "      <td>11003</td>\n",
       "      <td>101_62</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.799999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  lab_sample_id  ID    analysis_id    horizon_id sample_id profile  \\\n",
       "0             1  21   B_101/62_1_1  B_101/62_1_1     10999  101_62   \n",
       "1             2  22   B_101/62_2_1  B_101/62_2_1     11000  101_62   \n",
       "2             3  23   B_101/62_3_1  B_101/62_3_1     11001  101_62   \n",
       "3             4  24   B_101/62_4_1  B_101/62_4_1     11002  101_62   \n",
       "4             5  25  B_101/62_5_21  B_101/62_5_2     11003  101_62   \n",
       "\n",
       "  minerology_id soil_biology_id  EG  thick_clay  ...  Ta   W  Pt  Au  Hg  Tl  \\\n",
       "0          <NA>            <NA> NaN   61.700001  ... NaN NaN NaN NaN NaN NaN   \n",
       "1          <NA>            <NA> NaN   52.799999  ... NaN NaN NaN NaN NaN NaN   \n",
       "2          <NA>            <NA> NaN   42.500000  ... NaN NaN NaN NaN NaN NaN   \n",
       "3          <NA>            <NA> NaN   42.599998  ... NaN NaN NaN NaN NaN NaN   \n",
       "4          <NA>            <NA> NaN   36.799999  ... NaN NaN NaN NaN NaN NaN   \n",
       "\n",
       "   Pb  Bi  Th   U  \n",
       "0 NaN NaN NaN NaN  \n",
       "1 NaN NaN NaN NaN  \n",
       "2 NaN NaN NaN NaN  \n",
       "3 NaN NaN NaN NaN  \n",
       "4 NaN NaN NaN NaN  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged1 = analyses3.merge(elemental_analyses, on='sample_id', how='left')\n",
    "merged1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     lab_sample_id    ID    analysis_id     horizon_id sample_id profile  \\\n",
      "1361          1362  8471  Cb_134/59_6_1  Cb_134/59_6_1      6940  134_59   \n",
      "\n",
      "     minerology_id soil_biology_id  EG  thick_clay  ...   K  \\\n",
      "1361          <NA>            <NA> NaN        31.1  ... NaN   \n",
      "\n",
      "      exchangable_bases_sum  CEC   V  conductivity  soluble_sodium  \\\n",
      "1361                    NaN  NaN NaN           NaN             0.0   \n",
      "\n",
      "      Min_lt_0002  Min_005_002  Min_02_005  Min_2_02  \n",
      "1361          NaN          NaN         NaN       NaN  \n",
      "\n",
      "[1 rows x 38 columns]\n",
      "   sample_id Soil Profile field_sample_code Code.1      Depht Sampling Date  \\\n",
      "80      6940      P134/59          A-320/59  S-492  1.30-1.70    1954-08-06   \n",
      "\n",
      "             Mg             Al             Si      P  ...    Ta   W  Pt  Au  \\\n",
      "80  1457.666667  106266.333333  308859.666667  468.0  ...  26.0 NaN NaN NaN   \n",
      "\n",
      "           Hg   Tl         Pb  Bi         Th   U  \n",
      "80  10.666667  4.0  25.333333 NaN  12.333333 NaN  \n",
      "\n",
      "[1 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "print(analyses3[analyses3['sample_id'] == '6940'])\n",
    "print(elemental_analyses[elemental_analyses['sample_id'] == '6940'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of each column in merged1:\n",
      "\n",
      "lab_sample_id     object\n",
      "analysis_id       object\n",
      "horizon_id        object\n",
      "sample_id         object\n",
      "EG               float64\n",
      "                  ...   \n",
      "Tl               float64\n",
      "Pb               float64\n",
      "Bi               float64\n",
      "Th               float64\n",
      "U                float64\n",
      "Length: 70, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Define final column list\n",
    "final_columns = [\n",
    "    'lab_sample_id', 'analysis_id', 'horizon_id','sample_id', 'EG', 'thick_clay', 'fine_clay', 'silt', 'clay', 'Eq_Hum', 'atm_1/3', 'atm_15',\n",
    "    'CACO3', 'gypsum', 'free_iron', 'organic_carbon', 'total_N', 'P205', 'organic_material', 'pH_H2O', 'pH_KCL',\n",
    "    'Ca++', 'Mg++', 'Na+', 'K+', 'exchangable_bases_sum', 'CEC', 'V', 'conductivity', 'soluble_sodium', 'Min_<0,002',\n",
    "    'Min_0,05-0,02', 'Min_0,2-0,05', 'Min_2-0,2', \n",
    "    'field_sample_code', 'Depth', 'Al', 'Si', 'P', 'S', 'Cl', 'Ti', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn',\n",
    "    'As', 'Se', 'Rb', 'Sr', 'Zr', 'Nb', 'Mo', 'Cd', 'Sn', 'Sb', 'Ba', 'Ta', 'W', 'Pt', 'Au', 'Hg', 'Tl', 'Pb', 'Bi',\n",
    "    'Th', 'U',\n",
    "]\n",
    "\n",
    "# Ensure all columns exist in the DataFrame\n",
    "for col in final_columns:\n",
    "    if col not in merged1.columns:\n",
    "        merged1[col] = pd.NA  # or np.nan if preferred\n",
    "\n",
    "# Reorder DataFrame columns\n",
    "merged1 = merged1[final_columns]\n",
    "\n",
    "# Check and print the datatypes of each column\n",
    "print(\"Data types of each column in merged1:\\n\")\n",
    "print(merged1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check/Change datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab_sample_id     object\n",
      "analysis_id       object\n",
      "horizon_id        object\n",
      "sample_id         object\n",
      "EG               float64\n",
      "                  ...   \n",
      "Tl               float64\n",
      "Pb               float64\n",
      "Bi               float64\n",
      "Th               float64\n",
      "U                float64\n",
      "Length: 70, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Change PK (sample_id) to a consistent datatype: Converted to string and truncate to 20 characters (similar to VARCHAR(20))\n",
    "merged1['sample_id'] = merged1['sample_id'].astype(str).str.strip().str[:20]\n",
    "merged1['lab_sample_id'] = merged1['lab_sample_id'].astype(str).str.strip().str[:20]\n",
    "\n",
    "print(merged1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and/or add FK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'lab_sample_id' exists and drop it\n",
    "if 'lab_sample_id' in merged1.columns:\n",
    "    merged1 = merged1.drop(columns=['lab_sample_id'])\n",
    "\n",
    "# Insert a new column, e.g., 'lab_sample_id' as a primary key starting from 1\n",
    "merged1.insert(0, 'lab_sample_id', range(1, len(merged1) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab_sample_id</th>\n",
       "      <th>analysis_id</th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>EG</th>\n",
       "      <th>thick_clay</th>\n",
       "      <th>fine_clay</th>\n",
       "      <th>silt</th>\n",
       "      <th>clay</th>\n",
       "      <th>Eq_Hum</th>\n",
       "      <th>...</th>\n",
       "      <th>Ta</th>\n",
       "      <th>W</th>\n",
       "      <th>Pt</th>\n",
       "      <th>Au</th>\n",
       "      <th>Hg</th>\n",
       "      <th>Tl</th>\n",
       "      <th>Pb</th>\n",
       "      <th>Bi</th>\n",
       "      <th>Th</th>\n",
       "      <th>U</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>10999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.700001</td>\n",
       "      <td>32.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>11000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.799999</td>\n",
       "      <td>35.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>11.4</td>\n",
       "      <td>6.4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>11001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>46.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>11.1</td>\n",
       "      <td>6.3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>11002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.599998</td>\n",
       "      <td>41.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15.4</td>\n",
       "      <td>5.2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B_101/62_5_21</td>\n",
       "      <td>B_101/62_5_2</td>\n",
       "      <td>11003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.799999</td>\n",
       "      <td>47.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>14.5</td>\n",
       "      <td>7.3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lab_sample_id    analysis_id    horizon_id sample_id  EG  thick_clay  \\\n",
       "0              1   B_101/62_1_1  B_101/62_1_1     10999 NaN   61.700001   \n",
       "1              2   B_101/62_2_1  B_101/62_2_1     11000 NaN   52.799999   \n",
       "2              3   B_101/62_3_1  B_101/62_3_1     11001 NaN   42.500000   \n",
       "3              4   B_101/62_4_1  B_101/62_4_1     11002 NaN   42.599998   \n",
       "4              5  B_101/62_5_21  B_101/62_5_2     11003 NaN   36.799999   \n",
       "\n",
       "   fine_clay  silt  clay  Eq_Hum  ...  Ta   W  Pt  Au  Hg  Tl  Pb  Bi  Th   U  \n",
       "0       32.8   0.2   5.3     4.6  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "1       35.1   0.7  11.4     6.4  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "2       46.2   0.2  11.1     6.3  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "3       41.8   0.2  15.4     5.2  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "4       47.5   1.2  14.5     7.3  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged1.to_csv(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_clean/analyses_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soil Profile Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##rename columns for clarity add FK/PK\n",
    "#create copy of samples\n",
    "soil_profile_record = samples.copy()\n",
    "# Rename columns in the DataFrame\n",
    "samples.rename(columns={\n",
    "    'Registo': 'sample_id',\n",
    "    'N¬∫ Campo': 'site_info_id',\n",
    "    'Ano': 'year',\n",
    "    'Perfil': 'profile',\n",
    "    'Campanha': 'campaign',\n",
    "    'Col√≥nia_Pais': 'country',\n",
    "    'Distrito': 'district',\n",
    "    'AmostraCrivada': 'sample_sifted',\n",
    "    'AmostraNaoCrivada': 'sample_not_sifted',\n",
    "    'Prateleira': 'shelf',\n",
    "    'Sala': 'room'\n",
    "}, inplace=True)\n",
    "\n",
    "soil_profile_cleaning2 = soil_profile_record\n",
    "# Add missing columns\n",
    "soil_profile_cleaning2['site_info_id'] = pd.NA\n",
    "soil_profile_cleaning2['sample_id'] = pd.NA \n",
    "soil_profile_cleaning2['soil_type_id'] = pd.NA \n",
    "soil_profile_cleaning2['site_info_id'] = soil_profile_cleaning2['site_info_id'].astype(str)\n",
    "\n",
    "# Add a new Primary Key ID column starting from 1\n",
    "# Check if 'lab_sample_id' exists and drop it\n",
    "if 'profile_record_id' in soil_profile_cleaning2.columns:\n",
    "    soil_profile_cleaning2 = soil_profile_cleaning2.drop(columns=['profile_record_id'])\n",
    "\n",
    "# Insert a new column, e.g., 'lab_sample_id' as a primary key starting from 1\n",
    "soil_profile_cleaning2.insert(0, 'profile_record_id', range(1, len(soil_profile_cleaning2) + 1))\n",
    "\n",
    "# Reorder columns to match SAMPLES schema\n",
    "profile_record_clean = soil_profile_cleaning2[[\n",
    "    'profile_record_id',\n",
    "    'profile',\n",
    "    'site_info_id',\n",
    "    'sample_id',\n",
    "    'soil_type_id'\n",
    "]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure consistent data types and formatting\n",
    "\n",
    "# correct datatypes\n",
    "#profile to string\n",
    "profile_record_clean['profile'] = profile_record_clean['profile'].astype(str).str.strip().str[:20]\n",
    "\n",
    "#sample_id to string\n",
    "# Convert cleaned values to numeric safely\n",
    "# Convert sample_id to numeric first (coerce errors to NaN)\n",
    "profile_record_clean['sample_id'] = pd.to_numeric(profile_record_clean['sample_id'], errors='coerce')\n",
    "# Then convert to string (will keep NaNs as <NA>)\n",
    "profile_record_clean['sample_id'] = profile_record_clean['sample_id'].astype('Int64').astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<NA>']\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates and nulls\n",
    "# Filter out and print duplicated samples (including the first occurrence)\n",
    "duplicated_values = profile_record_clean['sample_id'][profile_record_clean['sample_id'].duplicated()].unique()\n",
    "print(duplicated_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "profile_record_id     int64\n",
       "profile              object\n",
       "site_info_id         object\n",
       "sample_id            object\n",
       "soil_type_id         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_record_clean.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the result\n",
    "profile_record_clean.head()\n",
    "profile_record_clean.to_csv(\"/Users/inesschwartz/Desktop/profile_record_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Site info table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>ID</th>\n",
       "      <th>profile</th>\n",
       "      <th>X_coord</th>\n",
       "      <th>Y_coord</th>\n",
       "      <th>land_cover_id</th>\n",
       "      <th>climate_id</th>\n",
       "      <th>geology_id</th>\n",
       "      <th>topo_feature_id</th>\n",
       "      <th>sampling_date</th>\n",
       "      <th>districts_id</th>\n",
       "      <th>district</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2770</td>\n",
       "      <td>1_57</td>\n",
       "      <td>12.161278</td>\n",
       "      <td>-15.222598</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Namibe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>1_59</td>\n",
       "      <td>12.575775</td>\n",
       "      <td>-4.866986</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Cabinda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1618</td>\n",
       "      <td>1_61</td>\n",
       "      <td>15.098840</td>\n",
       "      <td>-11.225411</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Cuanza Sul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>881</td>\n",
       "      <td>1_63</td>\n",
       "      <td>17.081955</td>\n",
       "      <td>-9.274587</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Malanje</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1750</td>\n",
       "      <td>1_64</td>\n",
       "      <td>20.788116</td>\n",
       "      <td>-11.568683</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Moxico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   site_info_id    ID profile    X_coord    Y_coord land_cover_id climate_id  \\\n",
       "0             1  2770    1_57  12.161278 -15.222598          <NA>       <NA>   \n",
       "1             2    48    1_59  12.575775  -4.866986          <NA>       <NA>   \n",
       "2             3  1618    1_61  15.098840 -11.225411          <NA>       <NA>   \n",
       "3             4   881    1_63  17.081955  -9.274587          <NA>       <NA>   \n",
       "4             5  1750    1_64  20.788116 -11.568683          <NA>       <NA>   \n",
       "\n",
       "  geology_id topo_feature_id sampling_date districts_id    district  \n",
       "0       <NA>            <NA>          <NA>         <NA>      Namibe  \n",
       "1       <NA>            <NA>          <NA>         <NA>     Cabinda  \n",
       "2       <NA>            <NA>          <NA>         <NA>  Cuanza Sul  \n",
       "3       <NA>            <NA>          <NA>         <NA>     Malanje  \n",
       "4       <NA>            <NA>          <NA>         <NA>      Moxico  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "site_info_cleaning = profile_loc\n",
    "\n",
    "site_info_cleaning.rename(columns={\n",
    "    'PERFIL': 'profile',\n",
    "    'X_COORD': 'X_coord',\n",
    "    'Y_COORD': 'Y_coord', \n",
    "    'PRO': 'district'\n",
    "}, inplace=True)\n",
    "\n",
    "# Add a new Primary Key ID column starting from 1\n",
    "site_info_cleaning.insert(0, 'site_info_id', range(1, len(site_info_cleaning) + 1))\n",
    "\n",
    "\n",
    "# Add missing columns\n",
    "site_info_cleaning['land_cover_id'] = pd.NA\n",
    "site_info_cleaning['climate_id'] = pd.NA  \n",
    "site_info_cleaning['geology_id'] = pd.NA\n",
    "site_info_cleaning['topo_feature_id'] = pd.NA  \n",
    "site_info_cleaning['sampling_date'] = pd.NA # might leave out...\n",
    "site_info_cleaning['districts_id'] = pd.NA  \n",
    "\n",
    "# Function to remove accents\n",
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        return ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    return text\n",
    "\n",
    "# Apply to all object (string) columns\n",
    "for col in site_info_cleaning.select_dtypes(include='object').columns:\n",
    "    site_info_cleaning[col] = site_info_cleaning[col].apply(remove_accents)\n",
    "\n",
    "# Replace / with _ and strip/shorten 'profile' BEFORE slicing into site_info_clean\n",
    "site_info_cleaning['profile'] = (\n",
    "    site_info_cleaning['profile']\n",
    "    .astype(str)\n",
    "    .str.replace('/', '_')\n",
    "    .str.strip()\n",
    "    .str[:20]\n",
    ")\n",
    "\n",
    "# Then select columns\n",
    "site_info_clean = site_info_cleaning[[\n",
    "    'site_info_id',\n",
    "    'ID',\n",
    "    'profile',\n",
    "    'X_coord',\n",
    "    'Y_coord',\n",
    "    'land_cover_id',\n",
    "    'climate_id',\n",
    "    'geology_id',\n",
    "    'topo_feature_id',\n",
    "    'sampling_date',\n",
    "    'districts_id',\n",
    "    'district'\n",
    "]]\n",
    "\n",
    "\n",
    "site_info_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_info_clean.to_csv(\"/Users/inesschwartz/Desktop/site_info1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphology Horizon Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID1</th>\n",
       "      <th>Morfo_id</th>\n",
       "      <th>Amostra</th>\n",
       "      <th>Perfil</th>\n",
       "      <th>Agrupamento</th>\n",
       "      <th>REF</th>\n",
       "      <th>Pro</th>\n",
       "      <th>CM</th>\n",
       "      <th>Limite Superior</th>\n",
       "      <th>Limite inferior</th>\n",
       "      <th>...</th>\n",
       "      <th>Quantidade de ra√≠zes</th>\n",
       "      <th>Di√¢metro de ra√≠zes</th>\n",
       "      <th>Grau de humidade</th>\n",
       "      <th>Horizonte de diagn√≥stico</th>\n",
       "      <th>Propriedade de diagn√≥stico</th>\n",
       "      <th>Designa√ß√£o do horizonte</th>\n",
       "      <th>Nitidez do limite</th>\n",
       "      <th>Observa√ßoes</th>\n",
       "      <th>Unnamed: 46</th>\n",
       "      <th>Confirmar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38</td>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>10999.0</td>\n",
       "      <td>101/62</td>\n",
       "      <td>B 74</td>\n",
       "      <td>74.0</td>\n",
       "      <td>Bi√©</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Muitas finas e bastantes m√©dias</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>101/62</td>\n",
       "      <td>B 74</td>\n",
       "      <td>74.0</td>\n",
       "      <td>Bi√©</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Bastantes finas e m√©dias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>11001.0</td>\n",
       "      <td>101/62</td>\n",
       "      <td>B 74</td>\n",
       "      <td>74.0</td>\n",
       "      <td>Bi√©</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Algumas finas e m√©dias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>11002.0</td>\n",
       "      <td>101/62</td>\n",
       "      <td>B 74</td>\n",
       "      <td>74.0</td>\n",
       "      <td>Bi√©</td>\n",
       "      <td>4.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Poucas finas, algumas m√©dias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>B_101/62_5_2</td>\n",
       "      <td>11003.2</td>\n",
       "      <td>101/62</td>\n",
       "      <td>B 74</td>\n",
       "      <td>74.0</td>\n",
       "      <td>Bi√©</td>\n",
       "      <td>5.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Raras</td>\n",
       "      <td>M√©dias e grossas</td>\n",
       "      <td>Seco a h√∫mido</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID1      Morfo_id  Amostra  Perfil Agrupamento   REF  Pro   CM  \\\n",
       "0   38  B_101/62_1_1  10999.0  101/62        B 74  74.0  Bi√©  1.0   \n",
       "1   39  B_101/62_2_1  11000.0  101/62        B 74  74.0  Bi√©  2.0   \n",
       "2   40  B_101/62_3_1  11001.0  101/62        B 74  74.0  Bi√©  3.0   \n",
       "3   41  B_101/62_4_1  11002.0  101/62        B 74  74.0  Bi√©  4.0   \n",
       "4   42  B_101/62_5_2  11003.2  101/62        B 74  74.0  Bi√©  5.0   \n",
       "\n",
       "   Limite Superior  Limite inferior  ...  \\\n",
       "0              0.0             11.0  ...   \n",
       "1             11.0             28.0  ...   \n",
       "2             28.0             54.0  ...   \n",
       "3             54.0             90.0  ...   \n",
       "4             90.0            160.0  ...   \n",
       "\n",
       "                           Quantidade de ra√≠zes Di√¢metro de ra√≠zes  \\\n",
       "0               Muitas finas e bastantes m√©dias                NaN   \n",
       "1      Bastantes finas e m√©dias e raras grossas                NaN   \n",
       "2        Algumas finas e m√©dias e raras grossas                NaN   \n",
       "3  Poucas finas, algumas m√©dias e raras grossas                NaN   \n",
       "4                                         Raras   M√©dias e grossas   \n",
       "\n",
       "   Grau de humidade  Horizonte de diagn√≥stico Propriedade de diagn√≥stico  \\\n",
       "0              Seco                       NaN                        NaN   \n",
       "1              Seco                       NaN                        NaN   \n",
       "2              Seco                       NaN                        NaN   \n",
       "3              Seco                       NaN                        NaN   \n",
       "4     Seco a h√∫mido                       NaN                        NaN   \n",
       "\n",
       "  Designa√ß√£o do horizonte  Nitidez do limite  Observa√ßoes Unnamed: 46  \\\n",
       "0                     NaN                NaN          NaN         NaN   \n",
       "1                     NaN                NaN          NaN         NaN   \n",
       "2                     NaN                NaN          NaN         NaN   \n",
       "3                     NaN                NaN          NaN         NaN   \n",
       "4                     NaN                NaN          NaN         NaN   \n",
       "\n",
       "  Confirmar  \n",
       "0       NaN  \n",
       "1       NaN  \n",
       "2       NaN  \n",
       "3       NaN  \n",
       "4       NaN  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morphology.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "morphology.rename(columns={\n",
    "    'Morfo_id':'horizon_id',\n",
    "    'Amostra': 'sample_id',\n",
    "    'Perfil': 'profile',\n",
    "    'CM':'horizon_layer',\n",
    "    'Limite Superior': 'upper_depth',\n",
    "    'Limite inferior': 'lower_depth',\n",
    "    'Grau de humidade': 'moisture_degree',\n",
    "    'Quantidade de ra√≠zes': 'root_quantity',\n",
    "    'Di√¢metro de ra√≠zes': 'root_diameter',\n",
    "    'Manchas': 'stains',\n",
    "    'Textura': 'texture',\n",
    "    'Abund√¢ncia de elementos grosseiros': 'thick_contents_count',\n",
    "    'Forma de elementos grosseiros': 'thick_contents_shape',\n",
    "    'Natureza de elementos grosseiros': 'thick_contents_nature',\n",
    "    'Tipo de estrutura': 'structure_type',\n",
    "    'Classes de estrutura': 'structure_class',\n",
    "    'Grau de estrutura': 'structure_degree',\n",
    "    'Di√¢metro de poros': 'pore_diameter',\n",
    "    'Quantidade de poros': 'pore_quantity',\n",
    "    'Forma de poros': 'pore_shape',\n",
    "    'Cor (s)': 'dry_color_name',\n",
    "    'Matiz (s)': 'dry_hue',\n",
    "    'Valor (s)':'dry_value',\n",
    "    'Croma (s)': 'dry_chroma',\n",
    "    'Cor (h)': 'moist_color_name',\n",
    "    'Matiz (h)': 'moist_hue',\n",
    "    'Valor (h)': 'moist_value',\n",
    "    'Croma (h)': 'moist_chroma',\n",
    "    'Compacidade':'compaction',\n",
    "    'Dureza': 'durability', \n",
    "    'Friabilidade': 'friability'\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "# morphology_cleaning = morphology.drop(columns=[\n",
    "#     'ID1', 'Pro', 'Observa√ßoes', 'Nitidez do limite', 'Designa√ß√£o do horizonte', 'Observa√ßoes', 'Confirmar', 'Adesividade', 'Plasticidade', 'Efervesc√™ncia com HCl', 'Orienta√ß√£o das Fendas', 'Largura das fendas', 'Quantidade de fendas'\n",
    "# ])\n",
    "morphology_cleaning = morphology\n",
    "#add profile_record_id to populate later from soil profile table\n",
    "morphology_cleaning['profile_record_id'] = pd.NA\n",
    "\n",
    "#drop accents\n",
    "import unicodedata\n",
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        # Normalize and remove diacritics\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "# Apply to all cells in the DataFrame\n",
    "morphology_cleaning = morphology_cleaning.applymap(remove_accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['friability'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Reorder columns to match SAMPLES schema\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m morphology_cleaning1 \u001b[38;5;241m=\u001b[39m \u001b[43mmorphology_cleaning\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhorizon_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msample_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprofile_record_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprofile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhorizon_layer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mupper_depth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlower_depth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmoisture_degree\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroot_quantity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroot_diameter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtexture\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstructure_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstructure_class\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstructure_degree\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpore_diameter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpore_quantity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpore_shape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdry_color_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdry_hue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdry_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdry_chroma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmoist_color_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmoist_hue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmoist_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmoist_chroma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcompaction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdurability\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfriability\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthick_contents_count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthick_contents_nature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Show first few rows\u001b[39;00m\n\u001b[1;32m     36\u001b[0m morphology_cleaning1\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3766\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3767\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3769\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5877\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5879\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5881\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:5941\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5940\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 5941\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['friability'] not in index\""
     ]
    }
   ],
   "source": [
    "# Reorder columns to match SAMPLES schema\n",
    "morphology_cleaning1 = morphology_cleaning [[\n",
    "    'horizon_id',\n",
    "    'sample_id',\n",
    "    'profile_record_id',\n",
    "    'profile',\n",
    "    'horizon_layer',\n",
    "    'upper_depth',\n",
    "    'lower_depth',\n",
    "    'moisture_degree',\n",
    "    'root_quantity',\n",
    "    'root_diameter',\n",
    "    'texture',\n",
    "    'structure_type',\n",
    "    'structure_class',\n",
    "    'structure_degree',\n",
    "    'pore_diameter',\n",
    "    'pore_quantity',\n",
    "    'pore_shape',\n",
    "    'dry_color_name',\n",
    "    'dry_hue',\n",
    "    'dry_value',\n",
    "    'dry_chroma',\n",
    "    'moist_color_name',\n",
    "    'moist_hue',\n",
    "    'moist_value',\n",
    "    'moist_chroma',\n",
    "    'compaction',\n",
    "    'durability',\n",
    "    'friability',\n",
    "    'thick_contents_count',\n",
    "    'thick_contents_nature'\n",
    "]]\n",
    "\n",
    "# Show first few rows\n",
    "morphology_cleaning1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "horizon_id                object\n",
       "sample_id                float64\n",
       "profile_record_id         object\n",
       "profile                   object\n",
       "horizon_layer            float64\n",
       "upper_depth              float64\n",
       "lower_depth              float64\n",
       "moisture_degree           object\n",
       "root_quantity             object\n",
       "root_diameter             object\n",
       "texture                   object\n",
       "structure_type            object\n",
       "structure_class           object\n",
       "structure_degree          object\n",
       "pore_diameter             object\n",
       "pore_quantity             object\n",
       "pore_shape                object\n",
       "dry_color_name            object\n",
       "dry_hue                   object\n",
       "dry_value                float64\n",
       "dry_chroma               float64\n",
       "moist_color_name          object\n",
       "moist_hue                 object\n",
       "moist_value              float64\n",
       "moist_chroma             float64\n",
       "compaction                object\n",
       "durability                object\n",
       "friability                object\n",
       "thick_contents_count      object\n",
       "thick_contents_nature     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morphology_cleaning1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure consistent data types and formatting\n",
    "morphology_cleaning1['profile'] = morphology_cleaning1['profile'].astype(str).str.replace('/', '_').str.strip().str[:20]\n",
    "\n",
    "#convert sample_to to string of the integer if it's a float like 11003.0\n",
    "#The original string if it's not an integer or has decimal places\n",
    "#\"NULL\" if the value is missing\n",
    "def clean_sample_id(x):\n",
    "    if pd.isnull(x):\n",
    "        return \"NULL\"\n",
    "    try:\n",
    "        float_val = float(x)\n",
    "        if float_val.is_integer():\n",
    "            return str(int(float_val))\n",
    "        else:\n",
    "            return str(float_val)\n",
    "    except:\n",
    "        return str(x)\n",
    "\n",
    "morphology_cleaning1['sample_id'] = morphology_cleaning1['sample_id'].apply(clean_sample_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NULL']\n",
      "üîç Number of sample_id values equal to 'NULL': 6639\n"
     ]
    }
   ],
   "source": [
    "# Check for nulls in 'sample_id' column\n",
    "null_string_values = morphology_cleaning1[morphology_cleaning1['sample_id'] == \"NULL\"]['sample_id'].unique()\n",
    "print(null_string_values)\n",
    "\n",
    "null_string_count = (morphology_cleaning1['sample_id'] == \"NULL\").sum()\n",
    "print(f\"üîç Number of sample_id values equal to 'NULL': {null_string_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 1260 sample_id(s) in morphology_cleaning but not in samples_check:\n",
      "['10064.2', '1007', '1022', '1024', '10338', '10339', '10340', '10341', '10342', '10343', '10344', '10345', '10346', '10347', '10348', '10377', '104', '10428.2', '10443', '10444', '10445', '10446.2', '10448', '10449', '10450', '10451', '10467', '10468', '10469', '10470.2', '10472', '10480.2', '10485.2', '105', '10509.2', '10536.2', '10558', '10562.2', '10576.2', '106', '1067', '1068', '1069', '107', '10843.2', '10849.2', '10885.2', '1089', '1090', '10901', '10902', '10903', '10904.2', '1091', '1092', '10922.2', '1093', '10933.2', '1094', '1099', '1100.2', '11003.2', '1108.2', '1110', '1111.2', '1117', '11176.2', '1118', '1119.2', '11285.2', '11327.2', '11356.2', '11373', '11374', '1138', '1139', '11393.2', '1140.2', '1142', '1143', '1144.3', '11454.2', '1147', '1148', '11485.2', '1149.3', '11492', '11493', '11494', '1152', '1153', '1154.2', '1156', '11560.2', '1157', '1158.3', '1160', '11794.2', '1182', '1183', '1184.2', '1186', '1187', '1188', '1189', '1190', '1191.2', '1198', '1199', '1200.3', '1204', '1205', '1206.2', '1208', '1209.2', '1211', '1212', '1213', '1214', '1215.2', '1217', '1219', '1220.2', '1222', '1223', '1224', '1225.2', '1227', '1228', '1229.2', '1231', '1232', '1233', '1234', '1235', '1236.2', '12450', '12451', '12452', '12494', '12495', '12496', '1250', '1251', '1252.3', '1255', '1256', '12630.2', '12636.2', '12653.2', '12692.2', '12715.2', '1274', '1275.3', '1286', '1287', '1288', '1291', '1292', '1298', '1299', '1300', '1303', '1304', '1323', '1324', '1325.3', '1327', '13274.2', '1328', '1329', '13299.2', '1333', '1334', '1335.3', '1338', '13387.2', '1339', '13394.2', '134', '1340', '13404.2', '1341', '1343.2', '13475.2', '13481.2', '13493.2', '135', '1350', '1351', '1352.3', '13521.2', '13532.2', '13549.2', '1355', '13556.2', '1356.3', '1358', '13581.2', '13599.2', '136', '1360', '13606.2', '1361', '13621.2', '13638.2', '1364', '1365', '1366.3', '13661.2', '1368', '13695.2', '1375', '1376.4', '13765.2', '13779.2', '1379', '13793.2', '13804.2', '1381', '13810.2', '13817.2', '1382', '1383.2', '1386', '1387', '1388.2', '13883.2', '13901.2', '13924.2', '13929', '13930', '13931.2', '13933', '13934', '13949', '1396', '1397', '1398', '13988.2', '1399', '13999.2', '140', '1401', '14018.2', '1402', '1403', '1406', '14064.2', '1407', '14071.2', '1408.3', '1411', '14112.2', '14119.2', '1412', '1413.2', '14135.2', '1415', '14156.2', '142', '14213.2', '1424', '14249.2', '1425', '1426.3', '14266.2', '14278.2', '1429', '14296.2', '1434', '14351', '14352', '14353', '1438', '1439', '144', '1440', '1441', '1442.2', '14451', '14452', '14453', '14454', '14455.2', '14508', '14509', '14510', '1452', '1453', '1454', '1455', '1456', '1457', '1458', '1459', '1460', '1461', '1462', '1463.2', '1465', '1466', '1473', '1474', '1475.2', '1482', '1483', '1484', '1485.2', '14857.2', '1487', '1488', '1489', '1490.3', '14900', '14901', '14920.2', '1493', '1494', '1495', '1496', '1497', '14982', '15005.2', '15031.2', '15032.2', '15049.2', '15132.2', '1525', '15286.2', '15305.2', '15314', '15315', '15316', '15337.2', '15352.2', '15370.2', '1538', '1539', '1542', '1543', '1544', '1546.2', '1548.3', '1557.2', '1560.2', '1562.2', '15641.2', '1568.2', '15769', '15770', '15771', '15772', '15773', '15774', '15775', '15776', '15777', '15778.2', '1580.2', '1585.2', '1589.3', '15896.2', '1592.2', '15951.2', '15957.2', '1598.2', '1602.2', '16031.2', '1604.2', '16266.2', '1627.2', '1629.2', '16400', '16401', '16402', '16403', '16404', '16405', '16424', '16425', '16426', '16427', '16428', '16429', '16494.2', '1653.4', '16633', '16634', '16637', '16638', '1678', '1679', '1680', '1681', '16826', '16827', '16828', '16829', '16830', '16845', '16846', '16847', '16848', '16849', '16850', '16851', '16852', '16853.2', '16855', '16867', '16868', '16869', '16870.2', '16872', '16873', '1699', '1700', '1701', '1702', '1703', '1704', '1711', '1712', '1713', '1718', '1747.3', '1752.2', '17536.2', '1754.2', '17607.2', '17794', '17795', '17796', '17797', '17798', '17799', '1785.3', '17884', '17885', '17886', '17887', '17888', '17889', '17890', '1792.2', '17994', '17995', '17996', '17997', '1802.2', '18025', '18026', '18027', '18028', '18058', '18059', '18060', '18061', '18062', '18063', '18077', '18078', '18079', '18080', '18100', '18101', '18102', '18103', '18104', '1843.2', '18441', '18442', '18443', '18444', '18445', '18446', '18447', '1845.2', '18498.2', '1850.2', '18513.2', '18575.2', '18588.2', '18671.2', '18703.2', '18736', '18737', '18738', '18739', '1874.2', '18740.2', '18759.2', '18774', '18775', '18776', '18777', '18778', '18779', '1880.3', '18814.2', '19176', '19177', '19178', '19179', '19180', '19181', '19182', '19183', '19184', '19185', '19186', '19187', '19188', '19189', '19190', '19192', '19193', '19194', '19195', '19196', '1924', '19240', '19241', '19242', '19243', '19244.2', '19251', '19252', '19253', '19254', '19255.2', '19267', '19268', '19269', '19270', '19271', '1928', '19283', '19284', '19285', '19286', '19287', '19298', '19299', '19301', '19302', '19303', '19304', '1932', '19330', '19331', '19332', '19333', '19334', '19371', '19372', '19373', '19374', '19375', '1938.2', '19391', '19392', '19393', '19394', '19404', '19405', '19406', '19407', '19408', '19431', '19432', '19433', '19434', '19435', '19436', '19437', '19438', '19439', '19440', '19448', '19449', '19450', '19451', '19467', '19468', '19469', '19537', '19538', '19539', '19541', '19542', '19543', '19544', '19545', '19550', '19551', '19552', '19553', '19554.2', '19556', '19557', '19558', '1956.3', '19565', '19566', '19567', '19568', '19569', '19570', '19571', '19572', '19573', '19574', '19575', '19576', '19577', '19579', '19580', '19581', '19582', '19583', '19585', '19586', '19587', '19588', '19654', '19655', '19656', '1968.2', '1991.2', '1993.2', '2002.2', '2007.3', '2013.4', '2019.2', '2023.2', '2051.2', '2055.2', '2057.2', '2069.3', '2075.2', '20880', '20881', '20882', '20883', '2089.2', '20891', '20892', '20893', '20894', '20895', '20896', '20897', '20898', '20899', '20905', '20906', '20907', '20908', '20909', '20910', '20911', '20912.2', '20914', '20915', '20916', '20917', '20918', '20919', '20920', '20921.2', '20925', '20926', '20927', '2093.2', '20955', '20956', '20957', '20958', '20959', '20970', '20971', '20972', '20973', '21002', '21003', '21004', '21005', '21006', '21007', '2107.4', '2115.2', '2136.2', '2149.3', '21528', '21529', '21530', '21531', '21532', '21533', '21534', '21535', '21536', '21537', '21542', '21543', '21544', '21545', '21546', '21547', '21548', '21549', '21555', '21556', '21557', '21558', '21559', '21575', '21576', '21577', '21603', '21604', '21605', '21606', '21613', '21614', '21615', '21616', '21617', '21618', '21619', '21620', '21627', '21628', '21629', '21630', '21634', '21635', '21636', '21637', '21642', '21643', '21644', '2165', '21652', '21653', '21654', '21666', '21667', '21668', '21669', '21670', '21671', '21672', '21673', '21674', '2169.2', '21704', '21705', '21706', '21707', '21708', '21711', '21712', '21713', '21714', '21715', '21716', '21718', '21719', '21720', '21721', '21724', '21725', '21726', '21727', '21728', '21732', '21733', '21734', '21735', '21736', '21744', '21745', '21746', '21747', '2248.2', '2253.2', '2263.2', '2268.2', '2282.2', '22839', '22840', '22841', '22842', '22843', '22844', '22845', '22846', '2371.2', '2373.3', '2378.2', '2380.2', '2382.2', '2393.2', '2398.2', '2400.2', '2408.2', '2410.3', '2415.3', '2463.2', '2475.2', '2546', '2547', '2549', '2550', '2560', '2561', '2562', '2563', '2630', '2631', '2632', '2633', '2663', '2664', '2665', '2666', '2668', '2670', '2671', '2672', '2674', '2675', '2677', '2685', '2686', '2687', '2689', '2693', '2694', '2695', '2709', '2711.2', '2716.3', '2730.2', '2738.2', '2743.3', '2747.2', '2750.3', '2757.3', '2763.2', '2765.2', '2776.2', '2785.2', '2842.3', '2847.2', '2869.2', '2877.2', '2888.2', '2890.3', '2906.2', '2908.2', '2910.2', '2921.2', '2927.2', '2945.2', '2978.2', '2981.2', '2987.2', '299.2', '2994.2', '3', '3006.2', '3008.2', '301', '3014.2', '3016.3', '302', '3021.3', '3027', '3029', '303', '3031.2', '3045.2', '3050.2', '3052.2', '3103.2', '3109.2', '3119.2', '3145.2', '3151.2', '3156.2', '3161.2', '3163.2', '3165.2', '3179.2', '3194.2', '3213.2', '3216.2', '3227', '3229', '3231', '3259.2', '3290.2', '3309.2', '3348.2', '3412.2', '3417.2', '3492.2', '3506.2', '3526.2', '3528.3', '3535.2', '3568', '3569', '3570.2', '3572.2', '3574.2', '3606.2', '3636.2', '3651', '3652', '3653', '3654', '3655', '3671', '3672', '3673', '3674', '3676', '3703', '3704', '3705', '3706', '3721', '3722', '3723', '3734', '3735', '3736', '3737', '3745', '3746', '3768', '3770', '3785', '3787', '3788', '3798', '3800', '3801', '3835', '3837', '3838', '3840', '3841', '3850', '3852', '3853', '3854', '3855', '3869', '3870', '3871', '3872', '3873', '3874', '3876', '3878', '3879', '3881', '3923', '3924', '3925', '4241', '437', '438', '439', '473', '474', '475', '476', '483', '4840.3', '4867.3', '489', '4899.2', '490', '4908.2', '4916.2', '4920.2', '4957', '4958.2', '4972.3', '4981.2', '4989.2', '4996.2', '4998', '4999', '50', '5000', '5006.2', '5010.2', '5026.2', '5036', '5045.2', '5052', '5053.2', '5082.2', '5088.2', '51.2', '5101.2', '5107.2', '5123.2', '5174.2', '5176.2', '5192.2', '5202.2', '5207.2', '5214.2', '5218.2', '5225.3', '5270.2', '5315.2', '5327.2', '5332.2', '5342.2', '5348.2', '5359.2', '5389', '5404.2', '5417.2', '5431', '5434.2', '5442.2', '5444.2', '5447.2', '5476.2', '5478.2', '5686', '5717.2', '5737.2', '5745.2', '5751.2', '5767', '577', '5770.2', '5777.2', '578.2', '5780.2', '579', '5792', '5810.2', '5822.2', '5839.2', '5883.2', '5886', '5897.3', '5902.2', '5914.2', '5916.2', '5945.2', '5963.2', '5973.2', '598', '5983.2', '599', '5990.2', '5995.2', '5997.2', '600', '6014.2', '6022.2', '6032.2', '607', '608', '609.2', '611', '612.2', '6314.2', '6317.2', '6323.2', '6332.2', '6334', '6335', '6336.3', '6339', '634.2', '6340', '6341', '6342.3', '6348.2', '6350.2', '6356', '6357', '6358.2', '6373', '6374', '6375', '6376.3', '6381.2', '6383.3', '6386', '6387', '6388.2', '6390', '6391.2', '6402.2', '6405.2', '6416.2', '6436', '6437', '6438', '6439.3', '6475', '6476', '6477', '6478', '6479', '6481', '6549.2', '6633.2', '6635.2', '6652.2', '6667', '6668', '6669.2', '6675', '6684.2', '6686.2', '6704', '6705', '6707', '6710', '6726.2', '6754.2', '6768.2', '6787.2', '6815.2', '6824', '6836', '6840', '6853.2', '6872.2', '6890.2', '6895.2', '6897.2', '6903.2', '6908.2', '6921.2', '6924.2', '6931.2', '694', '695', '6964.2', '6974.2', '6988.2', '7011.2', '7026', '7047.2', '709', '7108.2', '7127', '7140', '7141.2', '7184.2', '7255.2', '7257', '7261', '7265', '7268.2', '7278', '7284', '7289.2', '7291.2', '734', '735', '7352', '7390.2', '7395', '7397.2', '74', '7408.2', '745', '746', '747', '75', '75.1', '76', '761', '7815', '7817.2', '7838.2', '7849.2', '7859.2', '7931', '7932.2', '7935.2', '7944.2', '7958.2', '8000', '8012.2', '8057', '8058', '8059', '8066', '8067', '8068', '8077', '8119.2', '8271.2', '8274.2', '832', '8332', '8336', '8339', '836.2', '8360.2', '8431', '8493', '8565', '86', '8681.2', '87', '88', '8830.2', '8865.2', '89', '890', '891', '8959.2', '90', '9039.2', '91', '9125.2', '9143.2', '9155.2', '9183.2', '9202', '9208.2', '9281.2', '9297.2', '9302.2', '9442.2', '9449.2', '9455.2', '95', '9561', '9564.2', '96', '9604.2', '97', '98', '985', '989', '996', '9986', 'NULL']\n"
     ]
    }
   ],
   "source": [
    "#sample_id(s) in morphology_cleaning but not in samples_check:\n",
    "# Ensure both sample_id columns are strings for accurate comparison\n",
    "morphology_cleaning1['sample_id'] = morphology_cleaning1['sample_id'].astype(str)\n",
    "samples_check['sample_id'] = samples_check['sample_id'].astype(str)\n",
    "\n",
    "# Get unique sample_id values in both DataFrames\n",
    "morph_sample_ids = set(morphology_cleaning1['sample_id'].dropna())\n",
    "samples_check_ids = set(samples_check['sample_id'].dropna())\n",
    "\n",
    "# Find sample_ids in morphology_cleaning but not in samples_check\n",
    "sample_ids_not_in_check = sorted(morph_sample_ids - samples_check_ids)\n",
    "\n",
    "# Print results\n",
    "print(f\"‚úÖ Found {len(sample_ids_not_in_check)} sample_id(s) in morphology_cleaning but not in samples_check:\")\n",
    "print(sample_ids_not_in_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing sample IDs in horizon:\n",
      "['633' '700' '707' ... '18869' '18870' '18871']\n",
      "9635\n"
     ]
    }
   ],
   "source": [
    "#check which sample_id values from samples_check are missing from morphology_cleaning\n",
    "\n",
    "missing_ids_horizon = samples_check[~samples_check['sample_id'].isin(morphology_cleaning1['sample_id'])]\n",
    "print(\"Missing sample IDs in horizon:\")\n",
    "print(missing_ids_horizon['sample_id'].unique())\n",
    "print(len(missing_ids_horizon['sample_id'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal** = drop any horizon_id from morphology_cleaning that does not have both:\n",
    "\n",
    "1. Key horizon characteristics\n",
    "\n",
    "2. A valid profile (w location and site characteristics) that appears in site_info_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Found 2597 rows with at least one of the following issues:\n",
      "   ‚Ä¢ missing key horizon data\n",
      "   ‚Ä¢ invalid profile (not in site_info_clean)\n",
      "   ‚Ä¢ missing both upper_depth and lower_depth\n",
      "‚úÖ Remaining rows in morphology_cleaning: 10438\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------\n",
    "# 1. Standardize 'profile' column formatting\n",
    "# -----------------------------------------\n",
    "site_info_clean.loc[:, 'profile'] = (\n",
    "    site_info_clean['profile']\n",
    "    .astype(str)\n",
    "    .str.replace('/', '_')\n",
    "    .str.strip()\n",
    "    .str.upper()\n",
    "    .str[:20]\n",
    ")\n",
    "\n",
    "morphology_cleaning1.loc[:, 'profile'] = (\n",
    "    morphology_cleaning1['profile']\n",
    "    .astype(str)\n",
    "    .str.replace('/', '_')\n",
    "    .str.strip()\n",
    "    .str.upper()\n",
    "    .str[:20]\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Define valid profiles\n",
    "# ---------------------------\n",
    "valid_profiles = set(site_info_clean['profile'])\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Define helper to check for missing/zero\n",
    "# ---------------------------\n",
    "def is_missing(val):\n",
    "    return pd.isna(val) or val == 0 or val == '0'\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Build masks for filtering\n",
    "# ---------------------------\n",
    "\n",
    "# A. Missing key horizon data\n",
    "missing_key_data_mask = (\n",
    "    morphology_cleaning1['horizon_id'].notna() &\n",
    "    morphology_cleaning1['moist_color_name'].apply(is_missing) &\n",
    "    morphology_cleaning1['texture'].apply(is_missing) &\n",
    "    morphology_cleaning1['structure_type'].apply(is_missing)\n",
    ")\n",
    "\n",
    "# B. Invalid profile\n",
    "invalid_profile_mask = ~morphology_cleaning1['profile'].isin(valid_profiles)\n",
    "\n",
    "# C. Missing both upper and lower depth\n",
    "missing_depth_mask = (\n",
    "    morphology_cleaning1['upper_depth'].apply(is_missing) &\n",
    "    morphology_cleaning1['lower_depth'].apply(is_missing)\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Combine all masks\n",
    "# ---------------------------\n",
    "rows_to_flag_mask = missing_key_data_mask | invalid_profile_mask | missing_depth_mask\n",
    "rows_to_flag = morphology_cleaning[rows_to_flag_mask]\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Report and drop flagged rows\n",
    "# ---------------------------\n",
    "print(f\"‚ö†Ô∏è Found {len(rows_to_flag)} rows with at least one of the following issues:\")\n",
    "print(\"   ‚Ä¢ missing key horizon data\")\n",
    "print(\"   ‚Ä¢ invalid profile (not in site_info_clean)\")\n",
    "print(\"   ‚Ä¢ missing both upper_depth and lower_depth\")\n",
    "\n",
    "# Optional: Save flagged rows for review\n",
    "# rows_to_flag.to_csv(\"/Users/inesschwartz/Desktop/morph_flagged_rows_combined.csv\", index=False)\n",
    "\n",
    "# Drop flagged rows from main DataFrame\n",
    "morphology_cleaning1 = morphology_cleaning1[~rows_to_flag_mask].copy()\n",
    "\n",
    "# Summary after cleanup\n",
    "print(f\"‚úÖ Remaining rows in morphology_cleaning: {len(morphology_cleaning1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11016' '11017' 'NULL' '11352' '11353' '17532' '17533' '17534' '2438'\n",
      " '7192' '13178' '20883' '9027' '9231' '8232' '9255' '8639' '8536' '4918']\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates\n",
    "# Make sure sample_id and horizon_id are strings\n",
    "morphology_cleaning1['sample_id'] = morphology_cleaning1['sample_id'].astype(str).str.strip()\n",
    "morphology_cleaning1['horizon_id'] = morphology_cleaning1['horizon_id'].astype(str).str.strip()\n",
    "# Filter out and print duplicated samples (including the first occurrence)\n",
    "duplicated_values = morphology_cleaning1['sample_id'][morphology_cleaning1['sample_id'].duplicated()].unique()\n",
    "print(duplicated_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Morphology data cleaned and sample_id updated via horizon_id.\n"
     ]
    }
   ],
   "source": [
    "#addressing duplicates\n",
    "\n",
    "# -----------------------------------\n",
    "# 1. Correct sample_id using horizon_id map\n",
    "# -----------------------------------\n",
    "# Step 1: Make sure IDs are strings and trimmed\n",
    "morphology_cleaning1['horizon_id'] = morphology_cleaning1['horizon_id'].astype(str).str.strip()\n",
    "morphology_cleaning1['sample_id'] = morphology_cleaning1['sample_id'].astype(str).str.strip()\n",
    "\n",
    "# Step 2: Your mapping (horizon_id ‚Üí new sample_id)\n",
    "horizon_to_sample_map = {\n",
    "    'UZ_24c/60_1_1': '8639',\n",
    "    'UZ_24c/60_2_1': '8640',\n",
    "    'UZ_24c/60_3_1': '8641',\n",
    "    'UZ_24c/60_4_1': '8642',\n",
    "    'UZ_24c/60_5_1': '8643',\n",
    "    'UZ_66c/60_1_1': '8777',\n",
    "    'UZ_66c/60_2_1': '8778',\n",
    "    'UZ_66c/60_3_1': '8779',\n",
    "    'UZ_66c/60_4_1': '8780',\n",
    "    'CC_473/66_1_1': '17596',\n",
    "    'CC_473/66_2_1': '17597',\n",
    "    'CC_473/66_3_1': '17598',\n",
    "    'N_85/57_1_0': '4947',\n",
    "    'N_85/57_2_0': '4948',\n",
    "    'N_85/57_3_0': '4949',\n",
    "    'UZ_231/60_4_1': '8255',\n",
    "    'B_139/61_5_1': '10353',\n",
    "    'H_480/55_1_1': '3919',\n",
    "    'H_91/54_1_1': '1255',\n",
    "    'H_90/54_3_3': '1252',\n",
    "    'Hb_136/56_1_1':'2961',\n",
    "    'Hb_136/56_2_1': '2962',\n",
    "    'Hb_136/56_3_1': '2963',\n",
    "    'Hb_136/56_4_1': '2964',\n",
    "    'UZ_216/60_5_1': '8231',\n",
    "    'Bg_217/46_1_1': '694',\n",
    "    'Bg_217/46_2_1': '695',\n",
    "    'Bg_217/46_3_1': '696',\n",
    "    'UZ_215c/60_1_1':'9255',\n",
    "    'UZ_215c/60_2_1': '9256',\n",
    "    'UZ_215c/60_3_1': '9257',\n",
    "    'UZ_215c/60_4_1': '9258',\n",
    "    'UZ_215c/60_5_1': '9259',\n",
    "    'UZ_215c/60_6_1': '9260',\n",
    "    'N_30/57_1_1':'4848',\n",
    "    'N_30/57_2_2':'4849',\n",
    "    'N_30/57_3_1':'4850',\n",
    "    'UZ_66c/60_1_1': '8777',\n",
    "    'UZ_66c/60_2_1': '8778',\n",
    "    'UZ_66c/60_3_1': '8779',\n",
    "    'UZ_66c/60_4_1': '8780',\n",
    "    'UZ_66c/60_5_1': '8781',\n",
    "    'UZ_66c/60_6_1': '8783',\n",
    "    'UZ_66c/60_7_1': '8782',\n",
    "    'H_91/54_1_1': '1255',\n",
    "    'H_91/54_2_1': '1256',\n",
    "    'H_91/54_3_1': '1257'\n",
    "}\n",
    "\n",
    "# Step 3: Apply the mapping to replace sample_id based on horizon_id\n",
    "morphology_cleaning1['sample_id'] = morphology_cleaning1.apply(\n",
    "    lambda row: horizon_to_sample_map.get(row['horizon_id'], row['sample_id']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# 2. Drop specific horizon_ids (might have already been dropped)\n",
    "# -----------------------------------\n",
    "horizons_to_drop = [\n",
    "    'B_139/61_4_1', 'H_113/55_1_1', 'H_1306/52_1_1', 'H_1611/52_1_1',\n",
    "    'H_227/46_1_1', 'H_227/46_2_1', 'H_227/46_3_1', 'H_227/46_4_1',\n",
    "    'Mj_178c/63_4_1', 'Mj_321c/63_4_1', 'B_110/62_5_1', 'B_110/62_6_1'\n",
    "]\n",
    "morphology_cleaning1 = morphology_cleaning1[~morphology_cleaning1['horizon_id'].isin(horizons_to_drop)]\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# 3. Done ‚Äî Optional: Save to file\n",
    "# -----------------------------------\n",
    "#df_morpho_cleaned.to_csv(\"/Users/inesschwartz/Desktop/df_morpho_cleaned_corrected.csv\", index=False)\n",
    "print(\"‚úÖ Morphology data cleaned and sample_id updated via horizon_id.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NULL']\n",
      "üîç Number of sample_id values equal to 'NULL': 4713\n"
     ]
    }
   ],
   "source": [
    "#re-count null sample_id's\n",
    "morphology_cleaning1\n",
    "# Check for nulls in 'sample_id' column\n",
    "null_string_values2 = morphology_cleaning1[morphology_cleaning1['sample_id'] == \"NULL\"]['sample_id'].unique()\n",
    "print(null_string_values2)\n",
    "\n",
    "null_string_count2 = (morphology_cleaning1['sample_id'] == \"NULL\").sum()\n",
    "print(f\"üîç Number of sample_id values equal to 'NULL': {null_string_count2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Found 0 duplicated horizon_id(s).\n"
     ]
    }
   ],
   "source": [
    "num_duplicates = morphology_cleaning1['horizon_id'].duplicated().sum()\n",
    "print(f\"üîÅ Found {num_duplicates} duplicated horizon_id(s).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_morpho_cleaned = morphology_cleaning1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dealing w decimals in sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Found 440 sample_id(s) with decimal values.\n",
      "\n",
      "üßæ Affected sample_ids:\n",
      "['11003.2' '11285.2' '10843.2' '10428.2' '10470.2' '10480.2' '10485.2'\n",
      " '10446.2' '10849.2' '10509.2' '10562.2' '10536.2' '11327.2' '11356.2'\n",
      " '11393.2' '11176.2' '11454.2' '10885.2' '10904.2' '10922.2' '11485.2'\n",
      " '10933.2' '4972.3' '4981.2' '5810.2' '5045.2' '5822.2' '4989.2' '5006.2'\n",
      " '5010.2' '5839.2' '4996.2' '5082.2' '5026.2' '6314.2' '6317.2' '6323.2'\n",
      " '6332.2' '6336.3' '5883.2' '6342.3' '6348.2' '6350.2' '6358.2' '5902.2'\n",
      " '5897.3' '6376.3' '6381.2' '6383.3' '6388.2' '6391.2' '6402.2' '6405.2'\n",
      " '6416.2' '5914.2' '5916.2' '6439.3' '5088.2' '5053.2' '5101.2' '5107.2'\n",
      " '5123.2' '5945.2' '5174.2' '5176.2' '5963.2' '5973.2' '5202.2' '5207.2'\n",
      " '5192.2' '5214.2' '5218.2' '5225.3' '5983.2' '5990.2' '5270.2' '5995.2'\n",
      " '5997.2' '5447.2' '6014.2' '6022.2' '5417.2' '5332.2' '6032.2' '5476.2'\n",
      " '5478.2' '5434.2' '5359.2' '5442.2' '5444.2' '5327.2' '5342.2' '5315.2'\n",
      " '5348.2' '7390.2' '7397.2' '5717.2' '5745.2' '5751.2' '5737.2' '5777.2'\n",
      " '7408.2' '5770.2' '5780.2' '20912.2' '20921.2' '19554.2' '19255.2'\n",
      " '18814.2' '18740.2' '6787.2' '6903.2' '6924.2' '6684.2' '6686.2' '6921.2'\n",
      " '7047.2' '6726.2' '6815.2' '6754.2' '6633.2' '6635.2' '6890.2' '6908.2'\n",
      " '6895.2' '6897.2' '6652.2' '6964.2' '7108.2' '6872.2' '6669.2' '6768.2'\n",
      " '6931.2' '6853.2' '7268.2' '7141.2' '6988.2' '7011.2' '7184.2' '6974.2'\n",
      " '7289.2' '7291.2' '7255.2' '16853.2' '16870.2' '18498.2' '18513.2'\n",
      " '17536.2' '18575.2' '18588.2' '18671.2' '17607.2' '18703.2' '10064.2'\n",
      " '11560.2' '11794.2' '1100.2' '1557.2' '1560.2' '1752.2' '1754.2' '1747.3'\n",
      " '1275.3' '1785.3' '1792.2' '1343.2' '1352.3' '1356.3' '1802.2' '1366.3'\n",
      " '1325.3' '836.2' '1376.4' '1383.2' '1335.3' '1388.2' '1108.2' '1562.2'\n",
      " '1413.2' '1408.3' '1111.2' '1568.2' '2248.2' '1843.2' '1845.2' '2253.2'\n",
      " '1426.3' '1850.2' '1580.2' '1442.2' '1602.2' '1604.2' '2263.2' '1463.2'\n",
      " '1119.2' '1475.2' '1490.3' '1485.2' '1874.2' '2268.2' '1589.3' '2282.2'\n",
      " '1140.2' '1585.2' '1598.2' '1144.3' '1938.2' '1149.3' '1956.3' '1968.2'\n",
      " '1154.2' '1158.3' '1991.2' '1993.2' '1548.3' '2002.2' '2013.4' '2007.3'\n",
      " '2371.2' '2373.3' '2378.2' '2380.2' '2382.2' '2019.2' '2023.2' '2055.2'\n",
      " '2057.2' '2051.2' '2393.2' '2398.2' '2400.2' '2075.2' '2069.3' '2408.2'\n",
      " '2410.3' '2415.3' '2093.2' '2089.2' '2107.4' '2115.2' '2136.2' '1627.2'\n",
      " '1629.2' '1546.2' '2463.2' '1184.2' '2169.2' '2475.2' '1191.2' '1200.3'\n",
      " '1653.4' '1206.2' '1209.2' '1215.2' '1220.2' '1225.2' '1229.2' '1236.2'\n",
      " '2921.2' '2888.2' '2890.3' '2906.2' '2908.2' '2910.2' '2927.2' '609.2'\n",
      " '612.2' '3506.2' '2945.2' '2716.3' '2994.2' '2987.2' '2978.2' '2981.2'\n",
      " '3021.3' '3006.2' '3008.2' '3014.2' '3016.3' '3535.2' '299.2' '3031.2'\n",
      " '3526.2' '3528.3' '3045.2' '3050.2' '3052.2' '3103.2' '3109.2' '3119.2'\n",
      " '2730.2' '3145.2' '3179.2' '3570.2' '3572.2' '3574.2' '3156.2' '3151.2'\n",
      " '3161.2' '3163.2' '3165.2' '3194.2' '3213.2' '3216.2' '3606.2' '3259.2'\n",
      " '3636.2' '2750.3' '3290.2' '3309.2' '2743.3' '2738.2' '2763.2' '2765.2'\n",
      " '2757.3' '2776.2' '2785.2' '2711.2' '3492.2' '2847.2' '2842.3' '2877.2'\n",
      " '2869.2' '10576.2' '19244.2' '15778.2' '15305.2' '16266.2' '15005.2'\n",
      " '15286.2' '15049.2' '15337.2' '15031.2' '15032.2' '15957.2' '15951.2'\n",
      " '15896.2' '15352.2' '15370.2' '16494.2' '15641.2' '15132.2' '14857.2'\n",
      " '14455.2' '14920.2' '16031.2' '13521.2' '13779.2' '13765.2' '13493.2'\n",
      " '13817.2' '13793.2' '13810.2' '13804.2' '13475.2' '13883.2' '13901.2'\n",
      " '13481.2' '13924.2' '14018.2' '13999.2' '13988.2' '13931.2' '14112.2'\n",
      " '14119.2' '13394.2' '14071.2' '13549.2' '14135.2' '14064.2' '13387.2'\n",
      " '14156.2' '13556.2' '13274.2' '12692.2' '13532.2' '14249.2' '13299.2'\n",
      " '14278.2' '14213.2' '14266.2' '14296.2' '13404.2' '12715.2' '51.2'\n",
      " '12636.2' '578.2' '12630.2' '13599.2' '75.1' '13581.2' '12653.2'\n",
      " '13621.2' '13606.2' '13638.2' '13661.2' '13695.2' '4840.3' '3348.2'\n",
      " '3417.2' '3412.2' '4867.3' '4899.2' '4908.2' '4916.2' '4920.2' '5404.2'\n",
      " '4958.2' '9039.2' '8959.2' '9125.2' '8119.2' '9183.2' '9208.2' '9143.2'\n",
      " '9155.2' '7838.2' '9281.2' '7849.2' '9297.2' '9302.2' '9449.2' '8274.2'\n",
      " '8271.2' '9442.2' '6549.2' '9564.2' '9455.2' '9604.2' '8360.2' '7859.2'\n",
      " '7817.2' '8681.2' '7932.2' '7935.2' '7944.2' '7958.2' '8012.2' '8865.2'\n",
      " '8830.2']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Ensure sample_id is a string\n",
    "df_morpho_cleaned['sample_id'] = df_morpho_cleaned['sample_id'].astype(str).str.strip()\n",
    "\n",
    "# Step 2: Identify sample_ids that contain a decimal point\n",
    "decimal_sample_ids = df_morpho_cleaned[df_morpho_cleaned['sample_id'].str.contains(r'\\.\\d+$', regex=True)]\n",
    "\n",
    "# Step 3: Output results\n",
    "print(f\"üîç Found {len(decimal_sample_ids)} sample_id(s) with decimal values.\\n\")\n",
    "\n",
    "# Optional: Print the unique sample_ids with decimals\n",
    "print(\"üßæ Affected sample_ids:\")\n",
    "print(decimal_sample_ids['sample_id'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (2.0.3)\n",
      "Collecting deep-translator\n",
      "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /opt/homebrew/lib/python3.11/site-packages (from deep-translator) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /opt/homebrew/lib/python3.11/site-packages (from deep-translator) (2.31.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2023.11.17)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
      "Installing collected packages: deep-translator\n",
      "Successfully installed deep-translator-1.11.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas deep-translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Translating column: moisture_degree\n",
      "‚úÖ Finished and saved column: moisture_degree\n",
      "üîÑ Translating column: root_quantity\n",
      "‚úÖ Finished and saved column: root_quantity\n",
      "üîÑ Translating column: root_diameter\n",
      "‚ö†Ô∏è Failed to translate 'Finas, medias e muito  grossas': Finas, medias e muito  grossas --> No translation was found using the current translator. Try another translator?\n",
      "‚úÖ Finished and saved column: root_diameter\n",
      "üîÑ Translating column: texture\n",
      "‚ö†Ô∏è Failed to translate 'Muito argiloso a argiloso': Muito argiloso a argiloso --> No translation was found using the current translator. Try another translator?\n",
      "‚ö†Ô∏è Failed to translate 'Arenoso (um tanto grosseiro)': Arenoso (um tanto grosseiro) --> No translation was found using the current translator. Try another translator?\n",
      "‚ö†Ô∏è Failed to translate 'Franco-argiloso a argilo-arenoso': Franco-argiloso a argilo-arenoso --> No translation was found using the current translator. Try another translator?\n",
      "‚ö†Ô∏è Failed to translate 'Argiloso a argilo-arenoso grosseiro': Argiloso a argilo-arenoso grosseiro --> No translation was found using the current translator. Try another translator?\n",
      "‚ö†Ô∏è Failed to translate 'Arenoso (muito grosseiro)': Arenoso (muito grosseiro) --> No translation was found using the current translator. Try another translator?\n",
      "‚ö†Ô∏è Failed to translate 'Franco-arenoso pouco calcario': Franco-arenoso pouco calcario --> No translation was found using the current translator. Try another translator?\n",
      "‚ö†Ô∏è Failed to translate 'Areno-argiloso fino': Areno-argiloso fino --> No translation was found using the current translator. Try another translator?\n",
      "‚ö†Ô∏è Failed to translate 'Franco-arenoso a arenoso fino': Franco-arenoso a arenoso fino --> No translation was found using the current translator. Try another translator?\n",
      "‚úÖ Finished and saved column: texture\n",
      "üîÑ Translating column: structure_type\n",
      "‚úÖ Finished and saved column: structure_type\n",
      "üîÑ Translating column: structure_class\n",
      "‚úÖ Finished and saved column: structure_class\n",
      "üîÑ Translating column: structure_degree\n",
      "‚úÖ Finished and saved column: structure_degree\n",
      "üîÑ Translating column: pore_diameter\n",
      "‚úÖ Finished and saved column: pore_diameter\n",
      "üîÑ Translating column: pore_quantity\n",
      "‚úÖ Finished and saved column: pore_quantity\n",
      "üîÑ Translating column: pore_shape\n",
      "‚úÖ Finished and saved column: pore_shape\n",
      "üîÑ Translating column: dry_color_name\n",
      "‚ö†Ô∏è Failed to translate 'pardo-amarelado': pardo-amarelado --> No translation was found using the current translator. Try another translator?\n",
      "‚ö†Ô∏è Failed to translate 'Pardo-acinzentado-escuro com leve tom pardo': Pardo-acinzentado-escuro com leve tom pardo --> No translation was found using the current translator. Try another translator?\n",
      "‚ö†Ô∏è Failed to translate 'Cinzento-rosado a pardo-acinzentado': Cinzento-rosado a pardo-acinzentado --> No translation was found using the current translator. Try another translator?\n",
      "‚ö†Ô∏è Failed to translate 'pardo-acinzentado': pardo-acinzentado --> No translation was found using the current translator. Try another translator?\n",
      "‚ö†Ô∏è Failed to translate 'Vermelho com tom vermelho escuro': Vermelho com tom vermelho escuro --> No translation was found using the current translator. Try another translator?\n",
      "‚ö†Ô∏è Failed to translate 'Pardo-escuro a pardo com tom pardo-avermelhado': Pardo-escuro a pardo com tom pardo-avermelhado --> No translation was found using the current translator. Try another translator?\n",
      "‚ö†Ô∏è Failed to translate 'Laranja com leve tom de pardo-forte': Laranja com leve tom de pardo-forte --> No translation was found using the current translator. Try another translator?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "import time\n",
    "\n",
    "# Load your existing cleaned DataFrame\n",
    "df_morpho_translating = df_morpho_cleaned.copy()\n",
    "\n",
    "# Columns to translate\n",
    "columns_to_translate = [\n",
    "    'moisture_degree', 'root_quantity', 'root_diameter', 'texture', \n",
    "    'structure_type', 'structure_class', 'structure_degree', \n",
    "    'pore_diameter', 'pore_quantity', 'pore_shape', \n",
    "    'dry_color_name', 'moist_color_name', \n",
    "    'compaction', 'durability', 'friability', \n",
    "    'thick_contents_count', 'thick_contents_nature'\n",
    "]\n",
    "\n",
    "# Safe translation function\n",
    "def translate_text(text):\n",
    "    if pd.isnull(text) or str(text).strip() == '':\n",
    "        return text\n",
    "    try:\n",
    "        return GoogleTranslator(source='pt', target='en').translate(text)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to translate '{text}': {e}\")\n",
    "        return text\n",
    "\n",
    "# Iterate column by column\n",
    "for col in columns_to_translate:\n",
    "    print(f\"üîÑ Translating column: {col}\")\n",
    "    \n",
    "    # Skip column if already translated\n",
    "    if col + '_en' in df_morpho_translating.columns:\n",
    "        print(f\"‚úÖ Skipping {col} ‚Äî already translated.\")\n",
    "        continue\n",
    "\n",
    "    # Translate only unique values for efficiency\n",
    "    unique_vals = df_morpho_translating[col].dropna().unique()\n",
    "    translation_dict = {}\n",
    "    for val in unique_vals:\n",
    "        translation_dict[val] = translate_text(val)\n",
    "        time.sleep(0.5)  # avoid hitting rate limits\n",
    "\n",
    "    # Map translations back to the column\n",
    "    df_morpho_translating[col + '_en'] = df_morpho_translating[col].map(translation_dict).fillna(df_morpho_translating[col])\n",
    "    \n",
    "    # Save intermediate result after each column\n",
    "    df_morpho_translating.to_csv(\"/Users/inesschwartz/Desktop/morpho_translated_partial.csv\", index=False)\n",
    "    print(f\"‚úÖ Finished and saved column: {col}\")\n",
    "\n",
    "print(\"üéâ All columns translated and saved to 'morpho_translated_partial.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_morpho_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load your DataFrame\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df_morpho_translating \u001b[38;5;241m=\u001b[39m \u001b[43mdf_morpho_cleaned\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Column to extract words from\u001b[39;00m\n\u001b[1;32m      8\u001b[0m column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdurability\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_morpho_cleaned' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load your DataFrame\n",
    "df_morpho_translating = df_morpho_cleaned.copy()\n",
    "\n",
    "# Column to extract words from\n",
    "column = 'durability'\n",
    "\n",
    "# Ensure column is string type\n",
    "df_morpho_translating[column] = df_morpho_translating[column].astype(str)\n",
    "\n",
    "# Combine all rows into one string and extract words\n",
    "all_text = ' '.join(df_morpho_translating[column])\n",
    "words = re.findall(r'\\b\\w+\\b', all_text.lower())\n",
    "\n",
    "# Get unique words\n",
    "unique_words = sorted(set(words))\n",
    "\n",
    "# Create empty translation dictionary\n",
    "translation_dict = {word: '' for word in unique_words}\n",
    "\n",
    "# Print for manual translation\n",
    "print(\"üìù Unique words to translate:\\n\")\n",
    "for word in unique_words:\n",
    "    print(f\"'{word}': '',\")\n",
    "\n",
    "# # Optional: Save to CSV for easier editing\n",
    "# pd.DataFrame({'portuguese': unique_words, 'english': [''] * len(unique_words)}).to_csv(\n",
    "#     '/Users/inesschwartz/Desktop/durability_words_to_translate.csv',\n",
    "#     index=False\n",
    "# )\n",
    "\n",
    "print(\"\\n‚úÖ Extracted\", len(unique_words), \"unique words from 'durability' column.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['horizon_id', 'sample_id', 'profile_record_id', 'profile', 'horizon_layer', 'upper_depth', 'lower_depth', 'moisture_degree', 'root_quantity', 'root_diameter', 'texture', 'structure_type', 'structure_class', 'structure_degree', 'pore_diameter', 'pore_quantity', 'pore_shape', 'dry_color_name', 'dry_hue', 'dry_value', 'dry_chroma', 'moist_color_name', 'moist_hue', 'moist_value', 'moist_chroma', 'compaction', 'durability', 'friability', 'thick_contents_count', 'thick_contents_nature']\n"
     ]
    }
   ],
   "source": [
    "print(list(df_morpho_cleaned.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "df_morpho_cleaned.to_csv(\"/Users/inesschwartz/Desktop/morpho_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## take out all decimals in sample_id (keep as just whole number)\n",
    "# Function to truncate decimal part and return string\n",
    "def truncate_to_int_string(x):\n",
    "    try:\n",
    "        return str(int(float(x)))\n",
    "    except:\n",
    "        return str(x).strip()  # fallback to string version if it can't convert\n",
    "\n",
    "# Apply to sample_id column\n",
    "df_morpho_cleaned['sample_id'] = df_morpho_cleaned['sample_id'].apply(truncate_to_int_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 708 sample_id(s) in morphology_cleaning but not in samples_check:\n",
      "['1024', '10338', '10339', '10340', '10341', '10342', '10343', '10344', '10345', '10346', '10347', '10348', '10377', '104', '10443', '10444', '10445', '10448', '10449', '10450', '10451', '10467', '10468', '10469', '10470', '10472', '105', '10558', '10562', '106', '107', '1089', '10901', '10902', '10903', '10904', '1091', '1092', '1094', '1099', '1100', '1108', '1110', '1111', '1117', '1119', '11373', '1138', '1140', '1142', '1144', '1147', '1149', '11492', '11493', '11494', '1152', '1154', '1156', '1158', '1160', '1182', '1183', '1184', '1186', '1187', '1188', '1189', '1190', '1191', '1198', '1200', '1204', '1205', '1206', '1208', '1209', '1211', '1212', '1213', '1215', '1217', '1219', '1220', '1222', '1223', '1224', '1225', '1227', '1229', '1231', '1233', '1234', '1235', '1236', '12450', '12451', '12452', '12494', '12495', '12496', '1250', '1252', '1255', '1256', '1257', '1274', '1275', '1286', '1287', '1288', '1291', '1292', '1298', '1299', '1303', '1323', '1324', '1325', '1328', '1329', '1333', '1335', '1338', '1339', '134', '1340', '1341', '1343', '135', '1350', '1352', '1355', '1356', '136', '1360', '1361', '1364', '1365', '1366', '1375', '1376', '1381', '1382', '1383', '1386', '1388', '13929', '13930', '13931', '13933', '13934', '13949', '1396', '1397', '1398', '1399', '140', '1401', '1403', '1406', '1408', '1412', '1413', '1415', '142', '1424', '1425', '1426', '1434', '14351', '14352', '14353', '1438', '144', '1440', '1441', '1442', '14451', '14452', '14453', '14454', '14455', '14508', '14509', '1452', '1453', '1455', '1456', '1458', '1459', '1460', '1461', '1462', '1463', '1465', '1466', '1473', '1475', '1482', '1483', '1484', '1485', '1487', '1488', '1489', '1490', '14900', '14901', '1493', '1494', '1495', '1496', '1497', '14982', '15314', '15315', '1542', '1543', '1544', '1546', '1548', '15769', '15770', '15771', '15772', '15773', '15774', '15775', '15776', '15777', '15778', '16400', '16401', '16402', '16403', '16404', '16405', '16424', '16425', '16426', '16427', '16428', '16429', '16633', '16634', '16637', '16638', '1678', '1680', '1681', '16826', '16827', '16828', '16829', '16830', '16845', '16846', '16847', '16848', '16849', '16850', '16851', '16852', '16853', '16855', '16867', '16868', '16869', '16870', '16872', '16873', '1699', '1700', '1701', '1702', '1703', '1704', '1711', '1712', '1713', '1718', '17794', '17795', '17796', '17797', '17798', '17799', '17884', '17885', '17886', '17887', '17888', '17889', '17890', '17994', '17995', '17996', '17997', '18025', '18026', '18027', '18028', '18058', '18059', '18060', '18061', '18062', '18063', '18077', '18078', '18079', '18080', '18100', '18101', '18102', '18103', '18104', '18441', '18442', '18443', '18444', '18445', '18446', '18447', '18736', '18737', '18738', '18739', '18740', '18774', '18775', '18776', '18777', '18778', '18779', '19176', '19177', '19178', '19179', '19180', '19181', '19182', '19183', '19184', '19185', '19186', '19187', '19188', '19189', '19190', '19192', '19193', '19194', '19195', '19196', '1924', '19240', '19241', '19242', '19243', '19244', '19251', '19252', '19253', '19254', '19255', '19267', '19268', '19269', '19270', '19271', '19283', '19284', '19285', '19286', '19287', '19298', '19299', '19301', '19302', '19303', '19304', '1932', '19330', '19331', '19332', '19333', '19334', '19371', '19372', '19373', '19374', '19375', '19431', '19432', '19433', '19434', '19435', '19436', '19437', '19438', '19439', '19440', '19448', '19449', '19450', '19451', '19467', '19468', '19469', '19537', '19538', '19539', '19541', '19542', '19543', '19544', '19545', '19550', '19551', '19552', '19553', '19554', '19556', '19557', '19558', '19565', '19566', '19567', '19568', '19569', '19570', '19571', '19572', '19573', '19574', '19575', '19576', '19577', '19579', '19580', '19581', '19582', '19583', '19585', '19586', '19587', '19588', '19654', '19655', '19656', '20880', '20881', '20882', '20883', '20891', '20892', '20893', '20894', '20895', '20896', '20897', '20898', '20899', '20905', '20906', '20907', '20908', '20909', '20910', '20911', '20912', '20914', '20915', '20916', '20917', '20918', '20919', '20920', '20921', '20925', '20926', '20927', '20955', '20956', '20957', '20958', '20959', '20970', '20971', '20972', '20973', '21002', '21003', '21004', '21005', '21006', '21007', '21528', '21529', '21530', '21531', '21533', '21534', '21535', '21536', '21542', '21543', '21544', '21545', '21546', '21547', '21548', '21555', '21556', '21557', '21558', '21575', '21576', '21577', '21603', '21604', '21605', '21606', '21613', '21614', '21615', '21616', '21617', '21618', '21619', '21620', '21627', '21629', '21630', '21634', '21635', '21636', '21637', '21642', '21643', '21644', '2165', '21652', '21653', '21654', '21666', '21667', '21668', '21669', '21670', '21671', '21672', '21673', '21674', '21704', '21705', '21706', '21707', '21708', '21711', '21712', '21713', '21714', '21715', '21716', '21718', '21719', '21720', '21721', '21724', '21725', '21726', '21727', '21728', '21732', '21733', '21734', '21735', '21736', '21744', '21745', '21746', '21747', '2410', '2668', '2709', '299', '3027', '3029', '303', '3227', '3229', '3231', '3506', '3568', '3919', '4241', '437', '438', '439', '4957', '4999', '50', '5000', '5010', '5036', '5045', '5052', '51', '5389', '5431', '5686', '5767', '577', '578', '579', '5792', '5886', '598', '599', '600', '607', '608', '609', '611', '612', '6332', '6335', '6336', '6339', '6341', '6342', '6356', '6357', '6358', '6373', '6375', '6376', '6386', '6387', '6388', '6390', '6391', '6436', '6438', '6439', '6475', '6476', '6477', '6478', '6479', '6481', '6667', '6668', '6675', '6704', '6705', '6707', '6710', '6824', '6836', '6840', '694', '695', '696', '7026', '7127', '7140', '7257', '7265', '7278', '7284', '7352', '7395', '74', '75', '76', '7815', '7931', '8000', '8057', '8058', '8066', '8067', '8068', '8077', '8332', '8336', '8339', '8431', '8493', '8565', '86', '87', '88', '89', '90', '91', '9202', '95', '9561', '96', '97', '98', '9986', 'NULL']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/79mdnyy56_xc3g1jvp9wf4_80000gn/T/ipykernel_2227/2573390024.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples_check['sample_id'] = samples_check['sample_id'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "#sample_id(s) in morphology_cleaning but not in samples_check:\n",
    "# Ensure both sample_id columns are strings for accurate comparison\n",
    "df_morpho_cleaned['sample_id'] = df_morpho_cleaned['sample_id'].astype(str)\n",
    "samples_check['sample_id'] = samples_check['sample_id'].astype(str)\n",
    "\n",
    "# Get unique sample_id values in both DataFrames\n",
    "morph_sample_ids = set(df_morpho_cleaned['sample_id'].dropna())\n",
    "samples_check_ids = set(samples_check['sample_id'].dropna())\n",
    "\n",
    "# Find sample_ids in morphology_cleaning but not in samples_check\n",
    "sample_ids_not_in_check = sorted(morph_sample_ids - samples_check_ids)\n",
    "\n",
    "# Print results\n",
    "print(f\"‚úÖ Found {len(sample_ids_not_in_check)} sample_id(s) in morphology_cleaning but not in samples_check:\")\n",
    "print(sample_ids_not_in_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**consider just adding these 708 sample_id's to samples df (since they all have a related location)**\n",
    "\n",
    "as composite keys?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deciding what to do about missing sample_id's in morphology\n",
    "--> assumption: these horizons were not sampled, but just reviewed visually\n",
    "--> incorrect bc there are over 9000 sample_id's from sample df that do not have a morpho_id\n",
    "\n",
    "--> will probably just use profile record_id for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab_sample_id</th>\n",
       "      <th>analysis_id</th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>EG</th>\n",
       "      <th>thick_clay</th>\n",
       "      <th>fine_clay</th>\n",
       "      <th>silt</th>\n",
       "      <th>clay</th>\n",
       "      <th>Eq_Hum</th>\n",
       "      <th>...</th>\n",
       "      <th>Ta</th>\n",
       "      <th>W</th>\n",
       "      <th>Pt</th>\n",
       "      <th>Au</th>\n",
       "      <th>Hg</th>\n",
       "      <th>Tl</th>\n",
       "      <th>Pb</th>\n",
       "      <th>Bi</th>\n",
       "      <th>Th</th>\n",
       "      <th>U</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>10999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.700001</td>\n",
       "      <td>32.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>11000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.799999</td>\n",
       "      <td>35.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>11.4</td>\n",
       "      <td>6.4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>11001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>46.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>11.1</td>\n",
       "      <td>6.3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>11002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.599998</td>\n",
       "      <td>41.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15.4</td>\n",
       "      <td>5.2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B_101/62_5_21</td>\n",
       "      <td>B_101/62_5_2</td>\n",
       "      <td>11003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.799999</td>\n",
       "      <td>47.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>14.5</td>\n",
       "      <td>7.3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lab_sample_id    analysis_id    horizon_id sample_id  EG  thick_clay  \\\n",
       "0              1   B_101/62_1_1  B_101/62_1_1     10999 NaN   61.700001   \n",
       "1              2   B_101/62_2_1  B_101/62_2_1     11000 NaN   52.799999   \n",
       "2              3   B_101/62_3_1  B_101/62_3_1     11001 NaN   42.500000   \n",
       "3              4   B_101/62_4_1  B_101/62_4_1     11002 NaN   42.599998   \n",
       "4              5  B_101/62_5_21  B_101/62_5_2     11003 NaN   36.799999   \n",
       "\n",
       "   fine_clay  silt  clay  Eq_Hum  ...  Ta   W  Pt  Au  Hg  Tl  Pb  Bi  Th   U  \n",
       "0       32.8   0.2   5.3     4.6  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "1       35.1   0.7  11.4     6.4  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "2       46.2   0.2  11.1     6.3  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "3       41.8   0.2  15.4     5.2  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "4       47.5   1.2  14.5     7.3  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows missing analysis_id after merge: 3965\n",
      "           horizon_id sample_id_x  profile  horizon_layer  upper_depth  \\\n",
      "1399    Cb_127/59_3_0        NULL   127_59            3.0         17.0   \n",
      "7752   Mj_267c/63_1_0        NULL  267C_63            1.0          0.0   \n",
      "1969    Cb_226/59_2_0        NULL   226_59            2.0         12.0   \n",
      "7125    Mj_215/63_5_0        NULL   215_63            5.0        103.0   \n",
      "6118    Mj_122/63_4_0        NULL   122_63            4.0         36.0   \n",
      "5929   Mj_107c/63_1_0        NULL  107C_63            1.0          0.0   \n",
      "10356  UZ_244c/60_2_0        NULL  244C_60            2.0          7.0   \n",
      "6799    Mj_189/63_1_0        NULL   189_63            1.0          0.0   \n",
      "7199    Mj_221/63_1_0        NULL   221_63            1.0          0.0   \n",
      "8674    Mj_352/63_7_0        NULL   352_63            7.0        129.0   \n",
      "\n",
      "       lower_depth                                     dry_color_name dry_hue  \\\n",
      "1399          31.0                                              Pardo    10YR   \n",
      "7752          20.0                               Pardo a pardo-escuro   7,5YR   \n",
      "1969          30.0                           Pardo-acinzentado-escuro    10YR   \n",
      "7125         146.0                                Pardo-claro a pardo   7,5YR   \n",
      "6118          60.0                                            Laranja     5YR   \n",
      "5929          17.0                                Pardo a pardo-claro    10YR   \n",
      "10356         18.0                       Pardo a pardo-escuro a pardo    10YR   \n",
      "6799          15.0                                    Cinzento-escuro    10YR   \n",
      "7199          11.0  Pardo-acinzentado com tom pardo-acinzentado-es...    10YR   \n",
      "8674         185.0                                                NaN     NaN   \n",
      "\n",
      "       dry_value  dry_chroma  ...         pore_quantity        pore_diameter  \\\n",
      "1399         5.0         3.0  ...         Quase fechado                  NaN   \n",
      "7752         4.0         2.0  ...          Pouco poroso          Muito finos   \n",
      "1969         4.0         2.0  ...          Pouco poroso                  NaN   \n",
      "7125         6.0         4.0  ...          Pouco poroso          Muito finos   \n",
      "6118         5.0         6.0  ...  Moderadamente poroso  Muito finos e finos   \n",
      "5929         5.0         3.0  ...  Moderadamente poroso  Muito finos e finos   \n",
      "10356        4.5         3.0  ...          Pouco poroso                  NaN   \n",
      "6799         4.0         1.0  ...          Pouco poroso          Muito finos   \n",
      "7199         5.0         2.0  ...          Pouco poroso  Muito finos e finos   \n",
      "8674         NaN         NaN  ...               Fechado                  NaN   \n",
      "\n",
      "       pore_shape                              root_quantity  \\\n",
      "1399          NaN               Algumas finas e raras medias   \n",
      "7752          NaN              Muitas finas e algumas medias   \n",
      "1969          NaN                                    Algumas   \n",
      "7125          NaN                                     Poucas   \n",
      "6118          NaN                Poucas finas e raras medias   \n",
      "5929          NaN    Muitas finas e algumas medias e grossas   \n",
      "10356         NaN              Muitas finas e algumas medias   \n",
      "6799          NaN                                     Poucas   \n",
      "7199          NaN  Bastantes finas e poucas medias e grossas   \n",
      "8674          NaN                                 Sem raizes   \n",
      "\n",
      "                 root_diameter  \\\n",
      "1399                       NaN   \n",
      "7752                       NaN   \n",
      "1969            Finas (< 1 mm)   \n",
      "7125            Finas (< 1 mm)   \n",
      "6118                       NaN   \n",
      "5929                       NaN   \n",
      "10356                      NaN   \n",
      "6799   Finas, medias e grossas   \n",
      "7199                       NaN   \n",
      "8674                       NaN   \n",
      "\n",
      "                                         moisture_degree Unnamed: 46  \\\n",
      "1399                                          Quase seco         NaN   \n",
      "7752                                                Seco         NaN   \n",
      "1969                                                Seco         NaN   \n",
      "7125                                              Fresco         NaN   \n",
      "6118                                                Seco         NaN   \n",
      "5929                                                Seco         NaN   \n",
      "10356                                         Quase seco         NaN   \n",
      "6799                                                Seco         NaN   \n",
      "7199                                                Seco         NaN   \n",
      "8674   Muito humido a humido e muito humido com a pro...         NaN   \n",
      "\n",
      "      profile_record_id analysis_id sample_id_y  \n",
      "1399               <NA>         NaN         NaN  \n",
      "7752               <NA>         NaN         NaN  \n",
      "1969               <NA>         NaN         NaN  \n",
      "7125               <NA>         NaN         NaN  \n",
      "6118               <NA>         NaN         NaN  \n",
      "5929               <NA>         NaN         NaN  \n",
      "10356              <NA>         NaN         NaN  \n",
      "6799               <NA>         NaN         NaN  \n",
      "7199               <NA>         NaN         NaN  \n",
      "8674               <NA>         NaN         NaN  \n",
      "\n",
      "[10 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ensure sample_id columns are strings and stripped\n",
    "df_morpho_cleaned['sample_id'] = df_morpho_cleaned['sample_id'].astype(str).str.strip()\n",
    "merged1['sample_id'] = merged1['sample_id'].astype(str).str.strip()\n",
    "\n",
    "# Merge to add analysis_id, sample_id (from merged1) onto df_morpho_cleaned based on horizon_id\n",
    "df_morpho_enriched = df_morpho_cleaned.merge(\n",
    "    merged1[['horizon_id', 'analysis_id', 'sample_id']],\n",
    "    on='horizon_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Rows with missing analysis_id mean df_morpho_cleaned sample_id is not in merged1\n",
    "missing_analysis = df_morpho_enriched[df_morpho_enriched['analysis_id'].isna()]\n",
    "print(f\"Number of rows missing analysis_id after merge: {len(missing_analysis)}\")\n",
    "\n",
    "# Inspect some missing rows to understand the issue\n",
    "print(missing_analysis.sample(min(10, len(missing_analysis))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Duplicated horizon_ids in merged1: 452\n",
      "     lab_sample_id    analysis_id    horizon_id sample_id   EG  thick_clay  \\\n",
      "4                5  B_101/62_5_21  B_101/62_5_2     11003  NaN   36.799999   \n",
      "5                6  B_101/62_5_22  B_101/62_5_2     11004  NaN   45.099998   \n",
      "75              76  B_148/62_2_21  B_148/62_2_2     11285  NaN   21.799999   \n",
      "76              77  B_148/62_2_22  B_148/62_2_2     11286  NaN   22.000000   \n",
      "85              86   B_15/62_5_21   B_15/62_5_2     10843  NaN   27.100000   \n",
      "86              87   B_15/62_5_22   B_15/62_5_2     10844  NaN   27.200001   \n",
      "105            106  B_154/61_3_21  B_154/61_3_2     10335  NaN   17.799999   \n",
      "106            107  B_154/61_3_22  B_154/61_3_2     10336  NaN   18.700001   \n",
      "143            144  B_187/61_4_21  B_187/61_4_2     10428  1.0   31.900000   \n",
      "144            145  B_187/61_4_22  B_187/61_4_2     10429  1.0   33.000000   \n",
      "\n",
      "     fine_clay  silt  clay  Eq_Hum  ...  Ta   W  Pt  Au  Hg  Tl  Pb  Bi  Th  \\\n",
      "4         47.5   1.2  14.5     7.3  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "5         41.8   1.4  11.7     0.0  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "75        73.7   2.3   2.1     0.0  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "76        73.4   3.1   1.5     0.0  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "85        45.1   1.6  26.1     0.0  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "86        44.7   2.2  25.9     0.0  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "105       10.2   5.6  66.4    27.6  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "106       11.7   4.6  65.1     0.0  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "143       49.0   2.9  16.2     0.0  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "144       46.8   3.0  17.2     0.0  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "\n",
      "      U  \n",
      "4   NaN  \n",
      "5   NaN  \n",
      "75  NaN  \n",
      "76  NaN  \n",
      "85  NaN  \n",
      "86  NaN  \n",
      "105 NaN  \n",
      "106 NaN  \n",
      "143 NaN  \n",
      "144 NaN  \n",
      "\n",
      "[10 rows x 70 columns]\n"
     ]
    }
   ],
   "source": [
    "#452 duplicate horizon_id in analysis df\n",
    "# are there multiple horizon_id's per analysis_id?\n",
    "# is it an error that can resolved w new unique id\n",
    "dupes = merged1[merged1['horizon_id'].duplicated(keep=False)]\n",
    "print(f\"‚ö†Ô∏è Duplicated horizon_ids in merged1: {dupes['horizon_id'].nunique()}\")\n",
    "dupes_sorted = dupes.sort_values('horizon_id')\n",
    "print(dupes_sorted.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   total_in_morpho  total_in_merged1  dupes_in_merged1  missing_in_merged1\n",
      "0            10434              7292               506                3965\n"
     ]
    }
   ],
   "source": [
    "missing_in_merged1 = df_morpho_cleaned[~df_morpho_cleaned['horizon_id'].isin(merged1['horizon_id'])]\n",
    "\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'total_in_morpho': [df_morpho_cleaned['horizon_id'].nunique()],\n",
    "    'total_in_merged1': [merged1['horizon_id'].nunique()],\n",
    "    'dupes_in_merged1': [merged1['horizon_id'].duplicated().sum()],\n",
    "    'missing_in_merged1': [len(missing_in_merged1)]\n",
    "})\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soil Type Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soil_type_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>CEP_GR</th>\n",
       "      <th>CEP_NAME</th>\n",
       "      <th>FAO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1_51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1_57</td>\n",
       "      <td>Aridicos</td>\n",
       "      <td>Aridicos com calcario Pardo-cinzentos</td>\n",
       "      <td>CLha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   soil_type_id profile    CEP_GR                               CEP_NAME   FAO\n",
       "0             1    1_51       NaN                                    NaN   NaN\n",
       "1             2    1_57  Aridicos  Aridicos com calcario Pardo-cinzentos  CLha"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns\n",
    "soil_type = soil_profile.copy()\n",
    "\n",
    "soil_type.rename(columns={\n",
    "    'Perfil': 'profile',\n",
    "    'Agrupamento': 'grouping',\n",
    "    'Pro': 'province',\n",
    "    'Pa√≠s': 'country',\n",
    "    'Local': 'location',\n",
    "    'DATA': 'date',\n",
    "    'CEP_NOME': 'CEP_NAME',\n",
    "}, inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "soil_type_cleaning = soil_type.drop(columns=[\n",
    "    'REF', 'province', 'country', 'location', 'DESCRITOR1', 'DESCRITOR2', 'DESCRITOR3',\n",
    "    'date', 'Fase', 'D_INSER√áAO', 'Publica√ß√£o', 'WRB_old', 'Miss√£o'\n",
    "], errors='ignore')  # use errors='ignore' in case some columns were already missing\n",
    "\n",
    "# Add a new Primary Key ID column starting from 1\n",
    "soil_type_cleaning.insert(0, 'soil_type_id', range(1, len(soil_type_cleaning) + 1))\n",
    "\n",
    "# Drop accents\n",
    "import unicodedata\n",
    "\n",
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        return ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    return text\n",
    "\n",
    "# Apply to all cells in the DataFrame\n",
    "soil_type_cleaning = soil_type_cleaning.applymap(remove_accents)\n",
    "\n",
    "# Replace / with _ and strip/shorten 'profile' BEFORE slicing into site_info_clean\n",
    "soil_type_cleaning['profile'] = (\n",
    "    soil_type_cleaning['profile']\n",
    "    .astype(str)\n",
    "    .str.replace('/', '_')\n",
    "    .str.strip()\n",
    "    .str[:20]\n",
    ")\n",
    "\n",
    "# Keep only relevant columns\n",
    "soil_type_clean = soil_type_cleaning[[\n",
    "    'soil_type_id',\n",
    "    'profile',\n",
    "    'CEP_GR',\n",
    "    'CEP_NAME',\n",
    "    'FAO'\n",
    "]]\n",
    "\n",
    "# Preview\n",
    "soil_type_clean.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# check duplicates\n",
    "# Filter out and print duplicated samples (including the first occurrence)\n",
    "duplicated_values_st = soil_type_clean['profile'][soil_type_clean['profile'].duplicated()].unique()\n",
    "print(duplicated_values_st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Unique 'profile' values equal to string 'NULL': []\n",
      "üî¢ Number of 'profile' values equal to string 'NULL': 0\n"
     ]
    }
   ],
   "source": [
    "# Check for profile values that are the string \"NULL\"\n",
    "null_string_values_st = soil_type_clean[soil_type_clean['profile'] == \"NULL\"]['profile'].unique()\n",
    "print(\"üîç Unique 'profile' values equal to string 'NULL':\", null_string_values_st)\n",
    "\n",
    "null_string_count_st = (soil_type_clean['profile'] == \"NULL\").sum()\n",
    "print(f\"üî¢ Number of 'profile' values equal to string 'NULL': {null_string_count_st}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total profiles in site_info_clean: 4321\n",
      "Total profiles in soil_type_clean: 2518\n",
      "\n",
      "‚úÖ Profiles in both: 2376\n",
      "‚ùå Profiles only in site_info_clean: 1945 (45.0%)\n",
      "‚ùå Profiles only in soil_type_clean: 142 (5.6%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1. Define cleaning function for profile\n",
    "# ---------------------------------------\n",
    "def clean_profile_column(series):\n",
    "    return (\n",
    "        series.astype(str)\n",
    "        .str.replace('/', '_')\n",
    "        .str.strip()\n",
    "        .str.upper()\n",
    "        .str[:20]\n",
    "    )\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2. Safely clean 'profile' columns\n",
    "# ---------------------------------------\n",
    "site_info_clean = site_info_clean.copy()\n",
    "soil_type_clean = soil_type_clean.copy()\n",
    "\n",
    "site_info_clean.loc[:, 'profile'] = clean_profile_column(site_info_clean['profile'])\n",
    "soil_type_clean.loc[:, 'profile'] = clean_profile_column(soil_type_clean['profile'])\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3. Create sets of unique profiles\n",
    "# ---------------------------------------\n",
    "site_profiles = set(site_info_clean['profile'].unique())\n",
    "soil_profiles = set(soil_type_clean['profile'].unique())\n",
    "\n",
    "# ---------------------------------------\n",
    "# 4. Compare overlaps and mismatches\n",
    "# ---------------------------------------\n",
    "only_in_site_info = site_profiles - soil_profiles\n",
    "only_in_soil_type = soil_profiles - site_profiles\n",
    "in_both = site_profiles & soil_profiles\n",
    "\n",
    "# ---------------------------------------\n",
    "# 5. Report results\n",
    "# ---------------------------------------\n",
    "print(f\"Total profiles in site_info_clean: {len(site_profiles)}\")\n",
    "print(f\"Total profiles in soil_type_clean: {len(soil_profiles)}\")\n",
    "print()\n",
    "print(f\"‚úÖ Profiles in both: {len(in_both)}\")\n",
    "print(f\"‚ùå Profiles only in site_info_clean: {len(only_in_site_info)} \"\n",
    "      f\"({len(only_in_site_info) / len(site_profiles) * 100:.1f}%)\")\n",
    "print(f\"‚ùå Profiles only in soil_type_clean: {len(only_in_soil_type)} \"\n",
    "      f\"({len(only_in_soil_type) / len(soil_profiles) * 100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export for investigation\n",
    "\n",
    "unmatched_site_info = site_info_clean[site_info_clean['profile'].isin(only_in_site_info)]\n",
    "unmatched_site_info.to_csv(\"unmatched_profiles_site_info.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save clean table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>climate_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>ID</th>\n",
       "      <th>mean_annual_temp</th>\n",
       "      <th>mean_annual_precip</th>\n",
       "      <th>koppen_climate</th>\n",
       "      <th>thornthwaite_climate</th>\n",
       "      <th>hydric_regime</th>\n",
       "      <th>thermal_regime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1_57</td>\n",
       "      <td>2770</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arido</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1_59</td>\n",
       "      <td>48</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Humido</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1_61</td>\n",
       "      <td>1618</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1_63</td>\n",
       "      <td>881</td>\n",
       "      <td>21.5</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>Tropical chuvoso com estacao seca no Inverno, ...</td>\n",
       "      <td>Humido</td>\n",
       "      <td>Tropustico udico</td>\n",
       "      <td>Iso-Hipertermico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1_64</td>\n",
       "      <td>1750</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   climate_id profile    ID mean_annual_temp mean_annual_precip  \\\n",
       "0           1    1_57  2770                                       \n",
       "1           2    1_59    48                                       \n",
       "2           3    1_61  1618                                       \n",
       "3           4    1_63   881             21.5             1500.0   \n",
       "4           5    1_64  1750                                       \n",
       "\n",
       "                                      koppen_climate thornthwaite_climate  \\\n",
       "0                                                NaN                Arido   \n",
       "1                                                NaN               Humido   \n",
       "2                                                NaN                  NaN   \n",
       "3  Tropical chuvoso com estacao seca no Inverno, ...               Humido   \n",
       "4                                                NaN                  NaN   \n",
       "\n",
       "      hydric_regime    thermal_regime  \n",
       "0               NaN               NaN  \n",
       "1               NaN               NaN  \n",
       "2               NaN               NaN  \n",
       "3  Tropustico udico  Iso-Hipertermico  \n",
       "4               NaN               NaN  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load original Excel data\n",
    "profile_loc = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Perfis_local.xlsx\")\n",
    "\n",
    "# Create a copy for cleaning\n",
    "climate_features_cleaning = profile_loc.copy()\n",
    "\n",
    "# Mapping from climate codes to descriptions\n",
    "code_to_description = {\n",
    "    \"B1\": \"H√∫mido\",\n",
    "    \"B2\": \"H√∫mido\",\n",
    "    \"B3\": \"H√∫mido\",\n",
    "    \"B4\": \"H√∫mido\",\n",
    "    \"C1\": \"Sub-h√∫mido seco\",\n",
    "    \"C2\": \"Sub-h√∫mido chuvoso\",\n",
    "    \"D\": \"Semi-√°rido\",\n",
    "    \"E\": \"√Årido\",\n",
    "    \"Aw\": \"Tropical chuvoso com esta√ß√£o seca no Inverno, de savana\",\n",
    "    \"BSw\": \"Seco de estepe, com chuva predominante no Ver√£o\",\n",
    "    \"BWw\": \"Seco de deserto, com chuva predominante no Ver√£o\",\n",
    "    \"Cw\": \"Mesot√©rmico h√∫mido com esta√ß√£o seca no Inverno\",\n",
    "    \"ARe\": \"Ar√≠dico extremo\",\n",
    "    \"ARf\": \"Ar√≠dico fraco\",\n",
    "    \"ARt\": \"Ar√≠dico t√≠pico\",\n",
    "    \"tUDs\": \"Temp√∫dico seco\",\n",
    "    \"tUSh\": \"Temp√∫stico h√∫mido\",\n",
    "    \"tUSt\": \"Temp√∫stico t√≠pico\",\n",
    "    \"TUDs\": \"Trop√∫dico seco\",\n",
    "    \"TUSa\": \"Trop√∫stico ar√≠dico\",\n",
    "    \"TUSu\": \"Trop√∫stico √∫dico\",\n",
    "    \"TUSt\": \"Tropustico t√≠pico\",\n",
    "    \"H\": \"Hipert√©rmico\",\n",
    "    \"iH\": \"Iso-Hipert√©rmico\",\n",
    "    \"iT\": \"Iso-T√©rmico\",\n",
    "    \"T\": \"T√©rmico\"\n",
    "}\n",
    "\n",
    "# Replace climate codes with descriptions\n",
    "for col in [\"CL_THORNTH\", \"CL_KOPPEN\", \"REG_H√çDRIC\", \"REG_T√âRMIC\"]:\n",
    "    climate_features_cleaning[col] = climate_features_cleaning[col].replace(code_to_description)\n",
    "\n",
    "# Function to average values like \"21-22\" -> 21.5\n",
    "def average_range(value):\n",
    "    if isinstance(value, str) and '-' in value:\n",
    "        try:\n",
    "            nums = [float(x.strip()) for x in value.split('-')]\n",
    "            return sum(nums) / len(nums)\n",
    "        except:\n",
    "            return None\n",
    "    try:\n",
    "        return float(value)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply to temperature and precipitation columns\n",
    "climate_features_cleaning[\"TMA\"] = climate_features_cleaning[\"TMA\"].apply(average_range)\n",
    "climate_features_cleaning[\"PMA\"] = climate_features_cleaning[\"PMA\"].apply(average_range)\n",
    "\n",
    "# Rename columns for clarity\n",
    "climate_features_cleaning.rename(columns={\n",
    "    'PERFIL': 'profile',\n",
    "    'CL_THORNTH': 'thornthwaite_climate',\n",
    "    'CL_KOPPEN': 'koppen_climate',\n",
    "    'TMA': 'mean_annual_temp',\n",
    "    'PMA': 'mean_annual_precip',\n",
    "    'REG_H√çDRIC': 'hydric_regime',\n",
    "    'REG_T√âRMIC': 'thermal_regime'\n",
    "}, inplace=True)\n",
    "\n",
    "# ensure consistent profile formatting\n",
    "climate_features_cleaning['profile'] = climate_features_cleaning['profile'].astype(str).str.replace('/', '_').str.strip().str[:20]\n",
    "\n",
    "# Drop accents from text values\n",
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        return ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    return text\n",
    "\n",
    "climate_features_cleaning = climate_features_cleaning.applymap(remove_accents)\n",
    "\n",
    "# Add primary key column\n",
    "climate_features_cleaning.insert(0, 'climate_id', range(1, len(climate_features_cleaning) + 1))\n",
    "\n",
    "# Replace empty strings or nulls in numeric columns with \\N (Postgres null)\n",
    "for col in ['mean_annual_temp', 'mean_annual_precip']:\n",
    "    climate_features_cleaning[col] = climate_features_cleaning[col].replace(\n",
    "        ['', ' ', 'NULL', None, pd.NA, pd.NaT, 'nan', float('nan')], ''\n",
    "    )\n",
    "\n",
    "# Select and reorder relevant columns for export\n",
    "climate_features_clean = climate_features_cleaning[[\n",
    "    'climate_id',\n",
    "    'profile',\n",
    "    'ID',\n",
    "    'mean_annual_temp',\n",
    "    'mean_annual_precip',\n",
    "    'koppen_climate',\n",
    "    'thornthwaite_climate',\n",
    "    'hydric_regime',\n",
    "    'thermal_regime'\n",
    "]]\n",
    "\n",
    "# Preview\n",
    "climate_features_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check FK relationships and datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of each column in profile_clean:\n",
      "\n",
      "climate_id               int64\n",
      "profile                 object\n",
      "ID                       int64\n",
      "mean_annual_temp        object\n",
      "mean_annual_precip      object\n",
      "koppen_climate          object\n",
      "thornthwaite_climate    object\n",
      "hydric_regime           object\n",
      "thermal_regime          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check and print the datatypes of each column\n",
    "print(\"Data types of each column in profile_clean:\\n\")\n",
    "print(climate_features_clean.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topo features table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topo_features_id</th>\n",
       "      <th>ID</th>\n",
       "      <th>profile</th>\n",
       "      <th>slope_code</th>\n",
       "      <th>altitude</th>\n",
       "      <th>aspect</th>\n",
       "      <th>land_surface_temp</th>\n",
       "      <th>dem_elevation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2770</td>\n",
       "      <td>1_57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>1_59</td>\n",
       "      <td>D5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1618</td>\n",
       "      <td>1_61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>881</td>\n",
       "      <td>1_63</td>\n",
       "      <td>D1</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1750</td>\n",
       "      <td>1_64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topo_features_id    ID profile slope_code  altitude aspect  \\\n",
       "0                 1  2770    1_57        NaN      32.0   <NA>   \n",
       "1                 2    48    1_59         D5       NaN   <NA>   \n",
       "2                 3  1618    1_61        NaN       NaN   <NA>   \n",
       "3                 4   881    1_63         D1    1210.0   <NA>   \n",
       "4                 5  1750    1_64        NaN       NaN   <NA>   \n",
       "\n",
       "  land_surface_temp dem_elevation  \n",
       "0              <NA>          <NA>  \n",
       "1              <NA>          <NA>  \n",
       "2              <NA>          <NA>  \n",
       "3              <NA>          <NA>  \n",
       "4              <NA>          <NA>  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a copy for cleaning\n",
    "topo_features_cleaning = profile_loc.copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "topo_features_cleaning.rename(columns={\n",
    "    'PERFIL': 'profile',\n",
    "    'TOPOGRAFIA': 'slope_code',\n",
    "    'ALTITUDE': 'altitude',\n",
    "}, inplace=True)\n",
    "\n",
    "# ensure consistent profile formatting\n",
    "topo_features_cleaning['profile'] = topo_features_cleaning['profile'].astype(str).str.replace('/', '_').str.strip().str[:20]\n",
    "\n",
    "# Add missing columns\n",
    "topo_features_cleaning['aspect'] = pd.NA  \n",
    "topo_features_cleaning['land_surface_temp'] = pd.NA\n",
    "topo_features_cleaning['dem_elevation'] = pd.NA  \n",
    "\n",
    "# Add primary key column\n",
    "topo_features_cleaning.insert(0, 'topo_features_id', range(1, len(topo_features_cleaning) + 1))\n",
    "\n",
    "# Create slope class mapping dictionary\n",
    "slope_code_to_description = {\n",
    "    \"D1\": \"Plano (Declives < 2%)\",\n",
    "    \"D2\": \"Ondulado muito suave (Declives > 2% e < 3%)\",\n",
    "    \"D3\": \"Ondulado suave (Declives > 3% e < 5%)\",\n",
    "    \"D4\": \"Ondulado (Declives > 5% e < 8%)\",\n",
    "    \"D5\": \"Acidentado (Declives > 8% e < 15%)\",\n",
    "    \"D6\": \"Escarpado (Declives >15% e < 30%)\",\n",
    "    \"D7\": \"Montanhoso (Declives > 30%)\"\n",
    "}\n",
    "\n",
    "# Create a mapping DataFrame for slope classes\n",
    "slope_classes_df = pd.DataFrame([\n",
    "    {\"slope_code\": code, \"slope_description\": desc}\n",
    "    for code, desc in slope_code_to_description.items()\n",
    "])\n",
    "\n",
    "#drop accents\n",
    "import unicodedata\n",
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        # Normalize and remove diacritics\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "# Apply to all cells in the DataFrame\n",
    "topo_features_cleaning = topo_features_cleaning.applymap(remove_accents)\n",
    "\n",
    "# Final cleaned topo features table (referencing slope_code, not description)\n",
    "topo_features_clean = topo_features_cleaning[[\n",
    "    'topo_features_id',\n",
    "    'ID',\n",
    "    'profile', \n",
    "    'slope_code',\n",
    "    'altitude',\n",
    "    'aspect',\n",
    "    'land_surface_temp',\n",
    "    'dem_elevation'\n",
    "]]\n",
    "\n",
    "# Preview\n",
    "topo_features_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geological features table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/79mdnyy56_xc3g1jvp9wf4_80000gn/T/ipykernel_2227/2818143663.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  geo_features_clean['profile'] = geo_features_clean['profile'].astype(str).str.replace('/', '_').str.strip().str[:20]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_features_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>ID</th>\n",
       "      <th>geology_id</th>\n",
       "      <th>lithology_id</th>\n",
       "      <th>lithology_1954_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1_57</td>\n",
       "      <td>2770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1_59</td>\n",
       "      <td>48</td>\n",
       "      <td>Oendolongo</td>\n",
       "      <td>pp</td>\n",
       "      <td>Sistema do Maiombe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1_61</td>\n",
       "      <td>1618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1_63</td>\n",
       "      <td>881</td>\n",
       "      <td>Karroo</td>\n",
       "      <td>Cs/Cal</td>\n",
       "      <td>Serie de Cassanje - T2'T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1_64</td>\n",
       "      <td>1750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   geo_features_id profile    ID  geology_id lithology_id  \\\n",
       "0                1    1_57  2770         NaN            d   \n",
       "1                2    1_59    48  Oendolongo           pp   \n",
       "2                3    1_61  1618         NaN          NaN   \n",
       "3                4    1_63   881      Karroo       Cs/Cal   \n",
       "4                5    1_64  1750         NaN          NaN   \n",
       "\n",
       "           lithology_1954_id  \n",
       "0                        NaN  \n",
       "1         Sistema do Maiombe  \n",
       "2                        NaN  \n",
       "3  Serie de Cassanje - T2'T1  \n",
       "4                        NaN  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load cleaned geo features data\n",
    "geo_features_cleaning = pd.read_excel(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_soil_database/Perfis_local.xlsx\")\n",
    "\n",
    "# Rename columns\n",
    "geo_features_cleaning.rename(columns={\n",
    "    'PERFIL': 'profile',\n",
    "    'GEOLOGIA': 'geology_id',\n",
    "    'LITOLOGIA': 'lithology_id',\n",
    "    'LITOLOGIA_1954': 'lithology_1954_id',\n",
    "}, inplace=True)\n",
    "\n",
    "# Add primary key column\n",
    "geo_features_cleaning.insert(0, 'geo_features_id', range(1, len(geo_features_cleaning) + 1))\n",
    "\n",
    "# Final normalized geo_features table (with codes as foreign keys)\n",
    "geo_features_clean = geo_features_cleaning[[\n",
    "    'geo_features_id',\n",
    "    'profile',\n",
    "    'ID',\n",
    "    'geology_id',\n",
    "    'lithology_id',\n",
    "    'lithology_1954_id'\n",
    "]]\n",
    "\n",
    "# ensure consistent profile formatting\n",
    "geo_features_clean['profile'] = geo_features_clean['profile'].astype(str).str.replace('/', '_').str.strip().str[:20]\n",
    "\n",
    "#drop accents\n",
    "import unicodedata\n",
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        # Normalize and remove diacritics\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "# Apply to all cells in the DataFrame\n",
    "geo_features_clean = geo_features_clean.applymap(remove_accents)\n",
    "\n",
    "#preview\n",
    "geo_features_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mapping tables\n",
    "# Mappings for geology\n",
    "geology_mapping = {\n",
    "    \"Kalahari\": \"Sistema do Kalahari\",\n",
    "    \"Superficiais\": \"Forma√ß√µes Superficiais\",\n",
    "    \"Karroo\": \"Sistema do Karroo\",\n",
    "    \"Bembe\": \"Sistema do Bembe\",\n",
    "    \"Oendolongo\": \"Sistema do Oendolongo\",\n",
    "    \"Base\": \"Complexo de base\",\n",
    "    \"Proteroz√≥ico\": \"Proteroz√≥ico\",\n",
    "    \"Pleistoc√©nico\": \"Pleistoc√©nico\",\n",
    "    \"Terci√°rio\": \"Terci√°rio (m√©dio e inferior)\",\n",
    "    \"TQ\": \"Quatern√°rio e Terci√°rio superior\",\n",
    "    \"Cret√°cio\": \"\",  # no description provided\n",
    "    \"RPKS\": \"Recente Plistoc√©nico e Kalahari Superior\"\n",
    "}\n",
    "\n",
    "# Mappings for lithology_1954\n",
    "lithology_1954_mapping = {\n",
    "    \"Œ≥\": \"Granitos, Granodioritos e Quartzodioritos\",\n",
    "    \"PL\": \"Xistos, metaquartzitos, conglomerados, arcoses, ect.\",\n",
    "    \"Œª\": \"Rochas eruptivas indeterminadas\",\n",
    "    \"Œ¥p\": \"Doleritos, doleritos pigeon√≠ticos\",\n",
    "    \"Œ¥ab\": \"Diabases, diabases albito-cloriticas\",\n",
    "    \"Œµ\": \"Noritos, gabros e peridotitos\",\n",
    "    \"JK\": \"Composto de conglomerados, areias, cascalhos do Kalahari\",\n",
    "    \"C\": \"S√©rie Xisto - calc√°ria\",\n",
    "    \"Cal\": \"Sedimentos arenosos n√£o consolidados\",\n",
    "    \"K\": \"S√©rie xisto - gresosa\",\n",
    "    \"RT\": \"N√£o diferenciado\",\n",
    "    \"œÉ\": \"Sienitos, sienitos nefel√≠nicos\",\n",
    "    \"Q\": \"Dep√≥sitos fossil√≠feros\",\n",
    "    \"CS\": \"Grande conglomerado e s√©rie de Mwashya\"\n",
    "}\n",
    "\n",
    "# Mappings for lithology\n",
    "lithology_mapping = {\n",
    "    \"a\": \"Rochas aren√°ceas consolidadas\",\n",
    "    \"aq\": \"Gr√©s quartz√≠ticos do Oendolongo\",\n",
    "    \"b\": \"Rochas eruptivas b√°sicas\",\n",
    "    \"c\": \"Rochas sedimentares consolidadas calc√°rias\",\n",
    "    \"c'\": \"Rochas sedimentares n√£o consolidadas calc√°rias\",\n",
    "    \"cg\": \"Rochas cristalof√≠licas argil√°ceas\",\n",
    "    \"d\": \"Sedimentos n√£o consolidados de origem marinha\",\n",
    "    \"dc\": \"Dep√≥sitos coluvionares\",\n",
    "    \"dr\": \"Dior√≠tos\",\n",
    "    \"e\": \"Rochas sedimentares consolidadas n√£o calc√°rias\",\n",
    "    \"g\": \"Rochas argil√°ceas consolidadas n√£o calc√°rias\",\n",
    "    \"g'\": \"Rochas argil√°ceas n√£o consolidadas n√£o calc√°rias\",\n",
    "    \"g''\": \"Rochas cristalinas pouco micas em quartzo\",\n",
    "    \"gp\": \"Rochas do  complexo gabro-plagioclast√≠co\",\n",
    "    \"k\": \"Sedimentos n√£o consolidados grosseiros do Kalahari\",\n",
    "    \"m\": \"Rochas sedimentares n√£o consolidadas calco-gips√≠feras\",\n",
    "    \"m'\": \"Dep√≥sitos coluvionares margosos\",\n",
    "    \"mm\": \"Materiais mistos\",\n",
    "    \"n\": \"Sedimentos n√£o consolidados de origem continental\",\n",
    "    \"nd\": \"n√£o descrito\",\n",
    "    \"pp\": \"Sedimentos n√£o consolidados grosseiros plio-plistoc√©nicos\",\n",
    "    \"q\": \"Rochas cristalinas quartz√≠feras\",\n",
    "    \"q'\": \"Materiais redistribu√≠dos provenientes de desagrega√ß√£o rochas crist. quartz√≠feras\",\n",
    "    \"qf\": \"Quartzitos ferruginosos do Oendolongo\",\n",
    "    \"r\": \"Sedimentos grosseiros n√£o especificados\",\n",
    "    \"s\": \"Sienitos\",\n",
    "    \"sx\": \"Forma√ß√µes (ou rochas) sedimentares n√£o especificadas\",\n",
    "    \"sx1\": \"Rochas sedimentares consolidadas com e sem calc√°rio\",\n",
    "    \"sx2\": \"Rochas sedimentares consolidadas\",\n",
    "    \"v\": \"Materiais vulc√¢nicos\",\n",
    "    \"v'\": \"Rochas do complexo alcalino e/ou carboat√≠tico\",\n",
    "    \"x\": \"Rochas consolidadas n√£o especificadas\",\n",
    "    \"xm\": \"Xistos metam√≥rficos\",\n",
    "    \"xq\": \"Rochas cristalinas n√£o especificadas\",\n",
    "    \"z\": \"Rochas metassedimentares\"\n",
    "}\n",
    "\n",
    "# Save each mapping as a DataFrame\n",
    "pd.DataFrame([\n",
    "    {\"geology_code\": k, \"geology_description\": v}\n",
    "    for k, v in geology_mapping.items()\n",
    "]).to_csv(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_clean/geology_mapping.csv\", index=False)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"lithology_code\": k, \"lithology_description\": v}\n",
    "    for k, v in lithology_mapping.items()\n",
    "]).to_csv(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_clean/lithology_mapping.csv\", index=False)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"lithology_1954_code\": k, \"lithology_1954_description\": v}\n",
    "    for k, v in lithology_1954_mapping.items()\n",
    "]).to_csv(\"/Users/inesschwartz/GreenDataScience/Thesis/tables_clean/lithology1954_mapping.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# District table\n",
    "\n",
    "**don't need**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do I need to make a separate table??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minerology info table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biology Info table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking CSV Datatypes before DB export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ File: analyses.csv\n",
      "lab_sample_id      int64\n",
      "sample_id        float64\n",
      "EG               float64\n",
      "thick_clay       float64\n",
      "fine_clay        float64\n",
      "                  ...   \n",
      "Tl               float64\n",
      "Pb               float64\n",
      "Bi               float64\n",
      "Th               float64\n",
      "U                float64\n",
      "Length: 68, dtype: object\n",
      "\n",
      "üìÑ File: soil_type_clean.csv\n",
      "soil_type_id     int64\n",
      "profile         object\n",
      "CEP_GR          object\n",
      "CEP_NAME        object\n",
      "FAO             object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: analyses_clean.csv\n",
      "lab_sample_id      int64\n",
      "analysis_id       object\n",
      "horizon_id        object\n",
      "sample_id         object\n",
      "EG               float64\n",
      "                  ...   \n",
      "Tl               float64\n",
      "Pb               float64\n",
      "Bi               float64\n",
      "Th               float64\n",
      "U                float64\n",
      "Length: 70, dtype: object\n",
      "\n",
      "üìÑ File: profile_record.csv\n",
      "profile_record_id      int64\n",
      "profile               object\n",
      "site_info_id         float64\n",
      "soil_type_id         float64\n",
      "sample_id              int64\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: soil_type.csv\n",
      "soil_type_id     int64\n",
      "profile         object\n",
      "CEP_GR          object\n",
      "CEP_NAME        object\n",
      "FAO             object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: morpho_cleaned.csv\n",
      "Unnamed: 0                              int64\n",
      "horizon_id                             object\n",
      "sample_id                             float64\n",
      "profile                                object\n",
      "horizon_layer                         float64\n",
      "upper_depth                           float64\n",
      "lower_depth                           float64\n",
      "dry_color_name                         object\n",
      "dry_hue                                object\n",
      "dry_value                             float64\n",
      "dry_chroma                            float64\n",
      "moist_color_name                       object\n",
      "moist_hue                              object\n",
      "moist_value                           float64\n",
      "moist_chroma                          float64\n",
      "Manchas                                object\n",
      "texture                                object\n",
      "Abund√¢ncia de elementos grosseiros     object\n",
      "Forma de elementos grosseiros          object\n",
      "Natureza de elementos grosseiros       object\n",
      "structure_type                         object\n",
      "structure_class                        object\n",
      "structure_degree                       object\n",
      "compaction                             object\n",
      "durability                             object\n",
      "pore_quantity                          object\n",
      "pore_diameter                          object\n",
      "pore_shape                            float64\n",
      "root_quantity                          object\n",
      "root_diameter                          object\n",
      "moisture_degree                        object\n",
      "Unnamed: 46                           float64\n",
      "profile_record_id                     float64\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: geology_mapping.csv\n",
      "geology_code           object\n",
      "geology_description    object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: topo_feat_clean.csv\n",
      "topo_features_id       int64\n",
      "slope_code            object\n",
      "altitude             float64\n",
      "aspect               float64\n",
      "land_surface_temp    float64\n",
      "dem_elevation        float64\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: lithology1954_mapping.csv\n",
      "lithology_1954_code           object\n",
      "lithology_1954_description    object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: samples.csv\n",
      "sample_id         int64\n",
      "site_info_id     object\n",
      "profile          object\n",
      "horizon_id       object\n",
      "year            float64\n",
      "shelf            object\n",
      "room             object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: morphology_horizon.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/79mdnyy56_xc3g1jvp9wf4_80000gn/T/ipykernel_2227/3654665737.py:16: DtypeWarning: Columns (13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horizon_id            object\n",
      "sample_id            float64\n",
      "profile_record_id    float64\n",
      "horizon_layer        float64\n",
      "upper_depth          float64\n",
      "lower_depth          float64\n",
      "moisture_degree       object\n",
      "root_quantity         object\n",
      "root_diameter         object\n",
      "texture               object\n",
      "structure_type        object\n",
      "structure_class       object\n",
      "structure_degree      object\n",
      "pore_diameter         object\n",
      "pore_quantity         object\n",
      "pore_shape            object\n",
      "dry_color_name        object\n",
      "dry_hue               object\n",
      "dry_value            float64\n",
      "dry_chroma           float64\n",
      "moist_color_name      object\n",
      "moist_hue             object\n",
      "moist_value          float64\n",
      "moist_chroma         float64\n",
      "compaction            object\n",
      "durability            object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: site_info.csv\n",
      "site_info_id          int64\n",
      "profile              object\n",
      "X_coord             float64\n",
      "Y_coord             float64\n",
      "district             object\n",
      "geo_features_id       int64\n",
      "climate_id            int64\n",
      "topo_features_id      int64\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: profile_record_clean.csv\n",
      "profile_record_id      int64\n",
      "profile               object\n",
      "site_info_id         float64\n",
      "soil_type_id         float64\n",
      "sample_id              int64\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: morphology_horizon_clean.csv\n",
      "horizon_id            object\n",
      "sample_id            float64\n",
      "profile_record_id    float64\n",
      "horizon_layer        float64\n",
      "upper_depth          float64\n",
      "lower_depth          float64\n",
      "moisture_degree       object\n",
      "root_quantity         object\n",
      "root_diameter         object\n",
      "texture               object\n",
      "structure_type        object\n",
      "structure_class       object\n",
      "structure_degree      object\n",
      "pore_diameter         object\n",
      "pore_quantity         object\n",
      "pore_shape            object\n",
      "dry_color_name        object\n",
      "dry_hue               object\n",
      "dry_value            float64\n",
      "dry_chroma           float64\n",
      "moist_color_name      object\n",
      "moist_hue             object\n",
      "moist_value          float64\n",
      "moist_chroma         float64\n",
      "compaction            object\n",
      "durability            object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: lithology_mapping.csv\n",
      "lithology_code           object\n",
      "lithology_description    object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: topo_feat.csv\n",
      "topo_features_id       int64\n",
      "slope_code            object\n",
      "altitude             float64\n",
      "aspect               float64\n",
      "land_surface_temp    float64\n",
      "dem_elevation        float64\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: climate_feat_clean.csv\n",
      "climate_id                int64\n",
      "mean_annual_temp        float64\n",
      "mean_annual_precip      float64\n",
      "koppen_climate           object\n",
      "thornthwaite_climate     object\n",
      "hydric_regime            object\n",
      "thermal_regime           object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: geo_feat.csv\n",
      "geo_features_id       int64\n",
      "geology_id           object\n",
      "lithology_id         object\n",
      "lithology_1954_id    object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: site_info_clean.csv\n",
      "site_info_id          int64\n",
      "profile              object\n",
      "X_coord             float64\n",
      "Y_coord             float64\n",
      "district             object\n",
      "geo_features_id       int64\n",
      "climate_id            int64\n",
      "topo_features_id      int64\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: samples_clean.csv\n",
      "sample_id         int64\n",
      "site_info_id     object\n",
      "profile          object\n",
      "horizon_id       object\n",
      "year            float64\n",
      "shelf            object\n",
      "room             object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: districts_clean.csv\n",
      "district_id     int64\n",
      "district       object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: geo_feat_clean.csv\n",
      "geo_features_id       int64\n",
      "geology_id           object\n",
      "lithology_id         object\n",
      "lithology_1954_id    object\n",
      "dtype: object\n",
      "\n",
      "üìÑ File: climate_feat.csv\n",
      "climate_id                int64\n",
      "mean_annual_temp        float64\n",
      "mean_annual_precip      float64\n",
      "koppen_climate           object\n",
      "thornthwaite_climate     object\n",
      "hydric_regime            object\n",
      "thermal_regime           object\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/79mdnyy56_xc3g1jvp9wf4_80000gn/T/ipykernel_2227/3654665737.py:16: DtypeWarning: Columns (13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set the path to your folder containing the 10 CSV files\n",
    "csv_folder = \"/Users/inesschwartz/GreenDataScience/Thesis/tables_clean\"  \n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(csv_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# Loop through each file and display column data types\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(csv_folder, file)\n",
    "    print(f\"\\nüìÑ File: {file}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(df.dtypes)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foreign key imports and datatype consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## standardize datatypes for identifiers\n",
    "def convert_identifiers_to_string(df, id_columns):\n",
    "    \"\"\"\n",
    "    Converts specified identifier columns in a DataFrame to string type,\n",
    "    safely handling float64 values and preserving missing values (NA).\n",
    "    Avoids SettingWithCopyWarning.\n",
    "    \"\"\"\n",
    "    df = df.copy()  # ensure we're working with a copy, not a slice\n",
    "    for col in id_columns:\n",
    "        if col in df.columns:\n",
    "            df.loc[:, col] = df[col].apply(\n",
    "                lambda x: str(int(x)) if pd.notna(x) and isinstance(x, float) and x.is_integer()\n",
    "                else str(x) if pd.notna(x)\n",
    "                else pd.NA\n",
    "            ).astype(\"string\")\n",
    "    return df\n",
    "\n",
    "#usage\n",
    "\n",
    "# Define identifier columns\n",
    "identifier_columns = [\n",
    "    'sample_id', 'site_info_id', 'profile',\n",
    "    'horizon_id', 'lab_sample_id', 'lab_sample_id', 'climate_id', 'geo_features_id', 'topo_features_id', 'profile_record_id'\n",
    "]\n",
    "\n",
    "# Apply to each relevant dataframe\n",
    "samples_check = convert_identifiers_to_string(samples_check, identifier_columns)\n",
    "df_morpho_cleaned = convert_identifiers_to_string(df_morpho_cleaned, identifier_columns)\n",
    "profile_clean = convert_identifiers_to_string(profile_record_clean, identifier_columns)\n",
    "merged1 = convert_identifiers_to_string(merged1, identifier_columns)\n",
    "site_info_clean = convert_identifiers_to_string(site_info_clean, identifier_columns)\n",
    "#district_clean = convert_identifiers_to_string(district_clean, identifier_columns)\n",
    "soil_type_clean = convert_identifiers_to_string(soil_type_clean, identifier_columns)\n",
    "geo_features_clean = convert_identifiers_to_string(geo_features_clean, identifier_columns)\n",
    "topo_features_clean = convert_identifiers_to_string(topo_features_clean, identifier_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "samples column types:\n",
      "  sample_id: object\n",
      "  site_info_id: object\n",
      "  profile: object\n",
      "  horizon_id: object\n",
      "  profile_record_id: object\n",
      "\n",
      "horizon column types:\n",
      "  sample_id: object\n",
      "  profile: object\n",
      "  horizon_id: object\n",
      "  profile_record_id: object\n",
      "\n",
      "profile column types:\n",
      "  sample_id: object\n",
      "  site_info_id: object\n",
      "  profile: object\n",
      "  profile_record_id: string\n",
      "\n",
      "lab_analysis column types:\n",
      "  sample_id: object\n",
      "  horizon_id: object\n",
      "  lab_sample_id: string\n",
      "  lab_sample_id: string\n",
      "\n",
      "site_info column types:\n",
      "  site_info_id: string\n",
      "  profile: object\n",
      "  climate_id: object\n",
      "\n",
      "soil_type column types:\n",
      "  profile: object\n",
      "\n",
      "geo_features column types:\n",
      "  profile: object\n",
      "  geo_features_id: string\n",
      "\n",
      "topo_features column types:\n",
      "  profile: object\n",
      "  topo_features_id: string\n"
     ]
    }
   ],
   "source": [
    "# Check types of identifier columns in each dataframe\n",
    "dfs = {\n",
    "    \"samples\": samples_check,\n",
    "    \"horizon\": df_morpho_cleaned,\n",
    "    \"profile\": profile_clean,\n",
    "    \"lab_analysis\": merged1,\n",
    "    \"site_info\": site_info_clean,\n",
    "    #\"district_clean\": district_clean,\n",
    "    \"soil_type\": soil_type_clean,\n",
    "    \"geo_features\": geo_features_clean,\n",
    "    \"topo_features\": topo_features_clean,\n",
    "}\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{name} column types:\")\n",
    "    for col in identifier_columns:\n",
    "        if col in df.columns:\n",
    "            print(f\"  {col}: {df[col].dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9704 sample_id(s) in samples_check are missing in horizon.\n",
      "      sample_id\n",
      "1           631\n",
      "3           633\n",
      "7           697\n",
      "8           698\n",
      "9           699\n",
      "...         ...\n",
      "14710     18867\n",
      "14711     18868\n",
      "14712     18869\n",
      "14713     18870\n",
      "14714     18871\n",
      "\n",
      "[9704 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "## validate FK references in tables\n",
    "\n",
    "samples_check['sample_id'] = samples_check['sample_id'].astype(str).str.strip().str.replace('.0', '')\n",
    "df_morpho_cleaned['sample_id'] = df_morpho_cleaned['sample_id'].astype(str).str.strip().str.replace('.0', '')\n",
    "\n",
    "# Check for missing FK references in morphology\n",
    "missing_samples = samples_check[~samples_check['sample_id'].isin(df_morpho_cleaned['sample_id'])]\n",
    "\n",
    "print(f\"{len(missing_samples)} sample_id(s) in samples_check are missing in horizon.\")\n",
    "print(missing_samples[['sample_id']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sample_id\n",
      "0       2044\n",
      "1      15936\n",
      "2      17780\n",
      "3       5288\n",
      "4      13422\n",
      "5      10276\n",
      "6      12228\n",
      "7       9094\n",
      "8      15374\n",
      "9       7359\n",
      "10      1762\n",
      "11     17363\n",
      "12     12169\n",
      "13     18754\n",
      "14      2236\n",
      "15      9709\n",
      "16     16539\n",
      "17     17941\n",
      "18     14442\n",
      "19      7699\n",
      "20     11951\n",
      "21     15690\n",
      "22     11558\n",
      "23      1823\n",
      "24      6124\n"
     ]
    }
   ],
   "source": [
    "#inspecting a random 25 samples that are missing in horizon\n",
    "\n",
    "# Print random sample of 25 missing sample_ids\n",
    "sample_subset = missing_samples.sample(n=25, random_state=42)  # Set random_state for reproducibility\n",
    "print(sample_subset[['sample_id']].reset_index(drop=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9704 samples could not be matched with a horizon_id.\n"
     ]
    }
   ],
   "source": [
    "# Samples table horizon_id FK \n",
    "\n",
    "# Drop old horizon_id if it exists (to avoid confusion)\n",
    "samples_check = samples_check.drop(columns=['horizon_id'], errors='ignore')\n",
    "\n",
    "samples_check['sample_id'] = samples_check['sample_id'].astype(str).str.strip().str.replace('.0', '', regex=False)\n",
    "df_morpho_cleaned['sample_id'] = df_morpho_cleaned['sample_id'].astype(str).str.strip().str.replace('.0', '', regex=False)\n",
    "\n",
    "\n",
    "# Merge horizon_id from morph into samples\n",
    "samples_check = samples_check.merge(\n",
    "    df_morpho_cleaned[['sample_id', 'horizon_id']],\n",
    "    on='sample_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "missing = samples_check[samples_check['horizon_id'].isna()]\n",
    "print(f\"{len(missing)} samples could not be matched with a horizon_id.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>year</th>\n",
       "      <th>shelf</th>\n",
       "      <th>room</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>630</td>\n",
       "      <td>172</td>\n",
       "      <td>139</td>\n",
       "      <td>Hb_139/46_1_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>631</td>\n",
       "      <td>173</td>\n",
       "      <td>139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>174</td>\n",
       "      <td>139</td>\n",
       "      <td>Hb_139/46_3_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>633</td>\n",
       "      <td>175</td>\n",
       "      <td>139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>687</td>\n",
       "      <td>1034</td>\n",
       "      <td>208</td>\n",
       "      <td>Hb_208/46_1_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id site_info_id profile     horizon_id    year shelf room\n",
       "0       630          172     139  Hb_139/46_1_1  1946.0     1   22\n",
       "1       631          173     139            NaN  1946.0     1   22\n",
       "2       632          174     139  Hb_139/46_3_1  1946.0     1   22\n",
       "3       633          175     139            NaN  1946.0     1   22\n",
       "4       687         1034     208  Hb_208/46_1_1  1946.0     1   22"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-order and select relevant columns\n",
    "samples_clean1 = samples_check[[\n",
    "    'sample_id',\n",
    "    'site_info_id', \n",
    "    'profile',\n",
    "    'horizon_id',\n",
    "    'year',\n",
    "    'shelf',\n",
    "    'room'  # Ensure this matches the column name in your DataFrame\n",
    "]].copy()\n",
    "\n",
    "# Preview the result\n",
    "samples_clean1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>year</th>\n",
       "      <th>shelf</th>\n",
       "      <th>room</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>630</td>\n",
       "      <td>172</td>\n",
       "      <td>139</td>\n",
       "      <td>Hb_139/46_1_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>631</td>\n",
       "      <td>173</td>\n",
       "      <td>139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>174</td>\n",
       "      <td>139</td>\n",
       "      <td>Hb_139/46_3_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>633</td>\n",
       "      <td>175</td>\n",
       "      <td>139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>687</td>\n",
       "      <td>1034</td>\n",
       "      <td>208</td>\n",
       "      <td>Hb_208/46_1_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id site_info_id profile     horizon_id    year shelf room\n",
       "0       630          172     139  Hb_139/46_1_1  1946.0     1   22\n",
       "1       631          173     139            NaN  1946.0     1   22\n",
       "2       632          174     139  Hb_139/46_3_1  1946.0     1   22\n",
       "3       633          175     139            NaN  1946.0     1   22\n",
       "4       687         1034     208  Hb_208/46_1_1  1946.0     1   22"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_clean1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## completing profile_record_clean table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_record_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>soil_type_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>208</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   profile_record_id profile site_info_id sample_id soil_type_id\n",
       "0                  1     139         <NA>      <NA>         <NA>\n",
       "1                  2     139         <NA>      <NA>         <NA>\n",
       "2                  3     139         <NA>      <NA>         <NA>\n",
       "3                  4     139         <NA>      <NA>         <NA>\n",
       "4                  5     208         <NA>      <NA>         <NA>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_record_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ID'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tp/79mdnyy56_xc3g1jvp9wf4_80000gn/T/ipykernel_2227/388914968.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'site_info_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sample_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'soil_type_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Merge site_info_id (one-to-one or many-to-one) using 'ID'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m profile_record_clean = profile_record_clean.merge(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msite_info_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'profile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'site_info_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   9839\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9840\u001b[0m     ) -> DataFrame:\n\u001b[1;32m   9841\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9843\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m   9844\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9845\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9846\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mindicator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m ) -> DataFrame:\n\u001b[0;32m--> 148\u001b[0;31m     op = _MergeOperation(\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    733\u001b[0m         (\n\u001b[1;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;31m# to avoid incompatible dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1774\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ID'"
     ]
    }
   ],
   "source": [
    "# Drop old foreign key columns if they exist\n",
    "profile_record_clean = profile_record_clean.drop(\n",
    "    columns=['site_info_id', 'sample_id', 'soil_type_id'], errors='ignore'\n",
    ")\n",
    "\n",
    "# Merge site_info_id (one-to-one or many-to-one) using 'ID'\n",
    "profile_record_clean = profile_record_clean.merge(\n",
    "    site_info_clean[['profile', 'site_info_id']],\n",
    "    on='ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge soil_type_id (one-to-one or many-to-one) using 'profile'\n",
    "profile_record_clean = profile_record_clean.merge(\n",
    "    soil_type_clean[['profile', 'soil_type_id']],\n",
    "    on='profile',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge sample_id (one-to-one or many-to-one) using 'profile'\n",
    "profile_record_clean = profile_record_clean.merge(\n",
    "    samples_clean1[['profile', 'sample_id']],\n",
    "    on='profile',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "#drop PK and replace w new profile_record_id PK\n",
    "# Drop old FK columns if they exist\n",
    "profile_record_clean = profile_record_clean.drop(\n",
    "    columns=['profile_record_id' ],\n",
    "    errors='ignore'\n",
    ")\n",
    "\n",
    "# Ensure primary key 'profile_record_id' exists\n",
    "if 'profile_record_id' not in profile_record_clean.columns:\n",
    "    profile_record_clean.insert(0, 'profile_record_id', range(1, len(profile_record_clean) + 1))\n",
    "\n",
    "profile_record_clean['profile_record_id'] = profile_record_clean['profile_record_id'].astype(str)\n",
    "\n",
    "# Final column order\n",
    "column_order = [\n",
    "    'profile_record_id',\n",
    "    'profile',\n",
    "    'site_info_id',\n",
    "    'soil_type_id',\n",
    "    'sample_id'\n",
    "]\n",
    "profile_record_clean = profile_record_clean[column_order]\n",
    "\n",
    "# Check for missing links (optional sanity check)\n",
    "missing_site = profile_record_clean[profile_record_clean['site_info_id'].isna()]\n",
    "missing_soil = profile_record_clean[profile_record_clean['soil_type_id'].isna()]\n",
    "\n",
    "print(f\"{len(missing_site)} profiles missing site_info_id\")\n",
    "print(f\"{len(missing_soil)} profiles missing soil_type_id\")\n",
    "\n",
    "profile_record_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>horizon_layer</th>\n",
       "      <th>upper_depth</th>\n",
       "      <th>lower_depth</th>\n",
       "      <th>dry_color_name</th>\n",
       "      <th>dry_hue</th>\n",
       "      <th>dry_value</th>\n",
       "      <th>dry_chroma</th>\n",
       "      <th>...</th>\n",
       "      <th>compaction</th>\n",
       "      <th>durability</th>\n",
       "      <th>pore_quantity</th>\n",
       "      <th>pore_diameter</th>\n",
       "      <th>pore_shape</th>\n",
       "      <th>root_quantity</th>\n",
       "      <th>root_diameter</th>\n",
       "      <th>moisture_degree</th>\n",
       "      <th>Unnamed: 46</th>\n",
       "      <th>profile_record_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>10999</td>\n",
       "      <td>101_62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Pardo-acinzentado a pardo</td>\n",
       "      <td>10YR</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>Pequena a minima</td>\n",
       "      <td>Brando</td>\n",
       "      <td>Pouco poroso</td>\n",
       "      <td>Muito finos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Muitas finas e bastantes medias</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>11000</td>\n",
       "      <td>101_62</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Pardo</td>\n",
       "      <td>10YR</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Brando</td>\n",
       "      <td>Pouco poroso</td>\n",
       "      <td>Muito finos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bastantes finas e medias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>11001</td>\n",
       "      <td>101_62</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>Pardo-amarelado-claro</td>\n",
       "      <td>10YR</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Pequena a minima</td>\n",
       "      <td>Brando</td>\n",
       "      <td>Pouco poroso</td>\n",
       "      <td>Muito finos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algumas finas e medias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>11002</td>\n",
       "      <td>101_62</td>\n",
       "      <td>4.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Amarelo a amarelo-avermelhado</td>\n",
       "      <td>8,75YR</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Pequena a minima</td>\n",
       "      <td>Brando</td>\n",
       "      <td>Pouco a medianamente poroso</td>\n",
       "      <td>Muito finos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Poucas finas, algumas medias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B_101/62_5_2</td>\n",
       "      <td>11003</td>\n",
       "      <td>101_62</td>\n",
       "      <td>5.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>Pardo-avermelhado</td>\n",
       "      <td>7,5YR</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Brando</td>\n",
       "      <td>Pouco a medianamente poroso</td>\n",
       "      <td>Muito finos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Raras</td>\n",
       "      <td>Medias e grossas</td>\n",
       "      <td>Seco a humido</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     horizon_id sample_id profile  horizon_layer  upper_depth  lower_depth  \\\n",
       "0  B_101/62_1_1     10999  101_62            1.0          0.0         11.0   \n",
       "1  B_101/62_2_1     11000  101_62            2.0         11.0         28.0   \n",
       "2  B_101/62_3_1     11001  101_62            3.0         28.0         54.0   \n",
       "3  B_101/62_4_1     11002  101_62            4.0         54.0         90.0   \n",
       "4  B_101/62_5_2     11003  101_62            5.0         90.0        160.0   \n",
       "\n",
       "                  dry_color_name dry_hue  dry_value  dry_chroma  ...  \\\n",
       "0      Pardo-acinzentado a pardo    10YR        5.0         2.5  ...   \n",
       "1                          Pardo    10YR        5.0         3.0  ...   \n",
       "2          Pardo-amarelado-claro    10YR        6.0         4.0  ...   \n",
       "3  Amarelo a amarelo-avermelhado  8,75YR        7.0         6.0  ...   \n",
       "4              Pardo-avermelhado   7,5YR        7.0         6.0  ...   \n",
       "\n",
       "         compaction durability                pore_quantity  pore_diameter  \\\n",
       "0  Pequena a minima     Brando                 Pouco poroso    Muito finos   \n",
       "1           Pequena     Brando                 Pouco poroso    Muito finos   \n",
       "2  Pequena a minima     Brando                 Pouco poroso    Muito finos   \n",
       "3  Pequena a minima     Brando  Pouco a medianamente poroso    Muito finos   \n",
       "4           Pequena     Brando  Pouco a medianamente poroso    Muito finos   \n",
       "\n",
       "  pore_shape                                 root_quantity     root_diameter  \\\n",
       "0        NaN               Muitas finas e bastantes medias               NaN   \n",
       "1        NaN      Bastantes finas e medias e raras grossas               NaN   \n",
       "2        NaN        Algumas finas e medias e raras grossas               NaN   \n",
       "3        NaN  Poucas finas, algumas medias e raras grossas               NaN   \n",
       "4        NaN                                         Raras  Medias e grossas   \n",
       "\n",
       "  moisture_degree Unnamed: 46 profile_record_id  \n",
       "0            Seco         NaN              <NA>  \n",
       "1            Seco         NaN              <NA>  \n",
       "2            Seco         NaN              <NA>  \n",
       "3            Seco         NaN              <NA>  \n",
       "4   Seco a humido         NaN              <NA>  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_morpho_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to string dtype in both DataFrames before merge\n",
    "df_morpho_cleaned['profile_record_id'] = df_morpho_cleaned['profile_record_id'].astype(str)\n",
    "profile_record_clean['profile_record_id'] = profile_record_clean['profile_record_id'].astype(str)\n",
    "\n",
    "#drop old FK\n",
    "# Drop old FK columns if they exist\n",
    "df_morpho_cleaned = df_morpho_cleaned.drop(\n",
    "    columns=['profile_record_id'],\n",
    "    errors='ignore'\n",
    ")\n",
    "# Now merge on 'profile_record_id'\n",
    "df_morpho_cleaned = df_morpho_cleaned.merge(\n",
    "    profile_record_clean[['profile', 'profile_record_id']],\n",
    "    on='profile',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_morpho_cleaned1 = df_morpho_cleaned[[\n",
    "    'horizon_id',\n",
    "    'sample_id',\n",
    "    #'profile_record_id',\n",
    "    'profile',\n",
    "    'horizon_layer',\n",
    "    'upper_depth',\n",
    "    'lower_depth',\n",
    "    'moisture_degree',\n",
    "    'root_quantity',\n",
    "    'root_diameter',\n",
    "    'texture',\n",
    "    'structure_type',\n",
    "    'structure_class',\n",
    "    'structure_degree',\n",
    "    'pore_diameter',\n",
    "    'pore_quantity',\n",
    "    'pore_shape',\n",
    "    'dry_color_name',\n",
    "    'dry_hue',\n",
    "    'dry_value',\n",
    "    'dry_chroma',\n",
    "    'moist_color_name',\n",
    "    'moist_hue',\n",
    "    'moist_value',\n",
    "    'moist_chroma',\n",
    "    'compaction',\n",
    "    'durability'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>horizon_layer</th>\n",
       "      <th>upper_depth</th>\n",
       "      <th>lower_depth</th>\n",
       "      <th>moisture_degree</th>\n",
       "      <th>root_quantity</th>\n",
       "      <th>root_diameter</th>\n",
       "      <th>texture</th>\n",
       "      <th>...</th>\n",
       "      <th>dry_color_name</th>\n",
       "      <th>dry_hue</th>\n",
       "      <th>dry_value</th>\n",
       "      <th>dry_chroma</th>\n",
       "      <th>moist_color_name</th>\n",
       "      <th>moist_hue</th>\n",
       "      <th>moist_value</th>\n",
       "      <th>moist_chroma</th>\n",
       "      <th>compaction</th>\n",
       "      <th>durability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B_101/62_1_1</td>\n",
       "      <td>10999</td>\n",
       "      <td>101_62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Seco</td>\n",
       "      <td>Muitas finas e bastantes medias</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arenoso</td>\n",
       "      <td>...</td>\n",
       "      <td>Pardo-acinzentado a pardo</td>\n",
       "      <td>10YR</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Pardo-acinzentado-escuro</td>\n",
       "      <td>10YR</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Pequena a minima</td>\n",
       "      <td>Brando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B_101/62_2_1</td>\n",
       "      <td>11000</td>\n",
       "      <td>101_62</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Seco</td>\n",
       "      <td>Bastantes finas e medias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arenoso-franco</td>\n",
       "      <td>...</td>\n",
       "      <td>Pardo</td>\n",
       "      <td>10YR</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Pardo-amarelado-escuro</td>\n",
       "      <td>10YR</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Brando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B_101/62_3_1</td>\n",
       "      <td>11001</td>\n",
       "      <td>101_62</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>Seco</td>\n",
       "      <td>Algumas finas e medias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arenoso-franco</td>\n",
       "      <td>...</td>\n",
       "      <td>Pardo-amarelado-claro</td>\n",
       "      <td>10YR</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Pardo-amarelado-escuro</td>\n",
       "      <td>10YR</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Pequena a minima</td>\n",
       "      <td>Brando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B_101/62_4_1</td>\n",
       "      <td>11002</td>\n",
       "      <td>101_62</td>\n",
       "      <td>4.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Seco</td>\n",
       "      <td>Poucas finas, algumas medias e raras grossas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Franco-arenoso a arenoso-franco</td>\n",
       "      <td>...</td>\n",
       "      <td>Amarelo a amarelo-avermelhado</td>\n",
       "      <td>8,75YR</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Pardo-forte</td>\n",
       "      <td>7,5YR</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Pequena a minima</td>\n",
       "      <td>Brando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B_101/62_5_2</td>\n",
       "      <td>11003</td>\n",
       "      <td>101_62</td>\n",
       "      <td>5.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>Seco a humido</td>\n",
       "      <td>Raras</td>\n",
       "      <td>Medias e grossas</td>\n",
       "      <td>Arenoso-franco</td>\n",
       "      <td>...</td>\n",
       "      <td>Pardo-avermelhado</td>\n",
       "      <td>7,5YR</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Pardo-forte</td>\n",
       "      <td>7,5YR</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Pequena</td>\n",
       "      <td>Brando</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     horizon_id sample_id profile  horizon_layer  upper_depth  lower_depth  \\\n",
       "0  B_101/62_1_1     10999  101_62            1.0          0.0         11.0   \n",
       "1  B_101/62_2_1     11000  101_62            2.0         11.0         28.0   \n",
       "2  B_101/62_3_1     11001  101_62            3.0         28.0         54.0   \n",
       "3  B_101/62_4_1     11002  101_62            4.0         54.0         90.0   \n",
       "4  B_101/62_5_2     11003  101_62            5.0         90.0        160.0   \n",
       "\n",
       "  moisture_degree                                 root_quantity  \\\n",
       "0            Seco               Muitas finas e bastantes medias   \n",
       "1            Seco      Bastantes finas e medias e raras grossas   \n",
       "2            Seco        Algumas finas e medias e raras grossas   \n",
       "3            Seco  Poucas finas, algumas medias e raras grossas   \n",
       "4   Seco a humido                                         Raras   \n",
       "\n",
       "      root_diameter                          texture  ...  \\\n",
       "0               NaN                          Arenoso  ...   \n",
       "1               NaN                   Arenoso-franco  ...   \n",
       "2               NaN                   Arenoso-franco  ...   \n",
       "3               NaN  Franco-arenoso a arenoso-franco  ...   \n",
       "4  Medias e grossas                   Arenoso-franco  ...   \n",
       "\n",
       "                  dry_color_name dry_hue dry_value dry_chroma  \\\n",
       "0      Pardo-acinzentado a pardo    10YR       5.0        2.5   \n",
       "1                          Pardo    10YR       5.0        3.0   \n",
       "2          Pardo-amarelado-claro    10YR       6.0        4.0   \n",
       "3  Amarelo a amarelo-avermelhado  8,75YR       7.0        6.0   \n",
       "4              Pardo-avermelhado   7,5YR       7.0        6.0   \n",
       "\n",
       "           moist_color_name moist_hue moist_value moist_chroma  \\\n",
       "0  Pardo-acinzentado-escuro      10YR         4.0          2.0   \n",
       "1    Pardo-amarelado-escuro      10YR         3.0          4.0   \n",
       "2    Pardo-amarelado-escuro      10YR         4.0          4.0   \n",
       "3               Pardo-forte     7,5YR         5.0          6.0   \n",
       "4               Pardo-forte     7,5YR         5.0          6.0   \n",
       "\n",
       "         compaction  durability  \n",
       "0  Pequena a minima      Brando  \n",
       "1           Pequena      Brando  \n",
       "2  Pequena a minima      Brando  \n",
       "3  Pequena a minima      Brando  \n",
       "4           Pequena      Brando  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_morpho_cleaned1.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_morpho_cleaned1.to_csv('/Users/inesschwartz/Desktop/morpho_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing site_info_clean table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/79mdnyy56_xc3g1jvp9wf4_80000gn/T/ipykernel_8750/4452788.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  climate_features_clean['ID'] = climate_features_clean['ID'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Convert keys to string before merge to avoid dtype mismatch errors\n",
    "site_info_clean['site_info_id'] = site_info_clean['ID'].astype(str)\n",
    "geo_features_clean['ID'] = geo_features_clean['ID'].astype(str)\n",
    "climate_features_clean['ID'] = climate_features_clean['ID'].astype(str)\n",
    "topo_features_clean['ID'] = topo_features_clean['ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop old FK columns if they exist\n",
    "site_info_clean = site_info_clean.drop(\n",
    "    columns=['geo_features_id', 'climate_id', 'topo_features_id', 'district_id', 'land_cover_id', 'geology_id','topo_feature_id','sampling_date', 'districts_id' ],\n",
    "    errors='ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dropped 'ID' from site_info_clean\n",
      "‚úÖ Dropped 'ID' from geo_features_clean\n",
      "‚úÖ Dropped 'ID' from climate_features_clean\n",
      "‚úÖ Dropped 'ID' from topo_features_clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/79mdnyy56_xc3g1jvp9wf4_80000gn/T/ipykernel_8750/2925909182.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  climate_features_clean['ID'] = climate_features_clean['ID'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Convert 'ID' columns to string ---\n",
    "site_info_clean['ID'] = site_info_clean['ID'].astype(str)\n",
    "geo_features_clean['ID'] = geo_features_clean['ID'].astype(str)\n",
    "climate_features_clean['ID'] = climate_features_clean['ID'].astype(str)\n",
    "topo_features_clean['ID'] = topo_features_clean['ID'].astype(str)\n",
    "\n",
    "# --- Step 2: Merge feature tables into site_info_clean ---\n",
    "site_info_clean = site_info_clean.merge(\n",
    "    geo_features_clean[['ID', 'geo_features_id']],\n",
    "    on='ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "site_info_clean = site_info_clean.merge(\n",
    "    climate_features_clean[['ID', 'climate_id']],\n",
    "    on='ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "site_info_clean = site_info_clean.merge(\n",
    "    topo_features_clean[['ID', 'topo_features_id']],\n",
    "    on='ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# --- Step 3: Drop 'ID' column from all involved tables ---\n",
    "for table in ['site_info_clean', 'geo_features_clean', 'climate_features_clean', 'topo_features_clean']:\n",
    "    df = globals().get(table)\n",
    "    if df is not None and 'ID' in df.columns:\n",
    "        df = df.drop(columns=['ID'])\n",
    "        globals()[table] = df\n",
    "        print(f\"‚úÖ Dropped 'ID' from {table}\")\n",
    "    else:\n",
    "        print(f\"‚è≠Ô∏è Skipping {table}: not found or no 'ID' column\")\n",
    "\n",
    "# --- District: Skip merging district_id, use district name directly ---\n",
    "\n",
    "# don't see need for district_id, just use district name\n",
    "# For district (assuming district columns are already strings)\n",
    "# site_info_clean = site_info_clean.merge(\n",
    "#     district_clean[['district', 'district_id']],\n",
    "#     on='district',\n",
    "#     how='left'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>X_coord</th>\n",
       "      <th>Y_coord</th>\n",
       "      <th>district</th>\n",
       "      <th>geo_features_id</th>\n",
       "      <th>climate_id</th>\n",
       "      <th>topo_features_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2770</td>\n",
       "      <td>1/57</td>\n",
       "      <td>12.161278</td>\n",
       "      <td>-15.222598</td>\n",
       "      <td>Namibe</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>1/59</td>\n",
       "      <td>12.575775</td>\n",
       "      <td>-4.866986</td>\n",
       "      <td>Cabinda</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1618</td>\n",
       "      <td>1/61</td>\n",
       "      <td>15.098840</td>\n",
       "      <td>-11.225411</td>\n",
       "      <td>Cuanza Sul</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>881</td>\n",
       "      <td>1/63</td>\n",
       "      <td>17.081955</td>\n",
       "      <td>-9.274587</td>\n",
       "      <td>Malanje</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1750</td>\n",
       "      <td>1/64</td>\n",
       "      <td>20.788116</td>\n",
       "      <td>-11.568683</td>\n",
       "      <td>Moxico</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  site_info_id profile    X_coord    Y_coord    district  geo_features_id  \\\n",
       "0         2770    1/57  12.161278 -15.222598      Namibe                1   \n",
       "1           48    1/59  12.575775  -4.866986     Cabinda                2   \n",
       "2         1618    1/61  15.098840 -11.225411  Cuanza Sul                3   \n",
       "3          881    1/63  17.081955  -9.274587     Malanje                4   \n",
       "4         1750    1/64  20.788116 -11.568683      Moxico                5   \n",
       "\n",
       "   climate_id  topo_features_id  \n",
       "0           1                 1  \n",
       "1           2                 2  \n",
       "2           3                 3  \n",
       "3           4                 4  \n",
       "4           5                 5  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_info_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to drop ID from climate, geo_features, topo_features, and site_info_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>X_coord</th>\n",
       "      <th>Y_coord</th>\n",
       "      <th>district</th>\n",
       "      <th>geo_features_id</th>\n",
       "      <th>climate_id</th>\n",
       "      <th>topo_features_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2770</td>\n",
       "      <td>1/57</td>\n",
       "      <td>12.161278</td>\n",
       "      <td>-15.222598</td>\n",
       "      <td>Namibe</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>1/59</td>\n",
       "      <td>12.575775</td>\n",
       "      <td>-4.866986</td>\n",
       "      <td>Cabinda</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1618</td>\n",
       "      <td>1/61</td>\n",
       "      <td>15.098840</td>\n",
       "      <td>-11.225411</td>\n",
       "      <td>Cuanza Sul</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>881</td>\n",
       "      <td>1/63</td>\n",
       "      <td>17.081955</td>\n",
       "      <td>-9.274587</td>\n",
       "      <td>Malanje</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1750</td>\n",
       "      <td>1/64</td>\n",
       "      <td>20.788116</td>\n",
       "      <td>-11.568683</td>\n",
       "      <td>Moxico</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  site_info_id profile    X_coord    Y_coord    district  geo_features_id  \\\n",
       "0         2770    1/57  12.161278 -15.222598      Namibe                1   \n",
       "1           48    1/59  12.575775  -4.866986     Cabinda                2   \n",
       "2         1618    1/61  15.098840 -11.225411  Cuanza Sul                3   \n",
       "3          881    1/63  17.081955  -9.274587     Malanje                4   \n",
       "4         1750    1/64  20.788116 -11.568683      Moxico                5   \n",
       "\n",
       "   climate_id  topo_features_id  \n",
       "0           1                 1  \n",
       "1           2                 2  \n",
       "2           3                 3  \n",
       "3           4                 4  \n",
       "4           5                 5  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_info_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_record_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>soil_type_id</th>\n",
       "      <th>sample_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  profile_record_id profile site_info_id  soil_type_id sample_id\n",
       "0                 1     139         <NA>           NaN       630\n",
       "1                 2     139         <NA>           NaN       631\n",
       "2                 3     139         <NA>           NaN       632\n",
       "3                 4     139         <NA>           NaN       633\n",
       "4                 5     139         <NA>           NaN      1817"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_record_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## changing column headers to be SQL friendly\n",
    "# Rename problematic columns\n",
    "merged1 = merged1.rename(columns={\n",
    "    'atm_1/3': 'atm_1_3',\n",
    "    'Ca++': 'Ca',\n",
    "    'Mg++': 'Mg',\n",
    "    'Na+': 'Na',\n",
    "    'K+': 'K',\n",
    "    'Min_<0,002': 'Min_lt_0002',\n",
    "    'Min_0,05-0,02': 'Min_005_002',\n",
    "    'Min_0,2-0,05': 'Min_02_005',\n",
    "    'Min_2-0,2': 'Min_2_02',\n",
    "    'As': 'arsenic',\n",
    "    'P': 'phosphorus',\n",
    "    'S': 'sulfur',\n",
    "    'V': 'vanadium'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVING CLEANED TABLES TO CSV FOR DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepare dictionary of clean tables\n",
    "tables = {\n",
    "    \"samples\": samples_clean1,\n",
    "    \"analyses\": merged1,\n",
    "    \"profile_record\": profile_record_clean,\n",
    "    \"morphology_horizon\": morphology_clean1,\n",
    "    \"soil_type\": soil_type_clean,\n",
    "    \"site_info\": site_info_clean,\n",
    "    \"climate_feat\": climate_features_clean,\n",
    "    \"topo_feat\": topo_features_clean,\n",
    "    \"geo_feat\": geo_features_clean,\n",
    "    #\"districts\": district_clean\n",
    "}\n",
    "\n",
    "# # Save each table to CSV with 'NULL' in empty cells\n",
    "for name, df in tables.items():\n",
    "#     df.replace(\"\", pd.NA, inplace=True)  # Replace empty strings with NA\n",
    "    df.to_csv(f\"/Users/inesschwartz/GreenDataScience/Thesis/tables_clean/{name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save random 100 rows of each table as mini versions for DB test\n",
    "import os\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "mini_path = \"/Users/inesschwartz/GreenDataScience/Thesis/tables_clean_mini\"\n",
    "os.makedirs(mini_path, exist_ok=True)\n",
    "\n",
    "# Save random 100 rows of each table as mini versions\n",
    "samples_clean.sample(n=100, random_state=42).to_csv(f\"{mini_path}/samples.csv\", index=False)\n",
    "merged1.sample(n=100, random_state=42).to_csv(f\"{mini_path}/analyses.csv\", index=False)\n",
    "profile_record_clean.sample(n=100, random_state=42).to_csv(f\"{mini_path}/profile_record.csv\", index=False)\n",
    "morphology_clean1.sample(n=100, random_state=42).to_csv(f\"{mini_path}/morphology_horizon.csv\", index=False)\n",
    "soil_type_clean.sample(n=100, random_state=42).to_csv(f\"{mini_path}/soil_type.csv\", index=False)\n",
    "site_info_clean.sample(n=100, random_state=42).to_csv(f\"{mini_path}/site_info.csv\", index=False)\n",
    "climate_features_clean.sample(n=100, random_state=42).to_csv(f\"{mini_path}/climate_feat.csv\", index=False)\n",
    "topo_features_clean.sample(n=100, random_state=42).to_csv(f\"{mini_path}/topo_feat.csv\", index=False)\n",
    "geo_features_clean.sample(n=100, random_state=42).to_csv(f\"{mini_path}/geo_feat.csv\", index=False)\n",
    "#district_clean.sample(n=100, random_state=42).to_csv(f\"{mini_path}/districts.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "samples (14766 rows):\n",
      "  - sample_id (object)\n",
      "  - site_info_id (object)\n",
      "  - profile (object)\n",
      "  - horizon_id (object)\n",
      "  - year (float64)\n",
      "  - shelf (string)\n",
      "  - room (object)\n",
      "\n",
      "analyses (7847 rows):\n",
      "  - lab_sample_id (object)\n",
      "  - sample_id (object)\n",
      "  - EG (float64)\n",
      "  - thick_clay (float64)\n",
      "  - fine_clay (float64)\n",
      "  - silt (float64)\n",
      "  - clay (float64)\n",
      "  - Eq_Hum (float64)\n",
      "  - atm_1_3 (float64)\n",
      "  - atm_15 (float64)\n",
      "  - CACO3 (float64)\n",
      "  - gypsum (float64)\n",
      "  - free_iron (float64)\n",
      "  - organic_carbon (float64)\n",
      "  - total_N (float64)\n",
      "  - P205 (float64)\n",
      "  - organic_material (float64)\n",
      "  - pH_H2O (float64)\n",
      "  - pH_KCL (float64)\n",
      "  - Ca (float64)\n",
      "  - Mg (float64)\n",
      "  - Na (float64)\n",
      "  - K (float64)\n",
      "  - exchangable_bases_sum (float64)\n",
      "  - CEC (float64)\n",
      "  - vanadium (object)\n",
      "  - conductivity (float64)\n",
      "  - soluble_sodium (float64)\n",
      "  - Min_lt_0002 (object)\n",
      "  - Min_005_002 (object)\n",
      "  - Min_02_005 (object)\n",
      "  - Min_2_02 (object)\n",
      "  - field_sample_code (object)\n",
      "  - Depth (object)\n",
      "  - Al (float64)\n",
      "  - Si (float64)\n",
      "  - phosphorus (float64)\n",
      "  - sulfur (object)\n",
      "  - Cl (float64)\n",
      "  - Ti (float64)\n",
      "  - Cr (float64)\n",
      "  - Mn (float64)\n",
      "  - Fe (float64)\n",
      "  - Co (float64)\n",
      "  - Ni (float64)\n",
      "  - Cu (float64)\n",
      "  - Zn (float64)\n",
      "  - arsenic (float64)\n",
      "  - Se (float64)\n",
      "  - Rb (float64)\n",
      "  - Sr (float64)\n",
      "  - Zr (float64)\n",
      "  - Nb (float64)\n",
      "  - Mo (float64)\n",
      "  - Cd (float64)\n",
      "  - Sn (float64)\n",
      "  - Sb (float64)\n",
      "  - Ba (float64)\n",
      "  - Ta (float64)\n",
      "  - W (float64)\n",
      "  - Pt (float64)\n",
      "  - Au (float64)\n",
      "  - Hg (float64)\n",
      "  - Tl (float64)\n",
      "  - Pb (float64)\n",
      "  - Bi (float64)\n",
      "  - Th (float64)\n",
      "  - U (float64)\n",
      "\n",
      "profile_record (86665 rows):\n",
      "  - profile_record_id (object)\n",
      "  - profile (object)\n",
      "  - site_info_id (string)\n",
      "  - soil_type_id (float64)\n",
      "  - sample_id (object)\n",
      "\n",
      "morphology_horizon (207408 rows):\n",
      "  - horizon_id (object)\n",
      "  - sample_id (string)\n",
      "  - profile_record_id (object)\n",
      "  - horizon_layer (float64)\n",
      "  - upper_depth (float64)\n",
      "  - lower_depth (float64)\n",
      "  - moisture_degree (object)\n",
      "  - root_quantity (object)\n",
      "  - root_diameter (object)\n",
      "  - texture (object)\n",
      "  - structure_type (object)\n",
      "  - structure_class (object)\n",
      "  - structure_degree (object)\n",
      "  - pore_diameter (object)\n",
      "  - pore_quantity (object)\n",
      "  - pore_shape (object)\n",
      "  - dry_color_name (object)\n",
      "  - dry_hue (object)\n",
      "  - dry_value (float64)\n",
      "  - dry_chroma (float64)\n",
      "  - moist_color_name (object)\n",
      "  - moist_hue (object)\n",
      "  - moist_value (float64)\n",
      "  - moist_chroma (float64)\n",
      "  - compaction (object)\n",
      "  - durability (object)\n",
      "\n",
      "soil_type (2518 rows):\n",
      "  - soil_type_id (int64)\n",
      "  - profile (object)\n",
      "  - CEP_GR (object)\n",
      "  - CEP_NAME (object)\n",
      "  - FAO (object)\n",
      "\n",
      "site_info (4321 rows):\n",
      "  - site_info_id (object)\n",
      "  - profile (object)\n",
      "  - X_coord (float64)\n",
      "  - Y_coord (float64)\n",
      "  - district (object)\n",
      "  - geo_features_id (int64)\n",
      "  - climate_id (int64)\n",
      "  - topo_features_id (int64)\n",
      "\n",
      "climate_feat (4321 rows):\n",
      "  - climate_id (int64)\n",
      "  - mean_annual_temp (object)\n",
      "  - mean_annual_precip (object)\n",
      "  - koppen_climate (object)\n",
      "  - thornthwaite_climate (object)\n",
      "  - hydric_regime (object)\n",
      "  - thermal_regime (object)\n",
      "\n",
      "topo_feat (4321 rows):\n",
      "  - topo_features_id (int64)\n",
      "  - slope_code (object)\n",
      "  - altitude (float64)\n",
      "  - aspect (object)\n",
      "  - land_surface_temp (object)\n",
      "  - dem_elevation (object)\n",
      "\n",
      "geo_feat (4321 rows):\n",
      "  - geo_features_id (int64)\n",
      "  - geology_id (object)\n",
      "  - lithology_id (object)\n",
      "  - lithology_1954_id (object)\n"
     ]
    }
   ],
   "source": [
    "# List your DataFrame variables here\n",
    "dataframes = {\n",
    "    \"samples\": samples_clean1,\n",
    "    \"analyses\": merged1,\n",
    "    \"profile_record\": profile_record_clean,\n",
    "    \"morphology_horizon\": morphology_clean1,\n",
    "    \"soil_type\": soil_type_clean,\n",
    "    \"site_info\": site_info_clean,\n",
    "    \"climate_feat\": climate_features_clean,\n",
    "    \"topo_feat\": topo_features_clean,\n",
    "    \"geo_feat\": geo_features_clean\n",
    "    #\"districts\": district_clean\n",
    "}\n",
    "\n",
    "# Print column names and their datatypes\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n{name} ({len(df)} rows):\")\n",
    "    for col in df.columns:\n",
    "        dtype = df[col].dtype\n",
    "        print(f\"  - {col} ({dtype})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      profile_record_id profile site_info_id  soil_type_id sample_id\n",
      "0                     1     139         <NA>           NaN       630\n",
      "1                     2     139         <NA>           NaN       631\n",
      "2                     3     139         <NA>           NaN       632\n",
      "3                     4     139         <NA>           NaN       633\n",
      "4                     5     139         <NA>           NaN      1817\n",
      "...                 ...     ...          ...           ...       ...\n",
      "86660             86661  540/67         3627           NaN     18867\n",
      "86661             86662  540/67         3627           NaN     18868\n",
      "86662             86663  540/67         3627           NaN     18869\n",
      "86663             86664  540/67         3627           NaN     18870\n",
      "86664             86665  538/67         3611           NaN     18871\n",
      "\n",
      "[51532 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "missing = profile_record_clean[~profile_record_clean['soil_type_id'].isin(soil_type_clean['soil_type_id'])]\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>year</th>\n",
       "      <th>shelf</th>\n",
       "      <th>room</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>630</td>\n",
       "      <td>172</td>\n",
       "      <td>139</td>\n",
       "      <td>Hb_139/46_1_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>631</td>\n",
       "      <td>173</td>\n",
       "      <td>139</td>\n",
       "      <td>Hb_139/46_2_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>174</td>\n",
       "      <td>139</td>\n",
       "      <td>Hb_139/46_3_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>633</td>\n",
       "      <td>175</td>\n",
       "      <td>139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>687</td>\n",
       "      <td>1034</td>\n",
       "      <td>208</td>\n",
       "      <td>Hb_208/46_1_1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id site_info_id profile     horizon_id    year shelf room\n",
       "0       630          172     139  Hb_139/46_1_1  1946.0     1   22\n",
       "1       631          173     139  Hb_139/46_2_1  1946.0     1   22\n",
       "2       632          174     139  Hb_139/46_3_1  1946.0     1   22\n",
       "3       633          175     139            NaN  1946.0     1   22\n",
       "4       687         1034     208  Hb_208/46_1_1  1946.0     1   22"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_clean1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     site_info_id profile    X_coord    Y_coord    district  geo_features_id  \\\n",
      "0            2770    1/57  12.161278 -15.222598      Namibe                1   \n",
      "2            1618    1/61  15.098840 -11.225411  Cuanza Sul                3   \n",
      "4            1750    1/64  20.788116 -11.568683      Moxico                5   \n",
      "5            3097    1/66  17.666766 -14.655526         NaN                6   \n",
      "8            2675   10/54  14.445188 -14.922688       Huila                9   \n",
      "...           ...     ...        ...        ...         ...              ...   \n",
      "4313         1689   99/63  18.164654 -11.382465     Malanje             4314   \n",
      "4314         3247   99/66  18.994294 -15.814591         NaN             4315   \n",
      "4317         1485  99c/63  17.541534 -10.890781     Malanje             4318   \n",
      "4319         1213   9c/63  16.357859  -9.986349     Malanje             4320   \n",
      "4320         1505   9c/65  20.937136 -10.942842   Lunda Sul             4321   \n",
      "\n",
      "      climate_id  topo_features_id  \n",
      "0              1                 1  \n",
      "2              3                 3  \n",
      "4              5                 5  \n",
      "5              6                 6  \n",
      "8              9                 9  \n",
      "...          ...               ...  \n",
      "4313        4314              4314  \n",
      "4314        4315              4315  \n",
      "4317        4318              4318  \n",
      "4319        4320              4320  \n",
      "4320        4321              4321  \n",
      "\n",
      "[3136 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "missing1 = site_info_clean[~site_info_clean['site_info_id'].isin(samples_clean['site_info_id'])]\n",
    "print(missing1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site_info_id\n",
       "16       16\n",
       "14       15\n",
       "9        15\n",
       "42       15\n",
       "40       15\n",
       "         ..\n",
       "32/57     1\n",
       "31/57     1\n",
       "30/57     1\n",
       "29/57     1\n",
       "657 c     1\n",
       "Name: count, Length: 6439, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_clean1['site_info_id'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>horizon_id</th>\n",
       "      <th>year</th>\n",
       "      <th>shelf</th>\n",
       "      <th>room</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10641</th>\n",
       "      <td>14219</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>318/63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1963.0</td>\n",
       "      <td>458</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12742</th>\n",
       "      <td>16639</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>582</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12743</th>\n",
       "      <td>16640</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>582</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744</th>\n",
       "      <td>16641</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>582</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12745</th>\n",
       "      <td>16642</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>582</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12746</th>\n",
       "      <td>16643</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>582</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12747</th>\n",
       "      <td>16644</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>582</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12748</th>\n",
       "      <td>16645</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>582</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12749</th>\n",
       "      <td>16646</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>582</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12750</th>\n",
       "      <td>16647</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>582</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12751</th>\n",
       "      <td>16648</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12752</th>\n",
       "      <td>16649</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12753</th>\n",
       "      <td>16650</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12754</th>\n",
       "      <td>16651</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12755</th>\n",
       "      <td>16652</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12756</th>\n",
       "      <td>16653</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12757</th>\n",
       "      <td>16654</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12758</th>\n",
       "      <td>16655</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12759</th>\n",
       "      <td>16656</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12760</th>\n",
       "      <td>16657</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12761</th>\n",
       "      <td>16658</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12762</th>\n",
       "      <td>16659</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12763</th>\n",
       "      <td>16660</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12764</th>\n",
       "      <td>16661</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12765</th>\n",
       "      <td>16662</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12766</th>\n",
       "      <td>16663</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12767</th>\n",
       "      <td>16664</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12768</th>\n",
       "      <td>16666</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>583</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_id site_info_id profile horizon_id    year shelf room\n",
       "10641     14219         <NA>  318/63        NaN  1963.0   458   26\n",
       "12742     16639         <NA>     nan        NaN  1965.0   582   26\n",
       "12743     16640         <NA>     nan        NaN  1965.0   582   26\n",
       "12744     16641         <NA>     nan        NaN  1965.0   582   26\n",
       "12745     16642         <NA>     nan        NaN  1965.0   582   26\n",
       "12746     16643         <NA>     nan        NaN  1965.0   582   26\n",
       "12747     16644         <NA>     nan        NaN  1965.0   582   26\n",
       "12748     16645         <NA>     nan        NaN  1965.0   582   26\n",
       "12749     16646         <NA>     nan        NaN  1965.0   582   26\n",
       "12750     16647         <NA>     nan        NaN  1965.0   582   26\n",
       "12751     16648         <NA>     nan        NaN  1965.0   583   26\n",
       "12752     16649         <NA>     nan        NaN  1965.0   583   26\n",
       "12753     16650         <NA>     nan        NaN  1965.0   583   26\n",
       "12754     16651         <NA>     nan        NaN  1965.0   583   26\n",
       "12755     16652         <NA>     nan        NaN  1965.0   583   26\n",
       "12756     16653         <NA>     nan        NaN  1965.0   583   26\n",
       "12757     16654         <NA>     nan        NaN  1965.0   583   26\n",
       "12758     16655         <NA>     nan        NaN  1965.0   583   26\n",
       "12759     16656         <NA>     nan        NaN  1965.0   583   26\n",
       "12760     16657         <NA>     nan        NaN  1965.0   583   26\n",
       "12761     16658         <NA>     nan        NaN  1965.0   583   26\n",
       "12762     16659         <NA>     nan        NaN  1965.0   583   26\n",
       "12763     16660         <NA>     nan        NaN  1965.0   583   26\n",
       "12764     16661         <NA>     nan        NaN  1965.0   583   26\n",
       "12765     16662         <NA>     nan        NaN  1965.0   583   26\n",
       "12766     16663         <NA>     nan        NaN  1965.0   583   26\n",
       "12767     16664         <NA>     nan        NaN  1965.0   583   26\n",
       "12768     16666         <NA>     nan        NaN  1965.0   583   26"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_clean1[samples_clean1['site_info_id'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_record_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>soil_type_id</th>\n",
       "      <th>sample_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  profile_record_id profile site_info_id  soil_type_id sample_id\n",
       "0                 1     139         <NA>           NaN       630\n",
       "1                 2     139         <NA>           NaN       631\n",
       "2                 3     139         <NA>           NaN       632\n",
       "3                 4     139         <NA>           NaN       633\n",
       "4                 5     139         <NA>           NaN      1817"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_record_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soil_type_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>CEP_GR</th>\n",
       "      <th>CEP_NAME</th>\n",
       "      <th>FAO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1/51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1/57</td>\n",
       "      <td>Aridicos</td>\n",
       "      <td>Aridicos com calcario Pardo-cinzentos</td>\n",
       "      <td>CLha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1/59</td>\n",
       "      <td>Psamoferralicos</td>\n",
       "      <td>Psamo-ferralicos Amarelos ou Alaranjados, sedi...</td>\n",
       "      <td>FRxa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1/63</td>\n",
       "      <td>Ferraliticos</td>\n",
       "      <td>Fracamente Ferralicos Vermelhos Clino-argilico...</td>\n",
       "      <td>FRh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10/54</td>\n",
       "      <td>Ferraliticos</td>\n",
       "      <td>Fracamente Ferralicos pardo-amarelados</td>\n",
       "      <td>FRh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   soil_type_id profile           CEP_GR  \\\n",
       "0             1    1/51              NaN   \n",
       "1             2    1/57         Aridicos   \n",
       "2             3    1/59  Psamoferralicos   \n",
       "3             4    1/63     Ferraliticos   \n",
       "4             5   10/54     Ferraliticos   \n",
       "\n",
       "                                            CEP_NAME   FAO  \n",
       "0                                                NaN   NaN  \n",
       "1              Aridicos com calcario Pardo-cinzentos  CLha  \n",
       "2  Psamo-ferralicos Amarelos ou Alaranjados, sedi...  FRxa  \n",
       "3  Fracamente Ferralicos Vermelhos Clino-argilico...   FRh  \n",
       "4             Fracamente Ferralicos pardo-amarelados   FRh  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soil_type_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå These rows reference soil_type_id values that do NOT exist in soil_type.csv:\n",
      "      profile_record_id profile site_info_id  soil_type_id sample_id\n",
      "0                     1     139         <NA>           NaN       630\n",
      "1                     2     139         <NA>           NaN       631\n",
      "2                     3     139         <NA>           NaN       632\n",
      "3                     4     139         <NA>           NaN       633\n",
      "4                     5     139         <NA>           NaN      1817\n",
      "...                 ...     ...          ...           ...       ...\n",
      "86660             86661  540/67         3627           NaN     18867\n",
      "86661             86662  540/67         3627           NaN     18868\n",
      "86662             86663  540/67         3627           NaN     18869\n",
      "86663             86664  540/67         3627           NaN     18870\n",
      "86664             86665  538/67         3611           NaN     18871\n",
      "\n",
      "[51532 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get list of valid soil_type_ids\n",
    "valid_soil_type_ids = set(soil_type_clean['soil_type_id'])\n",
    "\n",
    "# Find invalid soil_type_ids in profile_df\n",
    "invalid_rows = profile_record_clean[~profile_record_clean['soil_type_id'].isin(valid_soil_type_ids)]\n",
    "\n",
    "# Show the result\n",
    "if not invalid_rows.empty:\n",
    "    print(\"‚ùå These rows reference soil_type_id values that do NOT exist in soil_type.csv:\")\n",
    "    print(invalid_rows)\n",
    "else:\n",
    "    print(\"‚úÖ All soil_type_id values in profile_record.csv are valid.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_record_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>site_info_id</th>\n",
       "      <th>soil_type_id</th>\n",
       "      <th>sample_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>139</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  profile_record_id profile site_info_id  soil_type_id sample_id\n",
       "0                 1     139         <NA>           NaN       630\n",
       "1                 2     139         <NA>           NaN       631\n",
       "2                 3     139         <NA>           NaN       632\n",
       "3                 4     139         <NA>           NaN       633\n",
       "4                 5     139         <NA>           NaN      1817"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_record_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå These rows have NULL values in the site_info_id column of profile_record.csv:\n",
      "      profile_record_id profile site_info_id  soil_type_id sample_id\n",
      "0                     1     139         <NA>           NaN       630\n",
      "1                     2     139         <NA>           NaN       631\n",
      "2                     3     139         <NA>           NaN       632\n",
      "3                     4     139         <NA>           NaN       633\n",
      "4                     5     139         <NA>           NaN      1817\n",
      "...                 ...     ...          ...           ...       ...\n",
      "86240             86241  508/71         <NA>           NaN     18765\n",
      "86267             86268  516/68         <NA>           NaN     18772\n",
      "86268             86269  516/69         <NA>           NaN     18773\n",
      "86270             86271  493/68         <NA>           NaN     18781\n",
      "86271             86272  493/69         <NA>           NaN     18782\n",
      "\n",
      "[29884 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find rows where soil_type_id is null in profile_record_clean\n",
    "null_rows = profile_record_clean[profile_record_clean['site_info_id'].isnull()]\n",
    "\n",
    "# Show the result\n",
    "if not null_rows.empty:\n",
    "    print(\"‚ùå These rows have NULL values in the site_info_id column of profile_record.csv:\")\n",
    "    print(null_rows)\n",
    "else:\n",
    "    print(\"‚úÖ No NULL values found in the site_info_id column of profile_record.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ No NULL values found in the ID column of perfis_local.csv.\n"
     ]
    }
   ],
   "source": [
    "# Find rows where soil_type_id is null in profile_record_clean\n",
    "null_rows = profile_loc[profile_loc['ID'].isnull()]\n",
    "\n",
    "# Show the result\n",
    "if not null_rows.empty:\n",
    "    print(\"‚ùå These rows have NULL values in the ID column of perfis_local.csv:\")\n",
    "    print(null_rows)\n",
    "else:\n",
    "    print(\"‚úÖ No NULL values found in the ID column of perfis_local.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soil_type_ids in profile_record missing from soil_type.csv: {nan}\n"
     ]
    }
   ],
   "source": [
    "soil_type_ids = set(soil_type_clean['soil_type_id'].unique())\n",
    "profile_soil_type_ids = set(profile_record_clean['soil_type_id'].unique())\n",
    "\n",
    "missing_ids = profile_soil_type_ids - soil_type_ids\n",
    "\n",
    "print(\"soil_type_ids in profile_record missing from soil_type.csv:\", missing_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4835 in soil_type.csv? False\n",
      "4835 in profile_record.csv? False\n"
     ]
    }
   ],
   "source": [
    "# Check if 4835 is in soil_type_id column of soil_type.csv\n",
    "soil_type_has_4835 = 4835 in soil_type_clean['soil_type_id'].values\n",
    "\n",
    "# Check if 4835 is in soil_type_id column of profile_record.csv\n",
    "profile_record_has_4835 = 4835 in profile_record_clean['soil_type_id'].values\n",
    "\n",
    "print(f\"4835 in soil_type.csv? {soil_type_has_4835}\")\n",
    "print(f\"4835 in profile_record.csv? {profile_record_has_4835}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(4835 in profile_record_clean['soil_type_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Empty DataFrame\n",
      "Columns: [profile_record_id, profile, site_info_id, soil_type_id, sample_id]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(4835 in profile_record_clean['soil_type_id'].values)\n",
    "print(profile_record_clean[profile_record_clean['soil_type_id'] == 4835])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing soil_type_ids in soil_type.csv: {nan}\n"
     ]
    }
   ],
   "source": [
    "soil_type_ids_in_soil_type = set(soil_type_clean['soil_type_id'].unique())\n",
    "soil_type_ids_in_profile_record = set(profile_record_clean['soil_type_id'].unique())\n",
    "\n",
    "missing_soil_type_ids = soil_type_ids_in_profile_record - soil_type_ids_in_soil_type\n",
    "\n",
    "print(f\"Missing soil_type_ids in soil_type.csv: {missing_soil_type_ids}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
